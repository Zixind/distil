{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "67b4c01d5de3463e9d30dc72329c3b51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_fb8b97eb65254f03b53d68b23a8b9038",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4ef3e67d21c14573b4c09b4f99e5d3ff",
              "IPY_MODEL_6443e2d5664046349f32153d8345576b",
              "IPY_MODEL_f1fe0c140bdb43f98c6f8e08ee438566"
            ]
          }
        },
        "fb8b97eb65254f03b53d68b23a8b9038": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4ef3e67d21c14573b4c09b4f99e5d3ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_98028807e86e44b69c415897310ea63c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_97caa8f994a841428898a3d7ff7570c3"
          }
        },
        "6443e2d5664046349f32153d8345576b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_76f4041474da418caf8f96f757b34da3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 38247903,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 38247903,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b060a696cc474d6d9a78cfb6cbdea01c"
          }
        },
        "f1fe0c140bdb43f98c6f8e08ee438566": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_81b8ca07b33943cd9f090a025a738555",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 38248448/? [00:04&lt;00:00, 11266464.64it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3b48d77c4173420f8bdd9139aaf07f2e"
          }
        },
        "98028807e86e44b69c415897310ea63c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "97caa8f994a841428898a3d7ff7570c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "76f4041474da418caf8f96f757b34da3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b060a696cc474d6d9a78cfb6cbdea01c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "81b8ca07b33943cd9f090a025a738555": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3b48d77c4173420f8bdd9139aaf07f2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8AxA7KVfv9m"
      },
      "source": [
        "# **DISTIL Usage Example: MedMNIST**\n",
        "\n",
        "Here, we show how to use DISTIL to perform active learning on image classification tasks (MedMNIST's OrganAMNIST). This notebook can be easily executed on Google Colab. ([Source](https://github.com/decile-team/distil/blob/main/tutorials/image_classification/DISTIL_Example_MedMNIST.ipynb))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4Y2r9Y_fH5B"
      },
      "source": [
        "## Installation and Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bsVDupO_EOh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "99fefaea-a979-4450-b0c3-b817cb5048f4"
      },
      "source": [
        "# Get DISTIL\n",
        "!git clone https://github.com/decile-team/distil.git\n",
        "!pip install -r distil/requirements/requirements.txt\n",
        "\n",
        "# Get MedMNIST\n",
        "!git clone https://github.com/MedMNIST/MedMNIST.git\n",
        "\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "sys.path.append(\"MedMNIST/\")\n",
        "from medmnist import OrganAMNIST\n",
        "\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, Subset, ConcatDataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import cifar\n",
        "\n",
        "sys.path.append('distil/')\n",
        "from distil.active_learning_strategies import GLISTER, BADGE, EntropySampling, RandomSampling   # All active learning strategies showcased in this example\n",
        "from distil.utils.models.resnet import ResNet18                                                 # The model used in our image classification example\n",
        "from distil.utils.train_helper import data_train                                                # A utility training class provided by DISTIL\n",
        "from distil.utils.utils import LabeledToUnlabeledDataset                                        # A utility wrapper class that removes labels from labeled PyTorch dataset objects"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'distil'...\n",
            "remote: Enumerating objects: 3318, done.\u001b[K\n",
            "remote: Counting objects: 100% (1275/1275), done.\u001b[K\n",
            "remote: Compressing objects: 100% (811/811), done.\u001b[K\n",
            "remote: Total 3318 (delta 788), reused 833 (delta 456), pack-reused 2043\u001b[K\n",
            "Receiving objects: 100% (3318/3318), 23.04 MiB | 7.22 MiB/s, done.\n",
            "Resolving deltas: 100% (2061/2061), done.\n",
            "Looking in indexes: https://test.pypi.org/simple/, https://pypi.org/simple/\n",
            "Collecting sphinxcontrib-bibtex>=2.3.0\n",
            "  Downloading sphinxcontrib_bibtex-2.4.1-py3-none-any.whl (38 kB)\n",
            "Collecting multipledispatch==0.6.0\n",
            "  Downloading multipledispatch-0.6.0-py3-none-any.whl (11 kB)\n",
            "Collecting scikit-learn==0.23.0\n",
            "  Downloading scikit_learn-0.23.0-cp37-cp37m-manylinux1_x86_64.whl (7.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.3 MB 3.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.2 in /usr/local/lib/python3.7/dist-packages (from -r distil/requirements/requirements.txt (line 4)) (1.21.5)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from -r distil/requirements/requirements.txt (line 5)) (1.4.1)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from -r distil/requirements/requirements.txt (line 6)) (1.10.0+cu111)\n",
            "Requirement already satisfied: tqdm>=4.24.0 in /usr/local/lib/python3.7/dist-packages (from -r distil/requirements/requirements.txt (line 7)) (4.62.3)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from -r distil/requirements/requirements.txt (line 8)) (0.51.2)\n",
            "Collecting submodlib\n",
            "  Downloading https://test-files.pythonhosted.org/packages/55/62/88e02a0e170498f38f7b9ce22b3e0a6a3cf9c82a33d3553da693c5c52872/submodlib-1.1.5.tar.gz (83 kB)\n",
            "\u001b[K     |████████████████████████████████| 83 kB 1.7 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r distil/requirements/requirements.txt (line 12)) (1.3.5)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from -r distil/requirements/requirements.txt (line 13)) (0.11.1+cu111)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from multipledispatch==0.6.0->-r distil/requirements/requirements.txt (line 2)) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.0->-r distil/requirements/requirements.txt (line 3)) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.0->-r distil/requirements/requirements.txt (line 3)) (3.1.0)\n",
            "Requirement already satisfied: docutils>=0.8 in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (0.17.1)\n",
            "Collecting Sphinx>=2.1\n",
            "  Downloading Sphinx-4.4.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 16.6 MB/s \n",
            "\u001b[?25hCollecting pybtex>=0.20\n",
            "  Downloading pybtex-0.24.0-py2.py3-none-any.whl (561 kB)\n",
            "\u001b[K     |████████████████████████████████| 561 kB 418 kB/s \n",
            "\u001b[?25hCollecting pybtex-docutils>=1.0.0\n",
            "  Downloading pybtex_docutils-1.0.1-py3-none-any.whl (4.8 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->-r distil/requirements/requirements.txt (line 6)) (3.10.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->-r distil/requirements/requirements.txt (line 8)) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->-r distil/requirements/requirements.txt (line 8)) (0.34.0)\n",
            "Requirement already satisfied: PyYAML>=3.01 in /usr/local/lib/python3.7/dist-packages (from pybtex>=0.20->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (3.13)\n",
            "Collecting latexcodec>=1.0.4\n",
            "  Downloading latexcodec-2.0.1-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from Sphinx>=2.1->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (2.2.0)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.7/dist-packages (from Sphinx>=2.1->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (2.11.3)\n",
            "Collecting sphinxcontrib-devhelp\n",
            "  Downloading sphinxcontrib_devhelp-1.0.2-py2.py3-none-any.whl (84 kB)\n",
            "\u001b[K     |████████████████████████████████| 84 kB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.7/dist-packages (from Sphinx>=2.1->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (2.6.1)\n",
            "Collecting sphinxcontrib-qthelp\n",
            "  Downloading sphinxcontrib_qthelp-1.0.3-py2.py3-none-any.whl (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 12.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from Sphinx>=2.1->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (1.3.0)\n",
            "Collecting sphinxcontrib-applehelp\n",
            "  Downloading sphinxcontrib_applehelp-1.0.2-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[K     |████████████████████████████████| 121 kB 75.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from Sphinx>=2.1->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (0.7.12)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from Sphinx>=2.1->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (4.11.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from Sphinx>=2.1->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (21.3)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.7/dist-packages (from Sphinx>=2.1->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (2.9.1)\n",
            "Collecting sphinxcontrib-htmlhelp>=2.0.0\n",
            "  Downloading sphinxcontrib_htmlhelp-2.0.0-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[K     |████████████████████████████████| 100 kB 13.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from Sphinx>=2.1->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (2.23.0)\n",
            "Collecting sphinxcontrib-jsmath\n",
            "  Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.7/dist-packages (from Sphinx>=2.1->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (1.1.5)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.7/dist-packages (from babel>=1.3->Sphinx>=2.1->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (2018.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->Sphinx>=2.1->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (3.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.3->Sphinx>=2.1->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (2.0.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->Sphinx>=2.1->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->Sphinx>=2.1->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->Sphinx>=2.1->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->Sphinx>=2.1->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from submodlib->-r distil/requirements/requirements.txt (line 11)) (0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r distil/requirements/requirements.txt (line 12)) (2.8.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->-r distil/requirements/requirements.txt (line 13)) (7.1.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->Sphinx>=2.1->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (3.0.7)\n",
            "Building wheels for collected packages: submodlib\n",
            "  Building wheel for submodlib (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for submodlib: filename=submodlib-1.1.5-cp37-cp37m-linux_x86_64.whl size=491515 sha256=35cd43d6fb13141c61f8c4e788a93c99f881115038556be18f1d1c35b0935706\n",
            "  Stored in directory: /root/.cache/pip/wheels/17/3c/03/48ce7dd03798c0564b61e61020e733443aed88e115518442b4\n",
            "Successfully built submodlib\n",
            "Installing collected packages: latexcodec, sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, scikit-learn, pybtex, Sphinx, pybtex-docutils, submodlib, sphinxcontrib-bibtex, multipledispatch\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "  Attempting uninstall: Sphinx\n",
            "    Found existing installation: Sphinx 1.8.6\n",
            "    Uninstalling Sphinx-1.8.6:\n",
            "      Successfully uninstalled Sphinx-1.8.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.23.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed Sphinx-4.4.0 latexcodec-2.0.1 multipledispatch-0.6.0 pybtex-0.24.0 pybtex-docutils-1.0.1 scikit-learn-0.23.0 sphinxcontrib-applehelp-1.0.2 sphinxcontrib-bibtex-2.4.1 sphinxcontrib-devhelp-1.0.2 sphinxcontrib-htmlhelp-2.0.0 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-1.0.3 submodlib-1.1.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sphinxcontrib"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MedMNIST'...\n",
            "remote: Enumerating objects: 361, done.\u001b[K\n",
            "remote: Counting objects: 100% (185/185), done.\u001b[K\n",
            "remote: Compressing objects: 100% (125/125), done.\u001b[K\n",
            "remote: Total 361 (delta 110), reused 124 (delta 56), pack-reused 176\u001b[K\n",
            "Receiving objects: 100% (361/361), 5.84 MiB | 13.37 MiB/s, done.\n",
            "Resolving deltas: 100% (203/203), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vvn_Dal4gb8S"
      },
      "source": [
        "## Preparing OrganAMNIST\n",
        "\n",
        "The MedMNIST dataset OrganAMNIST contains 58,850 28x28 monochrome images (34,581 train, 6,491 validation, 17,778 test) in 11 different classes. The 11 different classes represent 11 different organs, and the task is to correctly classify a given axial view of an organ. Here, we do a simple setup of the OrganAMNIST dataset that we will use in this example. More importantly, we define a split on OrganAMNIST's training set into an initial labeled seed set and an unlabeled set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J39lJ2uWVpbM"
      },
      "source": [
        "**Calculate Average/STD**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Khfi12OlSTSz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "67b4c01d5de3463e9d30dc72329c3b51",
            "fb8b97eb65254f03b53d68b23a8b9038",
            "4ef3e67d21c14573b4c09b4f99e5d3ff",
            "6443e2d5664046349f32153d8345576b",
            "f1fe0c140bdb43f98c6f8e08ee438566",
            "98028807e86e44b69c415897310ea63c",
            "97caa8f994a841428898a3d7ff7570c3",
            "76f4041474da418caf8f96f757b34da3",
            "b060a696cc474d6d9a78cfb6cbdea01c",
            "81b8ca07b33943cd9f090a025a738555",
            "3b48d77c4173420f8bdd9139aaf07f2e"
          ]
        },
        "outputId": "54007917-0d32-4494-b3c0-88cdc98a6a8d"
      },
      "source": [
        "# We do not have the average and standard deviation to use for data normalization. Here, we compute \n",
        "# it on a per-pixel basis, and we use the training set only for this calculation.\n",
        "\n",
        "train_dataset = OrganAMNIST(root=\".\", split=\"train\", download=True, transform=transforms.ToTensor())\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "average = 0.\n",
        "num_pixels = 0\n",
        "for batch_idx, (image, label) in enumerate(train_dataloader):\n",
        "    image = image.to(\"cuda\")\n",
        "    average += image.sum()\n",
        "    num_pixels += len(image.flatten())\n",
        "average = average.item() / num_pixels\n",
        "\n",
        "var = 0.\n",
        "for batch_idx, (image, label) in enumerate(train_dataloader):\n",
        "    image = image.to(\"cuda\").flatten()\n",
        "    image_diff = image - average\n",
        "    image_var = torch.dot(image_diff, image_diff)\n",
        "    var += image_var\n",
        "\n",
        "std = math.sqrt(var / (num_pixels - 1))\n",
        "\n",
        "print(\"AVERAGE:\",average)\n",
        "print(\"STD:\",std)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://zenodo.org/record/5208230/files/organamnist.npz?download=1 to ./organamnist.npz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "67b4c01d5de3463e9d30dc72329c3b51",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/38247903 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AVERAGE: 0.4677578197063505\n",
            "STD: 0.2975477576591013\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xm-wkPNJVwo-"
      },
      "source": [
        "**Create OrganAMNIST Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdLy9wyjCuT-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aefe92b9-c7e5-43ec-a399-1ae4fe98979e"
      },
      "source": [
        "data_set_name = 'OrganAMNIST'\n",
        "download_path = '.'\n",
        "\n",
        "# Define transforms on the dataset splits of OrganAMNIST. Here, we use random crops and horizontal flips for training augmentations.\n",
        "# Both the train and test sets are converted to PyTorch tensors and are normalized around the mean/std of OrganAMNIST.\n",
        "organ_amnist_training_transform = transforms.Compose([transforms.RandomCrop(28, padding=4), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize((average,), (std,))])\n",
        "organ_amnist_test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((average,), (std,))])\n",
        "\n",
        "# OrganAMNIST provides its labels in list form; so we need to get the original label\n",
        "target_transform = lambda x: x[0]\n",
        "\n",
        "# Get the dataset objects from MedMNIST. Here, OrganAMNIST is downloaded, and the transform is applied when points \n",
        "# are retrieved.\n",
        "organ_amnist_full_train = OrganAMNIST(root=download_path, split=\"train\", download=True, transform=organ_amnist_training_transform, target_transform=target_transform)\n",
        "organ_amnist_test = OrganAMNIST(root=download_path, split=\"test\", download=True, transform=organ_amnist_test_transform, target_transform=target_transform)\n",
        "\n",
        "# Get the dimension of the images. Here, we simply take the very first image of OrganAMNIST\n",
        "# and query its dimension.\n",
        "dim = np.shape(organ_amnist_full_train[0][0])\n",
        "\n",
        "# We now define a train-unlabeled split for the sake of the experiment. Here, we simply take 250 points as the initial seed set.\n",
        "# The rest of the points are taken as the unlabeled set. While the unlabeled set constructed here technically has labels, they \n",
        "# are only used when querying for labels. Hence, they only exist here for the sake of experimental design.\n",
        "train_size = 250\n",
        "organ_amnist_train = Subset(organ_amnist_full_train, list(range(train_size)))\n",
        "organ_amnist_unlabeled = Subset(organ_amnist_full_train, list(range(train_size, len(organ_amnist_full_train))))\n",
        "\n",
        "# Define the number of active learning rounds to conduct, the budget, and the number of classes in OrganAMNIST\n",
        "nclasses = 11\n",
        "n_rounds = 7\n",
        "budget = 250"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using downloaded and verified file: ./organamnist.npz\n",
            "Using downloaded and verified file: ./organamnist.npz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVwJ7RcOhVc7"
      },
      "source": [
        "## Preparing the Model\n",
        "\n",
        "Here, we use DISTIL's provided implementation of the [ResNet-18](https://arxiv.org/abs/1512.03385) architecture. We also create a model directory to store trained models in this example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pHzwNwvhVts"
      },
      "source": [
        "net = ResNet18(num_classes=nclasses, channels=1)\n",
        "base_dir = \"models\"\n",
        "os.makedirs(base_dir, exist_ok = True)\n",
        "model_directory = os.path.join(base_dir, 'base_model.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spOTFeWgfm4b"
      },
      "source": [
        "## Training an Initial Model\n",
        "Here, we train an initial model. We do so by creating a training loop object on the initial seed set, the model architecture, and a list of provided arguments. We then save the initial model. Note: If you've already run the first cell, then you can simply run the second cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ru5tj6u8fwEi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5977897-556f-4659-f188-8617ae065b04"
      },
      "source": [
        "# Specify additional training parameters. Here, we set the maximum number of epochs of training to 300, \n",
        "# the learning rate to 0.01, the batch size to 20, the maximum train accuracy of training to 0.99, and \n",
        "# the optimizer to stochastic gradient descent.\n",
        "args = {'n_epoch':300, 'lr':float(0.01), 'batch_size':20, 'max_accuracy':0.99, 'optimizer':'sgd'} \n",
        "dt = data_train(organ_amnist_train, net, args)\n",
        "clf = dt.train()\n",
        "torch.save(clf.state_dict(), model_directory)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training..\n",
            "Epoch: 108 Training accuracy: 0.996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dd-ieb7RgyJY"
      },
      "source": [
        "base_dir = \"models\"\n",
        "model_directory = os.path.join(base_dir, 'base_model.pth')\n",
        "net.load_state_dict(torch.load(model_directory))\n",
        "clf = net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhsUx57BjBT2"
      },
      "source": [
        "## Active Learning Strategies\n",
        "\n",
        "Here, we show examples of a couple active learning strategies being used in the setting of image classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-Bxn6LmCeEI"
      },
      "source": [
        "### Random Sampling\n",
        "This strategy is often used as a baseline, where we pick a subset of unlabeled points randomly. Here, we create a instance of `distil.active_learning_strategies.random_sampling.RandomSampling` by passing following parameters:\n",
        "\n",
        "**training_dataset** – The labeled dataset, which has a PyTorch interface that returns a tensor of data and a tensor of labels OR a dictionary with a \"labels\" key.\n",
        "\n",
        "**unlabeled_dataset** – The unlabeled dataset, which has a PyTorch interface that returns a tensor of data OR a dictionary.\n",
        "\n",
        "**net** – Model architecture used for training. Could be instance of models defined in distil.utils.models or something similar (in this instance, a Huggingface transformer).\n",
        "\n",
        "**nclasses** – No. of classes in the dataset\n",
        "\n",
        "**args**– This dictionary should have ‘batch_size’ as a key. 'batch_size' should be such that one can exploit the benefits of tensorization while honouring the resourse constraits. This ‘batch_size’ therefore can be different than the one used for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8OdeQ63DZuP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2228ce0e-062d-40cc-cdaa-29ba40a8de26"
      },
      "source": [
        "# Initialize the random sampling AL strategy. Note: The labels are shaved off the unlabeled dataset above to match the setting.\n",
        "strategy_args = {'batch_size' : 20}\n",
        "strategy = RandomSampling(organ_amnist_train, LabeledToUnlabeledDataset(organ_amnist_unlabeled), net, nclasses, strategy_args)\n",
        "\n",
        "# Use the same training parameters as before\n",
        "args = {'n_epoch':300, 'lr':float(0.01), 'batch_size':20, 'max_accuracy':0.99, 'optimizer':'sgd'} \n",
        "dt = data_train(organ_amnist_train, clf, args)\n",
        "\n",
        "# Update the model used in the AL strategy with the loaded initial model\n",
        "strategy.update_model(clf)\n",
        "\n",
        "# Get the test accuracy of the initial model\n",
        "acc = np.zeros(n_rounds)\n",
        "acc[0] = dt.get_acc_on_set(organ_amnist_test)\n",
        "print('Initial Testing accuracy:', round(acc[0]*100, 2), flush=True)\n",
        "\n",
        "# User Controlled Loop\n",
        "for rd in range(1, n_rounds):\n",
        "    print('-------------------------------------------------')\n",
        "    print('Round', rd) \n",
        "    print('-------------------------------------------------')\n",
        "\n",
        "    # Use select() to obtain the indices in the unlabeled set that should be labeled\n",
        "    organ_amnist_full_train.transform = organ_amnist_test_transform       # Disable augmentation while selecting new points as to not interfere with the strategies\n",
        "    idx = strategy.select(budget)\n",
        "    organ_amnist_full_train.transform = organ_amnist_training_transform   # Enable augmentation\n",
        "\n",
        "    # Add the selected points to the train set. The unlabeled set shown in the next couple lines \n",
        "    # already has the associated labels, so no human labeling is needed. Again, this is because \n",
        "    # we already have the labels a priori. In real scenarios, a human oracle would need to provide \n",
        "    # then before proceeding.\n",
        "    organ_amnist_train = ConcatDataset([organ_amnist_train, Subset(organ_amnist_unlabeled, idx)])\n",
        "    remaining_unlabeled_idx = list(set(range(len(organ_amnist_unlabeled))) - set(idx))\n",
        "    organ_amnist_unlabeled = Subset(organ_amnist_unlabeled, remaining_unlabeled_idx)\n",
        "\n",
        "    print('Number of training points -', len(organ_amnist_train))\n",
        "\n",
        "    # Update the data used in the AL strategy and the training class\n",
        "    strategy.update_data(organ_amnist_train, LabeledToUnlabeledDataset(organ_amnist_unlabeled))\n",
        "    dt.update_data(organ_amnist_train)\n",
        "\n",
        "    # Retrain the model and update the strategy with the result\n",
        "    clf = dt.train()\n",
        "    strategy.update_model(clf)\n",
        "\n",
        "    # Get new test accuracy\n",
        "    acc[rd] = dt.get_acc_on_set(organ_amnist_test)\n",
        "    print('Testing accuracy:', round(acc[rd]*100, 2), flush=True)\n",
        "\n",
        "print('Training Completed')\n",
        "\n",
        "# Lastly, we save the accuracies in case a comparison is warranted.\n",
        "with open(os.path.join(base_dir,'random.txt'), 'w') as f:\n",
        "    for item in acc:\n",
        "        f.write(\"%s\\n\" % item)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Testing accuracy: 75.22\n",
            "-------------------------------------------------\n",
            "Round 1\n",
            "-------------------------------------------------\n",
            "Number of training points - 500\n",
            "Training..\n",
            "Epoch: 93 Training accuracy: 0.99\n",
            "Testing accuracy: 83.47\n",
            "-------------------------------------------------\n",
            "Round 2\n",
            "-------------------------------------------------\n",
            "Number of training points - 750\n",
            "Training..\n",
            "Epoch: 84 Training accuracy: 0.993\n",
            "Testing accuracy: 83.63\n",
            "-------------------------------------------------\n",
            "Round 3\n",
            "-------------------------------------------------\n",
            "Number of training points - 1000\n",
            "Training..\n",
            "Epoch: 98 Training accuracy: 0.993\n",
            "Testing accuracy: 87.52\n",
            "-------------------------------------------------\n",
            "Round 4\n",
            "-------------------------------------------------\n",
            "Number of training points - 1250\n",
            "Training..\n",
            "Epoch: 93 Training accuracy: 0.99\n",
            "Testing accuracy: 87.45\n",
            "-------------------------------------------------\n",
            "Round 5\n",
            "-------------------------------------------------\n",
            "Number of training points - 1500\n",
            "Training..\n",
            "Epoch: 92 Training accuracy: 0.991\n",
            "Testing accuracy: 88.08\n",
            "-------------------------------------------------\n",
            "Round 6\n",
            "-------------------------------------------------\n",
            "Number of training points - 1750\n",
            "Training..\n",
            "Epoch: 75 Training accuracy: 0.993\n",
            "Testing accuracy: 89.99\n",
            "Training Completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jm_ILTinrkLk"
      },
      "source": [
        "### Entropy Sampling\n",
        "A very basic strategy to select unlabeled points is entropy sampling, where we select samples about which the model is most uncertain by measuring the entropy of the class prediction. Hence, a valid strategy is to select those points in the unlabeled set with highest entropy (maximum uncertainty). Specifically, let $z = f(x)$ be the output from the model with class $i$ having score $z_i$. By applying a softmax, we obtain probabilities that we can use: \n",
        "\n",
        "$$\\sigma(z)_i = \\frac{e^{z_i}}{\\sum_j e^{z_j}}$$ \n",
        "\n",
        "Using the softmax probabilities, entropy sampling selects a subset based on the following objective:\n",
        "\n",
        " $$\\text{argmin}_{A \\subseteq U, |A| = k} \\sum_{x \\in A} \\left[ -\\sum_j \\sigma(z)_j*\\log(\\sigma(z)_j) \\right]$$\n",
        "\n",
        "Here, we create a instance of `distil.active_learning_strategies.entropy_sampling.EntropySampling` with the same parameters passed to `distil.active_learning_strategies.random_sampling.RandomSampling`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieS3rs-TC8bE"
      },
      "source": [
        "**Reloading Base Model & Data**\n",
        "\n",
        "We make sure the fixture is the same by repeating the same setup."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwowelOwZRGJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39b52b7f-580c-4e38-d9d7-c92820c9f48f"
      },
      "source": [
        "data_set_name = 'OrganAMNIST'\n",
        "download_path = '.'\n",
        "\n",
        "# Define transforms on the dataset splits of OrganAMNIST. Here, we use random crops and horizontal flips for training augmentations.\n",
        "# Both the train and test sets are converted to PyTorch tensors and are normalized around the mean/std of OrganAMNIST.\n",
        "organ_amnist_training_transform = transforms.Compose([transforms.RandomCrop(28, padding=4), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize((average,), (std,))])\n",
        "organ_amnist_test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((average,), (std,))])\n",
        "\n",
        "# OrganAMNIST provides its labels in list form; so we need to get the original label\n",
        "target_transform = lambda x: x[0]\n",
        "\n",
        "# Get the dataset objects from MedMNIST. Here, OrganAMNIST is downloaded, and the transform is applied when points \n",
        "# are retrieved.\n",
        "organ_amnist_full_train = OrganAMNIST(root=download_path, split=\"train\", download=True, transform=organ_amnist_training_transform, target_transform=target_transform)\n",
        "organ_amnist_test = OrganAMNIST(root=download_path, split=\"test\", download=True, transform=organ_amnist_test_transform, target_transform=target_transform)\n",
        "\n",
        "# Get the dimension of the images. Here, we simply take the very first image of OrganAMNIST\n",
        "# and query its dimension.\n",
        "dim = np.shape(organ_amnist_full_train[0][0])\n",
        "\n",
        "# We now define a train-unlabeled split for the sake of the experiment. Here, we simply take 1000 points as the initial seed set.\n",
        "# The rest of the points are taken as the unlabeled set. While the unlabeled set constructed here technically has labels, they \n",
        "# are only used when querying for labels. Hence, they only exist here for the sake of experimental design.\n",
        "train_size = 250\n",
        "organ_amnist_train = Subset(organ_amnist_full_train, list(range(train_size)))\n",
        "organ_amnist_unlabeled = Subset(organ_amnist_full_train, list(range(train_size, len(organ_amnist_full_train))))\n",
        "\n",
        "# Define the number of active learning rounds to conduct, the budget, and the number of classes in OrganAMNIST\n",
        "nclasses = 11\n",
        "n_rounds = 7\n",
        "budget = 250"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using downloaded and verified file: ./organamnist.npz\n",
            "Using downloaded and verified file: ./organamnist.npz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcx_jnorZRGW"
      },
      "source": [
        "net = ResNet18(num_classes=nclasses, channels=1)\n",
        "base_dir = \"models\"\n",
        "os.makedirs(base_dir, exist_ok = True)\n",
        "model_directory = os.path.join(base_dir, 'base_model.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfMZmggtZRGX"
      },
      "source": [
        "base_dir = \"models\"\n",
        "model_directory = os.path.join(base_dir, 'base_model.pth')\n",
        "net.load_state_dict(torch.load(model_directory))\n",
        "clf = net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3C0729a2Vzg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02f47380-4e2c-4aa9-b557-9dcd76b6681c"
      },
      "source": [
        "# Initialize the random sampling AL strategy. Note: The labels are shaved off the unlabeled dataset above to match the setting.\n",
        "strategy_args = {'batch_size' : 20}\n",
        "strategy = EntropySampling(organ_amnist_train, LabeledToUnlabeledDataset(organ_amnist_unlabeled), net, nclasses, strategy_args)\n",
        "\n",
        "# Use the same training parameters as before\n",
        "args = {'n_epoch':300, 'lr':float(0.01), 'batch_size':20, 'max_accuracy':0.99, 'optimizer':'sgd'} \n",
        "dt = data_train(organ_amnist_train, clf, args)\n",
        "\n",
        "# Update the model used in the AL strategy with the loaded initial model\n",
        "strategy.update_model(clf)\n",
        "\n",
        "# Get the test accuracy of the initial model\n",
        "acc = np.zeros(n_rounds)\n",
        "acc[0] = dt.get_acc_on_set(organ_amnist_test)\n",
        "print('Initial Testing accuracy:', round(acc[0]*100, 2), flush=True)\n",
        "\n",
        "# User Controlled Loop\n",
        "for rd in range(1, n_rounds):\n",
        "    print('-------------------------------------------------')\n",
        "    print('Round', rd) \n",
        "    print('-------------------------------------------------')\n",
        "\n",
        "    # Use select() to obtain the indices in the unlabeled set that should be labeled\n",
        "    organ_amnist_full_train.transform = organ_amnist_test_transform       # Disable augmentation while selecting new points as to not interfere with the strategies\n",
        "    idx = strategy.select(budget)\n",
        "    organ_amnist_full_train.transform = organ_amnist_training_transform   # Enable augmentation\n",
        "\n",
        "    # Add the selected points to the train set. The unlabeled set shown in the next couple lines \n",
        "    # already has the associated labels, so no human labeling is needed. Again, this is because \n",
        "    # we already have the labels a priori. In real scenarios, a human oracle would need to provide \n",
        "    # then before proceeding.\n",
        "    organ_amnist_train = ConcatDataset([organ_amnist_train, Subset(organ_amnist_unlabeled, idx)])\n",
        "    remaining_unlabeled_idx = list(set(range(len(organ_amnist_unlabeled))) - set(idx))\n",
        "    organ_amnist_unlabeled = Subset(organ_amnist_unlabeled, remaining_unlabeled_idx)\n",
        "\n",
        "    print('Number of training points -', len(organ_amnist_train))\n",
        "\n",
        "    # Update the data used in the AL strategy and the training class\n",
        "    strategy.update_data(organ_amnist_train, LabeledToUnlabeledDataset(organ_amnist_unlabeled))\n",
        "    dt.update_data(organ_amnist_train)\n",
        "\n",
        "    # Retrain the model and update the strategy with the result\n",
        "    clf = dt.train()\n",
        "    strategy.update_model(clf)\n",
        "\n",
        "    # Get new test accuracy\n",
        "    acc[rd] = dt.get_acc_on_set(organ_amnist_test)\n",
        "    print('Testing accuracy:', round(acc[rd]*100, 2), flush=True)\n",
        "\n",
        "print('Training Completed')\n",
        "\n",
        "# Lastly, we save the accuracies in case a comparison is warranted.\n",
        "with open(os.path.join(base_dir,'entropy.txt'), 'w') as f:\n",
        "    for item in acc:\n",
        "        f.write(\"%s\\n\" % item)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Testing accuracy: 75.22\n",
            "-------------------------------------------------\n",
            "Round 1\n",
            "-------------------------------------------------\n",
            "Number of training points - 500\n",
            "Training..\n",
            "Epoch: 93 Training accuracy: 0.99\n",
            "Testing accuracy: 84.6\n",
            "-------------------------------------------------\n",
            "Round 2\n",
            "-------------------------------------------------\n",
            "Number of training points - 750\n",
            "Training..\n",
            "Epoch: 117 Training accuracy: 0.993\n",
            "Testing accuracy: 87.79\n",
            "-------------------------------------------------\n",
            "Round 3\n",
            "-------------------------------------------------\n",
            "Number of training points - 1000\n",
            "Training..\n",
            "Epoch: 106 Training accuracy: 0.991\n",
            "Testing accuracy: 87.28\n",
            "-------------------------------------------------\n",
            "Round 4\n",
            "-------------------------------------------------\n",
            "Number of training points - 1250\n",
            "Training..\n",
            "Epoch: 114 Training accuracy: 0.998\n",
            "Testing accuracy: 89.94\n",
            "-------------------------------------------------\n",
            "Round 5\n",
            "-------------------------------------------------\n",
            "Number of training points - 1500\n",
            "Training..\n",
            "Epoch: 112 Training accuracy: 0.992\n",
            "Testing accuracy: 90.71\n",
            "-------------------------------------------------\n",
            "Round 6\n",
            "-------------------------------------------------\n",
            "Number of training points - 1750\n",
            "Training..\n",
            "Epoch: 119 Training accuracy: 0.991\n",
            "Testing accuracy: 91.06\n",
            "Training Completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAGqAV0GrwwN"
      },
      "source": [
        "### BADGE\n",
        "This method is based on the paper [Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds](https://arxiv.org/abs/1906.03671). The strategy is meant to select points that are both diverse (e.g., their embeddings span multiple directions) and uncertain (e.g., their contribution to the loss is large). The following steps are taken:\n",
        "\n",
        "* Calculate the pseudo-label for each point in the unlabeled set. The pseudo-label is the class with the highest probability.\n",
        "* Compute the cross-entropy loss for each point in the unlabeled set using this pseudo-label.\n",
        "* Obtain the resulting loss gradients on the last linear layer of the model for each point. (These are referred to as the hypothesized loss gradients.)\n",
        "* Using these gradients as a form of embedding for each unlabeled point, run k-means++ initialization on this embedding set, retrieving $k$ centers. Each center is a point from the unlabeled set, and $k$ represents the active learning budget.\n",
        "* Request labels for the $k$ points whose embeddings were selected.\n",
        "\n",
        "Here we create a instance of `distil.active_learning_strategies.badge.BADGE` with same parameters passed to `distil.active_learning_strategies.random_sampling.RandomSampling`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Z2N1gsCGE8U"
      },
      "source": [
        "**Reloading Base Model & Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7a506L-ZqK6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5e5d53b-ad6d-4032-ef72-54e160d604f6"
      },
      "source": [
        "data_set_name = 'OrganAMNIST'\n",
        "download_path = '.'\n",
        "\n",
        "# Define transforms on the dataset splits of OrganAMNIST. Here, we use random crops and horizontal flips for training augmentations.\n",
        "# Both the train and test sets are converted to PyTorch tensors and are normalized around the mean/std of OrganAMNIST.\n",
        "organ_amnist_training_transform = transforms.Compose([transforms.RandomCrop(28, padding=4), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize((average,), (std,))])\n",
        "organ_amnist_test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((average,), (std,))])\n",
        "\n",
        "# OrganAMNIST provides its labels in list form; so we need to get the original label\n",
        "target_transform = lambda x: x[0]\n",
        "\n",
        "# Get the dataset objects from MedMNIST. Here, OrganAMNIST is downloaded, and the transform is applied when points \n",
        "# are retrieved.\n",
        "organ_amnist_full_train = OrganAMNIST(root=download_path, split=\"train\", download=True, transform=organ_amnist_training_transform, target_transform=target_transform)\n",
        "organ_amnist_test = OrganAMNIST(root=download_path, split=\"test\", download=True, transform=organ_amnist_test_transform, target_transform=target_transform)\n",
        "\n",
        "# Get the dimension of the images. Here, we simply take the very first image of OrganAMNIST\n",
        "# and query its dimension.\n",
        "dim = np.shape(organ_amnist_full_train[0][0])\n",
        "\n",
        "# We now define a train-unlabeled split for the sake of the experiment. Here, we simply take 1000 points as the initial seed set.\n",
        "# The rest of the points are taken as the unlabeled set. While the unlabeled set constructed here technically has labels, they \n",
        "# are only used when querying for labels. Hence, they only exist here for the sake of experimental design.\n",
        "train_size = 250\n",
        "organ_amnist_train = Subset(organ_amnist_full_train, list(range(train_size)))\n",
        "organ_amnist_unlabeled = Subset(organ_amnist_full_train, list(range(train_size, len(organ_amnist_full_train))))\n",
        "\n",
        "# Define the number of active learning rounds to conduct, the budget, and the number of classes in OrganAMNIST\n",
        "nclasses = 11\n",
        "n_rounds = 7\n",
        "budget = 250"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using downloaded and verified file: ./organamnist.npz\n",
            "Using downloaded and verified file: ./organamnist.npz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmENx4rgZqLB"
      },
      "source": [
        "net = ResNet18(num_classes=nclasses, channels=1)\n",
        "base_dir = \"models\"\n",
        "os.makedirs(base_dir, exist_ok = True)\n",
        "model_directory = os.path.join(base_dir, 'base_model.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OE6gmSgxZqLB"
      },
      "source": [
        "base_dir = \"models\"\n",
        "model_directory = os.path.join(base_dir, 'base_model.pth')\n",
        "net.load_state_dict(torch.load(model_directory))\n",
        "clf = net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TF4-oHDPZqLC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89f49f89-004d-475d-bf49-66494d905e98"
      },
      "source": [
        "# Initialize the random sampling AL strategy. Note: The labels are shaved off the unlabeled dataset above to match the setting.\n",
        "strategy_args = {'batch_size' : 20}\n",
        "strategy = BADGE(organ_amnist_train, LabeledToUnlabeledDataset(organ_amnist_unlabeled), net, nclasses, strategy_args)\n",
        "\n",
        "# Use the same training parameters as before\n",
        "args = {'n_epoch':300, 'lr':float(0.01), 'batch_size':20, 'max_accuracy':0.99, 'optimizer':'sgd'} \n",
        "dt = data_train(organ_amnist_train, clf, args)\n",
        "\n",
        "# Update the model used in the AL strategy with the loaded initial model\n",
        "strategy.update_model(clf)\n",
        "\n",
        "# Get the test accuracy of the initial model\n",
        "acc = np.zeros(n_rounds)\n",
        "acc[0] = dt.get_acc_on_set(organ_amnist_test)\n",
        "print('Initial Testing accuracy:', round(acc[0]*100, 2), flush=True)\n",
        "\n",
        "# User Controlled Loop\n",
        "for rd in range(1, n_rounds):\n",
        "    print('-------------------------------------------------')\n",
        "    print('Round', rd) \n",
        "    print('-------------------------------------------------')\n",
        "\n",
        "    # Use select() to obtain the indices in the unlabeled set that should be labeled\n",
        "    organ_amnist_full_train.transform = organ_amnist_test_transform       # Disable augmentation while selecting new points as to not interfere with the strategies\n",
        "    idx = strategy.select(budget)\n",
        "    organ_amnist_full_train.transform = organ_amnist_training_transform   # Enable augmentation\n",
        "\n",
        "    # Add the selected points to the train set. The unlabeled set shown in the next couple lines \n",
        "    # already has the associated labels, so no human labeling is needed. Again, this is because \n",
        "    # we already have the labels a priori. In real scenarios, a human oracle would need to provide \n",
        "    # then before proceeding.\n",
        "    organ_amnist_train = ConcatDataset([organ_amnist_train, Subset(organ_amnist_unlabeled, idx)])\n",
        "    remaining_unlabeled_idx = list(set(range(len(organ_amnist_unlabeled))) - set(idx))\n",
        "    organ_amnist_unlabeled = Subset(organ_amnist_unlabeled, remaining_unlabeled_idx)\n",
        "\n",
        "    print('Number of training points -', len(organ_amnist_train))\n",
        "\n",
        "    # Update the data used in the AL strategy and the training class\n",
        "    strategy.update_data(organ_amnist_train, LabeledToUnlabeledDataset(organ_amnist_unlabeled))\n",
        "    dt.update_data(organ_amnist_train)\n",
        "\n",
        "    # Retrain the model and update the strategy with the result\n",
        "    clf = dt.train()\n",
        "    strategy.update_model(clf)\n",
        "\n",
        "    # Get new test accuracy\n",
        "    acc[rd] = dt.get_acc_on_set(organ_amnist_test)\n",
        "    print('Testing accuracy:', round(acc[rd]*100, 2), flush=True)\n",
        "\n",
        "print('Training Completed')\n",
        "\n",
        "# Lastly, we save the accuracies in case a comparison is warranted.\n",
        "with open(os.path.join(base_dir,'badge.txt'), 'w') as f:\n",
        "    for item in acc:\n",
        "        f.write(\"%s\\n\" % item)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Testing accuracy: 75.22\n",
            "-------------------------------------------------\n",
            "Round 1\n",
            "-------------------------------------------------\n",
            "Number of training points - 500\n",
            "Training..\n",
            "Epoch: 86 Training accuracy: 0.992\n",
            "Testing accuracy: 87.31\n",
            "-------------------------------------------------\n",
            "Round 2\n",
            "-------------------------------------------------\n",
            "Number of training points - 750\n",
            "Training..\n",
            "Epoch: 117 Training accuracy: 0.995\n",
            "Testing accuracy: 87.83\n",
            "-------------------------------------------------\n",
            "Round 3\n",
            "-------------------------------------------------\n",
            "Number of training points - 1000\n",
            "Training..\n",
            "Epoch: 102 Training accuracy: 0.991\n",
            "Testing accuracy: 89.82\n",
            "-------------------------------------------------\n",
            "Round 4\n",
            "-------------------------------------------------\n",
            "Number of training points - 1250\n",
            "Training..\n",
            "Epoch: 123 Training accuracy: 0.992\n",
            "Testing accuracy: 90.95\n",
            "-------------------------------------------------\n",
            "Round 5\n",
            "-------------------------------------------------\n",
            "Number of training points - 1500\n",
            "Training..\n",
            "Epoch: 115 Training accuracy: 0.991\n",
            "Testing accuracy: 91.52\n",
            "-------------------------------------------------\n",
            "Round 6\n",
            "-------------------------------------------------\n",
            "Number of training points - 1750\n",
            "Training..\n",
            "Epoch: 128 Training accuracy: 0.992\n",
            "Testing accuracy: 91.63\n",
            "Training Completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNZjPPO7java"
      },
      "source": [
        "## Visualizing the Results\n",
        "\n",
        "If all strategies have run to completion, you can run the following cell to view the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vn65GnhZosp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "f22d19a5-cd1c-4f34-be95-06825faa5c5b"
      },
      "source": [
        "# Load the accuracies previously obtained\n",
        "with open(os.path.join(base_dir,'entropy.txt'), 'r') as f:\n",
        "  acc_ = f.readlines()\n",
        "acc_en = [round(float(x)*100, 2) for x in acc_]\n",
        "with open(os.path.join(base_dir,'badge.txt'), 'r') as f:\n",
        "  acc_ = f.readlines()\n",
        "acc_bd = [round(float(x)*100, 2) for x in acc_]\n",
        "with open(os.path.join(base_dir,'random.txt'), 'r') as f:\n",
        "  acc_ = f.readlines()\n",
        "acc_rd = [round(float(x)*100, 2) for x in acc_]\n",
        "\n",
        "# Plot them using matplotlib\n",
        "x_axis = np.array([train_size+budget*i for i in range(n_rounds)])\n",
        "plt.figure()\n",
        "plt.plot(x_axis, acc_en, 'g-', label='UNCERTAINTY',marker='o')\n",
        "plt.plot(x_axis, acc_bd, 'c', label='BADGE',marker='o')\n",
        "plt.plot(x_axis, acc_rd, 'r', label='RANDOM',marker='o')\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel('No of Images')\n",
        "plt.ylabel('Test Accuracy')\n",
        "plt.title('DISTIL_OrganAMNIST')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'DISTIL_OrganAMNIST')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}