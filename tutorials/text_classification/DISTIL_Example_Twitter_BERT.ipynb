{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWDkK4FivCYi"
      },
      "source": [
        "# **DISTIL Usage Example: Twitter**\n",
        "\n",
        "Here, we show how to use DISTIL to perform active learning on sentiment analysis tasks using a BERT model with a Twitter Sentiment dataset. This notebook can be easily executed on Google Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKOdPxKjvJZh"
      },
      "source": [
        "## Installation and Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AM3__Y78sl-B",
        "outputId": "56017574-7afb-4f90-9998-bdf2c8654aed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.8/dist-packages (22.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdd3bm-eu7ry",
        "outputId": "07f4cdc4-37b3-416d-bb56-5807161e0a3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'distil' already exists and is not an empty directory.\n",
            "Already on 'nlp_tutorials'\n",
            "Your branch is up to date with 'origin/nlp_tutorials'.\n",
            "Looking in indexes: https://test.pypi.org/simple/, https://pypi.org/simple/\n",
            "Requirement already satisfied: sphinxcontrib-bibtex>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from -r distil/requirements/requirements.txt (line 1)) (2.5.0)\n",
            "Requirement already satisfied: multipledispatch==0.6.0 in /usr/local/lib/python3.8/dist-packages (from -r distil/requirements/requirements.txt (line 2)) (0.6.0)\n",
            "Requirement already satisfied: scikit-learn==0.23.0 in /usr/local/lib/python3.8/dist-packages (from -r distil/requirements/requirements.txt (line 3)) (0.23.0)\n",
            "Requirement already satisfied: numpy>=1.14.2 in /usr/local/lib/python3.8/dist-packages (from -r distil/requirements/requirements.txt (line 4)) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from -r distil/requirements/requirements.txt (line 5)) (1.7.3)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from -r distil/requirements/requirements.txt (line 6)) (1.13.1+cu116)\n",
            "Requirement already satisfied: tqdm>=4.24.0 in /usr/local/lib/python3.8/dist-packages (from -r distil/requirements/requirements.txt (line 7)) (4.64.1)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.8/dist-packages (from -r distil/requirements/requirements.txt (line 8)) (0.56.4)\n",
            "Requirement already satisfied: submodlib in /usr/local/lib/python3.8/dist-packages (from -r distil/requirements/requirements.txt (line 11)) (1.1.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from -r distil/requirements/requirements.txt (line 12)) (1.3.5)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from -r distil/requirements/requirements.txt (line 13)) (0.14.1+cu116)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from multipledispatch==0.6.0->-r distil/requirements/requirements.txt (line 2)) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn==0.23.0->-r distil/requirements/requirements.txt (line 3)) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn==0.23.0->-r distil/requirements/requirements.txt (line 3)) (3.1.0)\n",
            "Requirement already satisfied: pybtex>=0.24 in /usr/local/lib/python3.8/dist-packages (from sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (0.24.0)\n",
            "Requirement already satisfied: pybtex-docutils>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (1.0.2)\n",
            "Requirement already satisfied: docutils>=0.8 in /usr/local/lib/python3.8/dist-packages (from sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (0.16)\n",
            "Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.8/dist-packages (from sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (6.0.0)\n",
            "Requirement already satisfied: Sphinx>=2.1 in /usr/local/lib/python3.8/dist-packages (from sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (3.5.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.4.0->-r distil/requirements/requirements.txt (line 6)) (4.4.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba>=0.43.0->-r distil/requirements/requirements.txt (line 8)) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from numba>=0.43.0->-r distil/requirements/requirements.txt (line 8)) (57.4.0)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.8/dist-packages (from submodlib->-r distil/requirements/requirements.txt (line 11)) (0.0.post2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->-r distil/requirements/requirements.txt (line 12)) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->-r distil/requirements/requirements.txt (line 12)) (2.8.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->-r distil/requirements/requirements.txt (line 13)) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision->-r distil/requirements/requirements.txt (line 13)) (2.25.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=3.6->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (3.12.0)\n",
            "Requirement already satisfied: latexcodec>=1.0.4 in /usr/local/lib/python3.8/dist-packages (from pybtex>=0.24->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (2.0.1)\n",
            "Requirement already satisfied: PyYAML>=3.01 in /usr/local/lib/python3.8/dist-packages (from pybtex>=0.24->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (6.0)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.8/dist-packages (from Sphinx>=2.1->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (2.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from Sphinx>=2.1->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (23.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.8/dist-packages (from Sphinx>=2.1->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (2.6.1)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.8/dist-packages (from Sphinx>=2.1->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (0.7.13)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp in /usr/local/lib/python3.8/dist-packages (from Sphinx>=2.1->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (2.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.8/dist-packages (from Sphinx>=2.1->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (1.1.5)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.8/dist-packages (from Sphinx>=2.1->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (1.0.4)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.8/dist-packages (from Sphinx>=2.1->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (1.0.2)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.8/dist-packages (from Sphinx>=2.1->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (2.11.3)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.8/dist-packages (from Sphinx>=2.1->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (2.11.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.8/dist-packages (from Sphinx>=2.1->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (1.0.1)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.8/dist-packages (from Sphinx>=2.1->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.8/dist-packages (from Sphinx>=2.1->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (1.0.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->-r distil/requirements/requirements.txt (line 13)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->-r distil/requirements/requirements.txt (line 13)) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->-r distil/requirements/requirements.txt (line 13)) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->-r distil/requirements/requirements.txt (line 13)) (4.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from Jinja2>=2.3->Sphinx>=2.1->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (2.0.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.12.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.8/dist-packages (2.9.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2023.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.12.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.25.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (23.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "# Get DISTIL and submodlib\n",
        "!git clone https://github.com/decile-team/distil.git\n",
        "\n",
        "# REPLACE WHEN FINISHED TESTING\n",
        "!cd distil && git checkout nlp_tutorials\n",
        "\n",
        "# Required installations\n",
        "!pip install -r distil/requirements/requirements.txt\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "\n",
        "# Standard imports\n",
        "import os\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import string\n",
        "import unicodedata\n",
        "import math\n",
        "\n",
        "# Imports for datasets\n",
        "from datasets import load_dataset, load_metric, concatenate_datasets\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset, Subset, ConcatDataset\n",
        "from collections import Counter\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Imports for active learning\n",
        "sys.path.append('distil/')\n",
        "from distil.active_learning_strategies import BADGE, EntropySampling, LeastConfidenceSampling, MarginSampling, RandomSampling   # All active learning strategies showcased in this example\n",
        "from distil.utils.utils import LabeledToUnlabeledDataset                        # A utility wrapper class that removes labels from labeled PyTorch dataset objects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qu7M_wcc8PJ"
      },
      "source": [
        "## Preparing tweet_eval\n",
        "\n",
        "tweet_eval is a dataset from Hugging Face that is used for sentiment analysis/sentiment classification. The dataset consists of 59899 English tweets from Twitter users and is split into 3 splits for training (45615), validation (2000), and testing (12284). The labels are as follows: 0 = negative, 1 = neutral, 2 = positive. The dataset can be found here: https://huggingface.co/datasets/tweet_eval\n",
        "\n",
        "Here we preprocess the poem verses by making all words lowercase, removing punctuation, converting to ASCII format, and removing stopwards. Stopwords are commonly used words in the English language that usually do not provide any useful information for sentiment classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bG_yAvtPxRlo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc54a025-d46a-4481-bea9-7706bc204f4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset tweet_eval (/root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n",
            "WARNING:datasets.builder:Found cached dataset tweet_eval (/root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-9642c90231075bed.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-aad948fd6d1a9b85.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-8d66e21073d341c9.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-074821155257838e.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-efcff37138bd934c.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-2276388c7af259c7.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-de36b5493c1aff7d.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-0bacefc92dafa711.arrow\n"
          ]
        }
      ],
      "source": [
        "# Converts all text to lowercase\n",
        "def to_lower(ex):\n",
        "    ex['text'] = ex['text'].lower()\n",
        "    return ex\n",
        "\n",
        "# Removes punctuation from the text\n",
        "def remove_punc(ex):\n",
        "    ex['text'] = ''.join(x for x in ex['text'] if x not in string.punctuation)\n",
        "    return ex\n",
        "\n",
        "# Converts text to ASCII (also converts accented text to normal)\n",
        "def to_ascii(ex):\n",
        "    ex['text'] = ''.join(x for x in unicodedata.normalize('NFD',ex['text']) if unicodedata.category(x) != 'Mn')\n",
        "    return ex\n",
        "\n",
        "# List of stopwords (commonly used words in English)\n",
        "stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \n",
        "             \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\",\n",
        "             \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \n",
        "             \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\",\n",
        "             \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\",\n",
        "             \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \n",
        "             \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\",\n",
        "             \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\",\n",
        "             \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\",\n",
        "             \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\",\n",
        "             \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
        "\n",
        "# Removes stopwords\n",
        "def remove_stopwords(ex):\n",
        "    ex['text'] = ' '.join(x for x in ex['text'].split() if x not in stopwords)\n",
        "    return ex\n",
        "\n",
        "# Process the data by making all text lowercase, removing puncuation, converting to ascii, and removing stopwords\n",
        "def process_data(data):\n",
        "    data1 = data.map(remove_punc)\n",
        "    data2 = data1.map(to_ascii)\n",
        "    data3 = data2.map(to_lower)\n",
        "    data4 = data3.map(remove_stopwords)\n",
        "    return data1\n",
        "\n",
        "# 0 = negative, 1 = positive, 2 = no impact, 3 = mixed (both positive and negative)\n",
        "train_data = load_dataset('tweet_eval', 'sentiment', split='train')\n",
        "test_data = load_dataset('tweet_eval', 'sentiment', split='test')\n",
        "\n",
        "# resulting data splits after processing the data\n",
        "training_data = process_data(train_data)\n",
        "testing_data = process_data(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Idu4OvTNc_no"
      },
      "source": [
        "## Tokenization and Pre-Active Learning\n",
        "\n",
        "Here we tokenize the poem verses using the transformers' library AutoTokenizer with the bert-base-uncased configuration. We then make sure that the datasets are of the torch format, separate the training dataset into an initial training subset and remaining unlabeled subset, and define the number of active learning rounds, the budget, and the number of classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Srdr5o0gxVUS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6f06a55-c7f3-4739-d850-e454e6918a10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-5e6d87e2ee031e39.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-791a24d1773be097.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 200, 1: 200, 2: 200}\n",
            "{0: 1000, 1: 1000, 2: 1000}\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, get_scheduler, BertConfig, AdamW, BertModel\n",
        "\n",
        "configuration = BertConfig()\n",
        "setattr(configuration, 'l1', 512)\n",
        "setattr(configuration, 'num_classes', 3)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def tokenize_func(ex):\n",
        "    return tokenizer(ex['text'], padding='max_length', truncation=True)\n",
        "\n",
        "# Removes unnecessary columns and renames columns to match the model specification\n",
        "train1 = training_data.map(tokenize_func)\n",
        "test1 = testing_data.map(tokenize_func)\n",
        "train2 = train1.remove_columns(\"text\")\n",
        "test2 = test1.remove_columns(\"text\")\n",
        "training_dataset = train2.rename_column(\"label\", \"labels\")\n",
        "testing_dataset = test2.rename_column(\"label\", \"labels\")\n",
        "\n",
        "training_dataset.set_format(\"torch\")\n",
        "testing_dataset.set_format(\"torch\")\n",
        "\n",
        "# Define the number of active learning rounds to conduct, the budget, and the number of classes in twitter dataset\n",
        "nclasses = 3\n",
        "n_rounds = 3\n",
        "budget = 300\n",
        "\n",
        "train_size_pc     = 200\n",
        "test_size_pc      = 1000\n",
        "unlabeled_size_pc = 5000\n",
        "\n",
        "# Ensure reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate random index lists for subset creation\n",
        "full_train_subset = []\n",
        "full_unlabeled_subset = []\n",
        "full_test_subset = []\n",
        "\n",
        "for class_num in range(nclasses):\n",
        "    train_idx_pool = []\n",
        "    for train_idx in range(len(training_dataset)):\n",
        "        if training_dataset[train_idx][\"labels\"].item() == class_num:\n",
        "            train_idx_pool.append(train_idx)\n",
        "    test_idx_pool = []\n",
        "    for test_idx in range(len(testing_dataset)):\n",
        "        if testing_dataset[test_idx][\"labels\"].item() == class_num:\n",
        "            test_idx_pool.append(test_idx)\n",
        "\n",
        "    # Get a random subset from each pool\n",
        "    train_idx_subset = np.random.choice(train_idx_pool, size=train_size_pc, replace = False).tolist()\n",
        "    remaining_idx = list(set(train_idx_pool) - set(train_idx_subset))\n",
        "    unlabeled_idx_subset = np.random.choice(remaining_idx, size=unlabeled_size_pc, replace = False).tolist()\n",
        "    test_idx_subset = np.random.choice(test_idx_pool, size=test_size_pc, replace = False).tolist()\n",
        "    full_train_subset.extend(train_idx_subset)\n",
        "    full_unlabeled_subset.extend(unlabeled_idx_subset)\n",
        "    full_test_subset.extend(test_idx_subset)\n",
        "\n",
        "tweet_train = Subset(training_dataset, full_train_subset)\n",
        "tweet_unlabeled = Subset(training_dataset, full_unlabeled_subset)\n",
        "testing_dataset = Subset(testing_dataset, full_test_subset)\n",
        "\n",
        "label_count = {0:0,1:0,2:0}\n",
        "for i in range(len(tweet_train)):\n",
        "    the_label = tweet_train[i][\"labels\"].item()\n",
        "    label_count[the_label] += 1\n",
        "print(label_count)\n",
        "\n",
        "label_count = {0:0,1:0,2:0}\n",
        "for i in range(len(testing_dataset)):\n",
        "    the_label = testing_dataset[i][\"labels\"].item()\n",
        "    label_count[the_label] += 1\n",
        "print(label_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQvEY9yx2afX"
      },
      "source": [
        "## Preparing the Model\n",
        "\n",
        "The model here is an CustomBERTModel based on https://arxiv.org/abs/1810.04805. It uses a bidirectional tranformer scheme that pretrains based on masked language modeling and next sentence prediction. The model we use is derivated from HuggingFace's transformers library's Bert Model that can be found here: https://huggingface.co/docs/transformers/model_doc/bert."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27oBcNfaxoTB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "706132b0-618f-477e-8b19-5693858358f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at prajjwal1/bert-small were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CustomBERTModel(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 512, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 512)\n",
              "      (token_type_embeddings): Embedding(2, 512)\n",
              "      (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
              "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
              "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
              "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
              "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (linear1): Linear(in_features=512, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "from transformers import BertModel\n",
        "\n",
        "class CustomBERTModel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(CustomBERTModel, self).__init__()\n",
        "        self.config = config\n",
        "        self.bert = BertModel.from_pretrained(\"prajjwal1/bert-small\")\n",
        "        classifier_dropout = (config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob)\n",
        "        self.dropout = nn.Dropout(classifier_dropout)\n",
        "        self.linear1 = nn.Linear(512,config.num_classes)\n",
        "        \n",
        "    def get_embedding_dim(self):\n",
        "        return 512\n",
        "\n",
        "    def forward(self, input_ids=None,\n",
        "               attention_mask=None,\n",
        "               token_type_ids=None,\n",
        "               position_ids=None,\n",
        "               head_mask=None,\n",
        "               inputs_embeds=None,\n",
        "               labels=None,\n",
        "               output_attentions=None,\n",
        "               output_hidden_states=None,\n",
        "               return_dict=None, freeze=False, last=False):\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "        if freeze:\n",
        "            with torch.no_grad():\n",
        "                outputs = self.bert(input_ids,\n",
        "                                    attention_mask=attention_mask,\n",
        "                                    token_type_ids=token_type_ids,\n",
        "                                    position_ids=position_ids,\n",
        "                                    head_mask=head_mask,\n",
        "                                    inputs_embeds=inputs_embeds,\n",
        "                                    output_attentions=output_attentions,\n",
        "                                    output_hidden_states=output_hidden_states,\n",
        "                                    return_dict=return_dict)\n",
        "                pooled_output = self.dropout(outputs[\"pooler_output\"])\n",
        "        else:\n",
        "            outputs = self.bert(input_ids,\n",
        "                                attention_mask=attention_mask,\n",
        "                                token_type_ids=token_type_ids,\n",
        "                                position_ids=position_ids,\n",
        "                                head_mask=head_mask,\n",
        "                                inputs_embeds=inputs_embeds,\n",
        "                                output_attentions=output_attentions,\n",
        "                                output_hidden_states=output_hidden_states,\n",
        "                                return_dict=return_dict)\n",
        "            pooled_output = self.dropout(outputs[\"pooler_output\"])\n",
        "        linear1_output = self.linear1(pooled_output)\n",
        "        if last:\n",
        "            return linear1_output, pooled_output\n",
        "        else:\n",
        "            return linear1_output\n",
        "\n",
        "base_dir = \"models\"\n",
        "os.makedirs(base_dir, exist_ok = True)\n",
        "model_directory = os.path.join(base_dir, 'base_model.pth')\n",
        "\n",
        "model = CustomBERTModel(configuration)\n",
        "if(torch.cuda.is_available()):\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vr9D4qmG4zRa"
      },
      "source": [
        "## data_train Class\n",
        "\n",
        "Here we define a data_train class modeled after the data_train class already included in distil/distil/active_learning_stratgies. However, the following data_train class is designed to work with the CustomBERTModel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSndS4i8xz7J"
      },
      "outputs": [],
      "source": [
        "#custom training\n",
        "class data_train:\n",
        "    \n",
        "    def __init__(self, training_dataset, net, args):\n",
        "\n",
        "        self.training_dataset = training_dataset\n",
        "        self.net = net\n",
        "        self.args = args\n",
        "        \n",
        "        self.n_pool = len(training_dataset)\n",
        "        \n",
        "        if 'islogs' not in args:\n",
        "            self.args['islogs'] = False\n",
        "\n",
        "        if 'optimizer' not in args:\n",
        "            self.args['optimizer'] = 'adam'\n",
        "        \n",
        "        if 'isverbose' not in args:\n",
        "            self.args['isverbose'] = False\n",
        "        \n",
        "        if 'isreset' not in args:\n",
        "            self.args['isreset'] = True\n",
        "\n",
        "        if 'n_epoch' not in args:\n",
        "            self.args['n_epoch'] = 200\n",
        "            \n",
        "        if 'criterion' not in args:\n",
        "            self.args['criterion'] = nn.CrossEntropyLoss()\n",
        "            \n",
        "        if 'device' not in args:\n",
        "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        else:\n",
        "            self.device = args['device']\n",
        "\n",
        "    def update_index(self, idxs_lb):\n",
        "        self.idxs_lb = idxs_lb\n",
        "\n",
        "    def update_data(self, new_training_dataset):\n",
        "        \"\"\"\n",
        "        Updates the training dataset with the provided new training dataset\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        new_training_dataset: torch.utils.data.Dataset\n",
        "            The new training dataset\n",
        "        \"\"\"\n",
        "        self.training_dataset = new_training_dataset\n",
        "\n",
        "    def get_acc_on_set(self, test_dataset):\n",
        "        \n",
        "        \"\"\"\n",
        "        Calculates and returns the accuracy on the given dataset to test\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        test_dataset: torch.utils.data.Dataset\n",
        "            The dataset to test\n",
        "        Returns\n",
        "        -------\n",
        "        accFinal: float\n",
        "            The fraction of data points whose predictions by the current model match their targets\n",
        "        \"\"\"\t\n",
        "        \n",
        "        try:\n",
        "            self.clf\n",
        "        except:\n",
        "            self.clf = self.net\n",
        "\n",
        "        if test_dataset is None:\n",
        "            raise ValueError(\"Test data not present\")\n",
        "        \n",
        "        if 'batch_size' in self.args:\n",
        "            batch_size = self.args['batch_size']\n",
        "        else:\n",
        "            batch_size = 1 \n",
        " \n",
        "        loader_te = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n",
        "        self.clf.eval()\n",
        "        accFinal = 0.\n",
        "\n",
        "        self.clf = self.clf.to(device=self.device)\n",
        "        for batch in loader_te:  \n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            with torch.no_grad():\n",
        "              out = self.clf(**batch)\n",
        "              accFinal += torch.sum(1.0*(torch.max(out,1)[1] == batch['labels'].view(-1))).item()\n",
        "\n",
        "        return accFinal / len(test_dataset)\n",
        "\n",
        "    def _train(self, epoch, loader_tr, optimizer):\n",
        "        self.clf.train()\n",
        "        accFinal = 0.\n",
        "        criterion = self.args['criterion']\n",
        "        criterion.reduction = \"mean\"\n",
        "\n",
        "        for batch in loader_tr:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            out = self.clf(**batch, freeze=True)\n",
        "            loss = criterion(out, batch['labels'].view(-1))\n",
        "            accFinal += torch.sum((torch.max(out,1)[1] == batch['labels'].view(-1)).float()).item()\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # clamp gradients, just in case\n",
        "            # for p in filter(lambda p: p.grad is not None, self.clf.parameters()): p.grad.data.clamp_(min=-.1, max=.1)\n",
        "\n",
        "        return accFinal / len(loader_tr.dataset), loss\n",
        "\n",
        "    def train(self, gradient_weights=None):\n",
        "\n",
        "        print('Training..')\n",
        "        train_logs = []\n",
        "        \n",
        "        try:\n",
        "            self.clf\n",
        "        except:\n",
        "            self.clf = self.net\n",
        "        \n",
        "        if self.args['isreset']:\n",
        "            self.clf.linear1.reset_parameters()\n",
        "        \n",
        "        if 'batch_size' in self.args:\n",
        "            batch_size = self.args['batch_size']\n",
        "        else:\n",
        "            batch_size = 1\n",
        "\n",
        "        loader_tr = DataLoader(self.training_dataset, shuffle=True, batch_size=batch_size)\n",
        "        epoch = 1\n",
        "        accCurrent = 0\n",
        "\n",
        "        if self.args['optimizer'] == 'adam':\n",
        "            optimizer = AdamW(self.clf.parameters(), lr = self.args['lr'])\n",
        "\n",
        "        while epoch < self.args['n_epoch']: \n",
        "            \n",
        "            accCurrent, lossCurrent = self._train(epoch, loader_tr, optimizer)\n",
        "            \n",
        "            epoch += 1\n",
        "            if epoch % 5 == 0:\n",
        "                print(str(epoch) + ' training accuracy: ' + str(accCurrent), flush=True)\n",
        "\n",
        "            log_string = 'Epoch:' + str(epoch) + '- training accuracy:'+str(accCurrent)+'- training loss:'+str(lossCurrent)\n",
        "            train_logs.append(log_string)\n",
        "\n",
        "        print('Epoch:', str(epoch), 'Training accuracy:', round(accCurrent, 3), flush=True)\n",
        "\n",
        "        if self.args['islogs']:\n",
        "            return self.clf, train_logs\n",
        "        else:\n",
        "            return self.clf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qepVUGu5AU1"
      },
      "source": [
        "## Training an Initial Model\n",
        "\n",
        "Here we train an initial model with the number of epochs as 15, the learning rate as 5e-4, the batch size of 5, max accuracy of 0.99 and the adam optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtpRisakx6_C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f788f029-e6bd-4c37-9012-29be1e1ed48b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 training accuracy: 0.475\n",
            "10 training accuracy: 0.5166666666666667\n",
            "15 training accuracy: 0.5516666666666666\n",
            "20 training accuracy: 0.585\n",
            "25 training accuracy: 0.5933333333333334\n",
            "30 training accuracy: 0.5766666666666667\n",
            "35 training accuracy: 0.595\n",
            "40 training accuracy: 0.595\n",
            "45 training accuracy: 0.6066666666666667\n",
            "50 training accuracy: 0.5933333333333334\n",
            "55 training accuracy: 0.585\n",
            "60 training accuracy: 0.6216666666666667\n",
            "65 training accuracy: 0.6033333333333334\n",
            "70 training accuracy: 0.6166666666666667\n",
            "75 training accuracy: 0.615\n",
            "80 training accuracy: 0.6216666666666667\n",
            "85 training accuracy: 0.6233333333333333\n",
            "90 training accuracy: 0.645\n",
            "95 training accuracy: 0.645\n",
            "100 training accuracy: 0.6283333333333333\n",
            "105 training accuracy: 0.6483333333333333\n",
            "110 training accuracy: 0.6166666666666667\n",
            "115 training accuracy: 0.6216666666666667\n",
            "120 training accuracy: 0.6216666666666667\n",
            "125 training accuracy: 0.6433333333333333\n",
            "130 training accuracy: 0.6466666666666666\n",
            "135 training accuracy: 0.6316666666666667\n",
            "140 training accuracy: 0.6216666666666667\n",
            "145 training accuracy: 0.6483333333333333\n",
            "150 training accuracy: 0.6716666666666666\n",
            "155 training accuracy: 0.6816666666666666\n",
            "160 training accuracy: 0.6383333333333333\n",
            "165 training accuracy: 0.6566666666666666\n",
            "170 training accuracy: 0.66\n",
            "175 training accuracy: 0.6583333333333333\n",
            "180 training accuracy: 0.645\n",
            "185 training accuracy: 0.6533333333333333\n",
            "190 training accuracy: 0.665\n",
            "195 training accuracy: 0.64\n",
            "200 training accuracy: 0.6833333333333333\n",
            "Epoch: 200 Training accuracy: 0.683\n"
          ]
        }
      ],
      "source": [
        "args = {'lr':1e-4, 'batch_size':5, 'n_epoch': 200, 'optimizer':'adam'} \n",
        "dt = data_train(tweet_train, model, args)\n",
        "clf = dt.train()\n",
        "torch.save(clf.state_dict(), model_directory)\n",
        "\n",
        "model = CustomBERTModel(configuration)\n",
        "state_dict = torch.load(model_directory)\n",
        "model.load_state_dict(state_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAhhaQCx5ISu"
      },
      "source": [
        "## Active Learning Strategies\n",
        "\n",
        "Here, we show some active learning strategies being used for sentiment analysis with the poem_sentiment dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQyfgZ_E5JIf"
      },
      "source": [
        "### Random Sampling\n",
        "This strategy is often used as a baseline, where we pick a subset of unlabeled points randomly. Here we create a instance of distil.active_learning_strategies.random_sampling.RandomSampling by passing following parameters:\n",
        "\n",
        "**training_dataset** – The labeled dataset\n",
        "\n",
        "**unlabeled_dataset** – The unlabeled dataset, which has a wrapper around it that strips the label\n",
        "\n",
        "**net (class object)** – Model architecture used for training. Could be instance of models defined in distil.utils.models or something similar.\n",
        "\n",
        "**nclasses (int)** – No. of classes in tha dataset\n",
        "\n",
        "**args (dictionary)**– This dictionary should have ‘batch_size’ as a key. 'batch_size' should be such that one can exploit the benefits of tensorization while honouring the resourse constraits. This ‘batch_size’ therefore can be different than the one used for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZoCUllg0a46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e0851a6-cdb3-45ba-a025-36f7eb7dd998"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Testing accuracy: 57.83\n",
            "-------------------------------------------------\n",
            "Round 1\n",
            "-------------------------------------------------\n",
            "Number of training points - 900\n",
            "Training..\n",
            "5 training accuracy: 0.48\n",
            "10 training accuracy: 0.5355555555555556\n",
            "15 training accuracy: 0.56\n",
            "20 training accuracy: 0.5855555555555556\n",
            "25 training accuracy: 0.58\n",
            "30 training accuracy: 0.5722222222222222\n",
            "35 training accuracy: 0.6088888888888889\n",
            "40 training accuracy: 0.5733333333333334\n",
            "45 training accuracy: 0.6\n",
            "50 training accuracy: 0.5966666666666667\n",
            "55 training accuracy: 0.5877777777777777\n",
            "60 training accuracy: 0.6266666666666667\n",
            "65 training accuracy: 0.6277777777777778\n",
            "70 training accuracy: 0.6344444444444445\n",
            "75 training accuracy: 0.6066666666666667\n",
            "80 training accuracy: 0.6033333333333334\n",
            "85 training accuracy: 0.64\n",
            "90 training accuracy: 0.6033333333333334\n",
            "95 training accuracy: 0.6366666666666667\n",
            "100 training accuracy: 0.6211111111111111\n",
            "105 training accuracy: 0.6144444444444445\n",
            "110 training accuracy: 0.6211111111111111\n",
            "115 training accuracy: 0.6366666666666667\n",
            "120 training accuracy: 0.6222222222222222\n",
            "125 training accuracy: 0.6111111111111112\n",
            "130 training accuracy: 0.6522222222222223\n",
            "135 training accuracy: 0.6388888888888888\n",
            "140 training accuracy: 0.6255555555555555\n",
            "145 training accuracy: 0.64\n",
            "150 training accuracy: 0.6244444444444445\n",
            "155 training accuracy: 0.6433333333333333\n",
            "160 training accuracy: 0.6188888888888889\n",
            "165 training accuracy: 0.6288888888888889\n",
            "170 training accuracy: 0.6288888888888889\n",
            "175 training accuracy: 0.6466666666666666\n",
            "180 training accuracy: 0.6444444444444445\n",
            "185 training accuracy: 0.64\n",
            "190 training accuracy: 0.6277777777777778\n",
            "195 training accuracy: 0.6455555555555555\n",
            "200 training accuracy: 0.6555555555555556\n",
            "Epoch: 200 Training accuracy: 0.656\n",
            "Testing accuracy: 58.87\n",
            "-------------------------------------------------\n",
            "Round 2\n",
            "-------------------------------------------------\n",
            "Number of training points - 1200\n",
            "Training..\n",
            "5 training accuracy: 0.47583333333333333\n",
            "10 training accuracy: 0.5291666666666667\n",
            "15 training accuracy: 0.5391666666666667\n",
            "20 training accuracy: 0.5558333333333333\n",
            "25 training accuracy: 0.5633333333333334\n",
            "30 training accuracy: 0.5791666666666667\n",
            "35 training accuracy: 0.585\n",
            "40 training accuracy: 0.5841666666666666\n",
            "45 training accuracy: 0.5916666666666667\n",
            "50 training accuracy: 0.5808333333333333\n",
            "55 training accuracy: 0.5858333333333333\n",
            "60 training accuracy: 0.585\n",
            "65 training accuracy: 0.6058333333333333\n",
            "70 training accuracy: 0.6025\n",
            "75 training accuracy: 0.6033333333333334\n",
            "80 training accuracy: 0.6125\n",
            "85 training accuracy: 0.6091666666666666\n",
            "90 training accuracy: 0.6075\n",
            "95 training accuracy: 0.6091666666666666\n",
            "100 training accuracy: 0.6008333333333333\n",
            "105 training accuracy: 0.6183333333333333\n",
            "110 training accuracy: 0.5966666666666667\n",
            "115 training accuracy: 0.615\n",
            "120 training accuracy: 0.6058333333333333\n",
            "125 training accuracy: 0.6258333333333334\n",
            "130 training accuracy: 0.6216666666666667\n",
            "135 training accuracy: 0.6075\n",
            "140 training accuracy: 0.6025\n",
            "145 training accuracy: 0.6225\n",
            "150 training accuracy: 0.6191666666666666\n",
            "155 training accuracy: 0.6141666666666666\n",
            "160 training accuracy: 0.6383333333333333\n",
            "165 training accuracy: 0.6375\n",
            "170 training accuracy: 0.625\n",
            "175 training accuracy: 0.6191666666666666\n",
            "180 training accuracy: 0.6258333333333334\n",
            "185 training accuracy: 0.615\n",
            "190 training accuracy: 0.6233333333333333\n",
            "195 training accuracy: 0.6208333333333333\n",
            "200 training accuracy: 0.6191666666666666\n",
            "Epoch: 200 Training accuracy: 0.619\n",
            "Testing accuracy: 58.27\n",
            "Training Completed\n"
          ]
        }
      ],
      "source": [
        "strategy_args = {'batch_size' : 5}\n",
        "strategy = RandomSampling(tweet_train, tweet_unlabeled, model, nclasses, strategy_args)\n",
        "\n",
        "# Update the model used in the AL strategy with the loaded initial model\n",
        "strategy.update_model(model)\n",
        "\n",
        "# Get the test accuracy of the initial model\n",
        "acc = np.zeros(n_rounds)\n",
        "acc[0] = dt.get_acc_on_set(testing_dataset)\n",
        "print('Initial Testing accuracy:', round(acc[0]*100, 2), flush=True)\n",
        "\n",
        "# User Controlled Loop\n",
        "for rd in range(1, n_rounds):\n",
        "    print('-------------------------------------------------')\n",
        "    print('Round', rd) \n",
        "    print('-------------------------------------------------')\n",
        "\n",
        "    # Use select() to obtain the indices in the unlabeled set that should be labeled\n",
        "    idx = strategy.select(budget)\n",
        "\n",
        "    # Add the selected points to the train set. The unlabeled set shown in the next couple lines \n",
        "    # already has the associated labels, so no human labeling is needed. Again, this is because \n",
        "    # we already have the labels a priori. In real scenarios, a human oracle would need to provide \n",
        "    # then before proceeding.\n",
        "    tweet_train = ConcatDataset([tweet_train, Subset(tweet_unlabeled, idx)])\n",
        "    remaining_unlabeled_idx = list(set(range(len(tweet_unlabeled))) - set(idx))\n",
        "    tweet_unlabeled = Subset(tweet_unlabeled, remaining_unlabeled_idx)\n",
        "\n",
        "    print('Number of training points -', len(tweet_train))\n",
        "\n",
        "    # Update the data used in the AL strategy and the training class\n",
        "    strategy.update_data(tweet_train, tweet_unlabeled)\n",
        "    dt.update_data(tweet_train)\n",
        "\n",
        "    # Retrain the model and update the strategy with the result\n",
        "    clf = dt.train()\n",
        "    strategy.update_model(clf)\n",
        "\n",
        "    # Get new test accuracy\n",
        "    acc[rd] = dt.get_acc_on_set(testing_dataset)\n",
        "    print('Testing accuracy:', round(acc[rd]*100, 2), flush=True)\n",
        "\n",
        "print('Training Completed')\n",
        "\n",
        "#Saving accuracies for further analysis\n",
        "with open(os.path.join(base_dir,'random.txt'), 'w') as f:\n",
        "    for item in acc:\n",
        "        f.write(\"%s\\n\" % item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "324UVK1e5SBu"
      },
      "source": [
        "### Reload Data\n",
        "\n",
        "Here we reload, reprocess, and retokenize the data and prepare for a different active learning strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVQohcf40clB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329,
          "referenced_widgets": [
            "3b49280313144a40ac6a12f9abb5988e",
            "de852474beab40b58cbdd77e7dd7171c",
            "a059a3cf23af423da59429220d4b4d46",
            "a47c6bb568284c5e8b6fa462cb8bbfd3",
            "8ec7969ba8974b0abe79f858049283a4",
            "94425dc1c3c84da1a8864eb715d5c472",
            "c380161aabd2477ca16503c3bab06cb5",
            "e6f70b78a62d4f46ac73f90d00783e86",
            "36280274ea9846d290081bf93f439d91",
            "a6b184399862473b9d140229614ce2c0",
            "698f98eab782407ebbc5fb0871de6d62"
          ]
        },
        "outputId": "fd71f8e3-0cc6-41d1-84a0-28bc62d5e986"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset tweet_eval (/root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n",
            "WARNING:datasets.builder:Found cached dataset tweet_eval (/root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-9642c90231075bed.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-aad948fd6d1a9b85.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-8d66e21073d341c9.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-074821155257838e.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-efcff37138bd934c.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-2276388c7af259c7.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-de36b5493c1aff7d.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-0bacefc92dafa711.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-5e6d87e2ee031e39.arrow\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/12284 [00:00<?, ?ex/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3b49280313144a40ac6a12f9abb5988e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at prajjwal1/bert-small were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Converts all text to lowercase\n",
        "def to_lower(ex):\n",
        "    ex['text'] = ex['text'].lower()\n",
        "    return ex\n",
        "\n",
        "# Removes punctuation from the text\n",
        "def remove_punc(ex):\n",
        "    ex['text'] = ''.join(x for x in ex['text'] if x not in string.punctuation)\n",
        "    return ex\n",
        "\n",
        "# Converts text to ASCII (also converts accented text to normal)\n",
        "def to_ascii(ex):\n",
        "    ex['text'] = ''.join(x for x in unicodedata.normalize('NFD',ex['text']) if unicodedata.category(x) != 'Mn')\n",
        "    return ex\n",
        "\n",
        "# List of stopwords (commonly used words in English)\n",
        "stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \n",
        "             \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\",\n",
        "             \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \n",
        "             \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\",\n",
        "             \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\",\n",
        "             \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \n",
        "             \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\",\n",
        "             \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\",\n",
        "             \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\",\n",
        "             \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\",\n",
        "             \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
        "\n",
        "# Removes stopwords\n",
        "def remove_stopwords(ex):\n",
        "    ex['text'] = ' '.join(x for x in ex['text'].split() if x not in stopwords)\n",
        "    return ex\n",
        "\n",
        "# Process the data by making all text lowercase, removing puncuation, converting to ascii, and removing stopwords\n",
        "def process_data(data):\n",
        "    data1 = data.map(remove_punc)\n",
        "    data2 = data1.map(to_ascii)\n",
        "    data3 = data2.map(to_lower)\n",
        "    data4 = data3.map(remove_stopwords)\n",
        "    return data1\n",
        "\n",
        "# 0 = negative, 1 = positive, 2 = no impact, 3 = mixed (both positive and negative)\n",
        "train_data = load_dataset('tweet_eval', 'sentiment', split='train')\n",
        "test_data = load_dataset('tweet_eval', 'sentiment', split='test')\n",
        "\n",
        "# resulting data splits after processing the data\n",
        "training_data = process_data(train_data)\n",
        "testing_data = process_data(test_data)\n",
        "\n",
        "from transformers import AutoTokenizer, get_scheduler, BertConfig, AdamW, BertModel\n",
        "\n",
        "configuration = BertConfig()\n",
        "setattr(configuration, 'l1', 512)\n",
        "setattr(configuration, 'num_classes', 3)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def tokenize_func(ex):\n",
        "    return tokenizer(ex['text'], padding='max_length', truncation=True)\n",
        "\n",
        "# Removes unnecessary columns and renames columns to match the model specification\n",
        "train1 = training_data.map(tokenize_func)\n",
        "test1 = testing_data.map(tokenize_func)\n",
        "train2 = train1.remove_columns(\"text\")\n",
        "test2 = test1.remove_columns(\"text\")\n",
        "training_dataset = train2.rename_column(\"label\", \"labels\")\n",
        "testing_dataset = test2.rename_column(\"label\", \"labels\")\n",
        "\n",
        "training_dataset.set_format(\"torch\")\n",
        "testing_dataset.set_format(\"torch\")\n",
        "\n",
        "# Define the number of active learning rounds to conduct, the budget, and the number of classes in twitter dataset\n",
        "nclasses = 3\n",
        "n_rounds = 3\n",
        "budget = 300\n",
        "\n",
        "train_size_pc     = 200\n",
        "test_size_pc      = 1000\n",
        "unlabeled_size_pc = 5000\n",
        "\n",
        "# Ensure reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate random index lists for subset creation\n",
        "full_train_subset = []\n",
        "full_unlabeled_subset = []\n",
        "full_test_subset = []\n",
        "\n",
        "for class_num in range(nclasses):\n",
        "    train_idx_pool = []\n",
        "    for train_idx in range(len(training_dataset)):\n",
        "        if training_dataset[train_idx][\"labels\"].item() == class_num:\n",
        "            train_idx_pool.append(train_idx)\n",
        "    test_idx_pool = []\n",
        "    for test_idx in range(len(testing_dataset)):\n",
        "        if testing_dataset[test_idx][\"labels\"].item() == class_num:\n",
        "            test_idx_pool.append(test_idx)\n",
        "\n",
        "    # Get a random subset from each pool\n",
        "    train_idx_subset = np.random.choice(train_idx_pool, size=train_size_pc, replace = False).tolist()\n",
        "    remaining_idx = list(set(train_idx_pool) - set(train_idx_subset))\n",
        "    unlabeled_idx_subset = np.random.choice(remaining_idx, size=unlabeled_size_pc, replace = False).tolist()\n",
        "    test_idx_subset = np.random.choice(test_idx_pool, size=test_size_pc, replace = False).tolist()\n",
        "    full_train_subset.extend(train_idx_subset)\n",
        "    full_unlabeled_subset.extend(unlabeled_idx_subset)\n",
        "    full_test_subset.extend(test_idx_subset)\n",
        "\n",
        "tweet_train = Subset(training_dataset, full_train_subset)\n",
        "tweet_unlabeled = Subset(training_dataset, full_unlabeled_subset)\n",
        "testing_dataset = Subset(testing_dataset, full_test_subset)\n",
        "\n",
        "model = CustomBERTModel(configuration)\n",
        "state_dict = torch.load(model_directory)\n",
        "model.load_state_dict(state_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3WGIX-L6ZzP"
      },
      "source": [
        "### Entropy Sampling\n",
        "A very basic strategy to select unlabeled points is entropy sampling, where we select samples about which the model is most uncertain by measuring the entropy of the class prediction. Hence, a valid strategy is to select those points in the unlabeled set with highest entropy (maximum uncertainty). Specifically, let $z_i$ be output from the model. By applying a softmax, we obtain probabilities that we can use: $$\\sigma(z)_i = \\frac{e^{z_i}}{\\sum_j e^{z_j}}$$ Then, the entropy can be calculated as $$ENTROPY = -\\sum_j \\sigma(z)_j*log(\\sigma(z)_j)$$\n",
        "\n",
        "Here we create a instance of distil.active_learning_strategies.entropy_sampling.EntropySampling with the same parameters passed to distil.active_learning_strategies.random_sampling.RandomSampling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MeNzrlk6dm2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c19895b-ad7b-46f2-e17c-2b51b92901f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Testing accuracy: 57.83\n",
            "-------------------------------------------------\n",
            "Round 1\n",
            "-------------------------------------------------\n",
            "Number of training points - 900\n",
            "Training..\n",
            "5 training accuracy: 0.4822222222222222\n",
            "10 training accuracy: 0.5222222222222223\n",
            "15 training accuracy: 0.5677777777777778\n",
            "20 training accuracy: 0.5444444444444444\n",
            "25 training accuracy: 0.57\n",
            "30 training accuracy: 0.5766666666666667\n",
            "35 training accuracy: 0.5744444444444444\n",
            "40 training accuracy: 0.5722222222222222\n",
            "45 training accuracy: 0.5611111111111111\n",
            "50 training accuracy: 0.5477777777777778\n",
            "55 training accuracy: 0.5988888888888889\n",
            "60 training accuracy: 0.5755555555555556\n",
            "65 training accuracy: 0.59\n",
            "70 training accuracy: 0.5822222222222222\n",
            "75 training accuracy: 0.5933333333333334\n",
            "80 training accuracy: 0.5888888888888889\n",
            "85 training accuracy: 0.5988888888888889\n",
            "90 training accuracy: 0.6022222222222222\n",
            "95 training accuracy: 0.5988888888888889\n",
            "100 training accuracy: 0.5822222222222222\n",
            "105 training accuracy: 0.6155555555555555\n",
            "110 training accuracy: 0.57\n",
            "115 training accuracy: 0.5888888888888889\n",
            "120 training accuracy: 0.6\n",
            "125 training accuracy: 0.57\n",
            "130 training accuracy: 0.6055555555555555\n",
            "135 training accuracy: 0.6\n",
            "140 training accuracy: 0.6\n",
            "145 training accuracy: 0.5655555555555556\n",
            "150 training accuracy: 0.5866666666666667\n",
            "155 training accuracy: 0.6011111111111112\n",
            "160 training accuracy: 0.57\n",
            "165 training accuracy: 0.5888888888888889\n",
            "170 training accuracy: 0.59\n",
            "175 training accuracy: 0.6066666666666667\n",
            "180 training accuracy: 0.5788888888888889\n",
            "185 training accuracy: 0.6022222222222222\n",
            "190 training accuracy: 0.6055555555555555\n",
            "195 training accuracy: 0.5766666666666667\n",
            "200 training accuracy: 0.5955555555555555\n",
            "Epoch: 200 Training accuracy: 0.596\n",
            "Testing accuracy: 58.17\n",
            "-------------------------------------------------\n",
            "Round 2\n",
            "-------------------------------------------------\n",
            "Number of training points - 1200\n",
            "Training..\n",
            "5 training accuracy: 0.4558333333333333\n",
            "10 training accuracy: 0.5033333333333333\n",
            "15 training accuracy: 0.52\n",
            "20 training accuracy: 0.5291666666666667\n",
            "25 training accuracy: 0.5358333333333334\n",
            "30 training accuracy: 0.5216666666666666\n",
            "35 training accuracy: 0.5425\n",
            "40 training accuracy: 0.5483333333333333\n",
            "45 training accuracy: 0.5533333333333333\n",
            "50 training accuracy: 0.5341666666666667\n",
            "55 training accuracy: 0.55\n",
            "60 training accuracy: 0.5583333333333333\n",
            "65 training accuracy: 0.5558333333333333\n",
            "70 training accuracy: 0.5516666666666666\n",
            "75 training accuracy: 0.5366666666666666\n",
            "80 training accuracy: 0.5491666666666667\n",
            "85 training accuracy: 0.5491666666666667\n",
            "90 training accuracy: 0.5466666666666666\n",
            "95 training accuracy: 0.5583333333333333\n",
            "100 training accuracy: 0.545\n",
            "105 training accuracy: 0.5441666666666667\n",
            "110 training accuracy: 0.5625\n",
            "115 training accuracy: 0.555\n",
            "120 training accuracy: 0.5491666666666667\n",
            "125 training accuracy: 0.555\n",
            "130 training accuracy: 0.5691666666666667\n",
            "135 training accuracy: 0.5516666666666666\n",
            "140 training accuracy: 0.5508333333333333\n",
            "145 training accuracy: 0.5591666666666667\n",
            "150 training accuracy: 0.5708333333333333\n",
            "155 training accuracy: 0.5666666666666667\n",
            "160 training accuracy: 0.5408333333333334\n",
            "165 training accuracy: 0.5783333333333334\n",
            "170 training accuracy: 0.5766666666666667\n",
            "175 training accuracy: 0.565\n",
            "180 training accuracy: 0.5616666666666666\n",
            "185 training accuracy: 0.54\n",
            "190 training accuracy: 0.5483333333333333\n",
            "195 training accuracy: 0.5875\n",
            "200 training accuracy: 0.5508333333333333\n",
            "Epoch: 200 Training accuracy: 0.551\n",
            "Testing accuracy: 56.57\n",
            "Training Completed\n"
          ]
        }
      ],
      "source": [
        "strategy_args = {'batch_size' : 5}\n",
        "strategy = EntropySampling(tweet_train, tweet_unlabeled, model, nclasses, strategy_args)\n",
        "\n",
        "# Use the same training parameters as before\n",
        "args = {'lr':1e-4, 'batch_size':5, 'n_epoch': 200, 'optimizer':'adam'} \n",
        "dt = data_train(tweet_train, model, args)\n",
        "\n",
        "# Update the model used in the AL strategy with the loaded initial model\n",
        "strategy.update_model(model)\n",
        "\n",
        "# Get the test accuracy of the initial model\n",
        "acc = np.zeros(n_rounds)\n",
        "acc[0] = dt.get_acc_on_set(testing_dataset)\n",
        "print('Initial Testing accuracy:', round(acc[0]*100, 2), flush=True)\n",
        "\n",
        "# User Controlled Loop\n",
        "for rd in range(1, n_rounds):\n",
        "    print('-------------------------------------------------')\n",
        "    print('Round', rd) \n",
        "    print('-------------------------------------------------')\n",
        "\n",
        "    # Use select() to obtain the indices in the unlabeled set that should be labeled\n",
        "    idx = strategy.select(budget)\n",
        "\n",
        "    # Add the selected points to the train set. The unlabeled set shown in the next couple lines \n",
        "    # already has the associated labels, so no human labeling is needed. Again, this is because \n",
        "    # we already have the labels a priori. In real scenarios, a human oracle would need to provide \n",
        "    # then before proceeding.\n",
        "    tweet_train = ConcatDataset([tweet_train, Subset(tweet_unlabeled, idx)])\n",
        "    remaining_unlabeled_idx = list(set(range(len(tweet_unlabeled))) - set(idx))\n",
        "    tweet_unlabeled = Subset(tweet_unlabeled, remaining_unlabeled_idx)\n",
        "\n",
        "    print('Number of training points -', len(tweet_train))\n",
        "\n",
        "    # Update the data used in the AL strategy and the training class\n",
        "    strategy.update_data(tweet_train, tweet_unlabeled)\n",
        "    dt.update_data(tweet_train)\n",
        "\n",
        "    # Retrain the model and update the strategy with the result\n",
        "    clf = dt.train()\n",
        "    strategy.update_model(clf)\n",
        "\n",
        "    # Get new test accuracy\n",
        "    acc[rd] = dt.get_acc_on_set(testing_dataset)\n",
        "    print('Testing accuracy:', round(acc[rd]*100, 2), flush=True)\n",
        "\n",
        "print('Training Completed')\n",
        "\n",
        "#Saving accuracies for further analysis\n",
        "with open(os.path.join(base_dir,'entropy.txt'), 'w') as f:\n",
        "    for item in acc:\n",
        "        f.write(\"%s\\n\" % item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMv330yq6mqs"
      },
      "source": [
        "### Reload Data\n",
        "\n",
        "Here we reload, reprocess, and retokenize the data and prepare for a different active learning strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bH4yd_LE6pe7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "369e7707-2a7a-4cab-b1b5-f143e64733d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset tweet_eval (/root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n",
            "WARNING:datasets.builder:Found cached dataset tweet_eval (/root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-9642c90231075bed.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-aad948fd6d1a9b85.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-8d66e21073d341c9.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-074821155257838e.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-efcff37138bd934c.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-2276388c7af259c7.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-de36b5493c1aff7d.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-0bacefc92dafa711.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-5e6d87e2ee031e39.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-791a24d1773be097.arrow\n",
            "Some weights of the model checkpoint at prajjwal1/bert-small were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# Converts all text to lowercase\n",
        "def to_lower(ex):\n",
        "    ex['text'] = ex['text'].lower()\n",
        "    return ex\n",
        "\n",
        "# Removes punctuation from the text\n",
        "def remove_punc(ex):\n",
        "    ex['text'] = ''.join(x for x in ex['text'] if x not in string.punctuation)\n",
        "    return ex\n",
        "\n",
        "# Converts text to ASCII (also converts accented text to normal)\n",
        "def to_ascii(ex):\n",
        "    ex['text'] = ''.join(x for x in unicodedata.normalize('NFD',ex['text']) if unicodedata.category(x) != 'Mn')\n",
        "    return ex\n",
        "\n",
        "# List of stopwords (commonly used words in English)\n",
        "stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \n",
        "             \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\",\n",
        "             \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \n",
        "             \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\",\n",
        "             \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\",\n",
        "             \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \n",
        "             \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\",\n",
        "             \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\",\n",
        "             \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\",\n",
        "             \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\",\n",
        "             \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
        "\n",
        "# Removes stopwords\n",
        "def remove_stopwords(ex):\n",
        "    ex['text'] = ' '.join(x for x in ex['text'].split() if x not in stopwords)\n",
        "    return ex\n",
        "\n",
        "# Process the data by making all text lowercase, removing puncuation, converting to ascii, and removing stopwords\n",
        "def process_data(data):\n",
        "    data1 = data.map(remove_punc)\n",
        "    data2 = data1.map(to_ascii)\n",
        "    data3 = data2.map(to_lower)\n",
        "    data4 = data3.map(remove_stopwords)\n",
        "    return data1\n",
        "\n",
        "# 0 = negative, 1 = positive, 2 = no impact, 3 = mixed (both positive and negative)\n",
        "train_data = load_dataset('tweet_eval', 'sentiment', split='train')\n",
        "test_data = load_dataset('tweet_eval', 'sentiment', split='test')\n",
        "\n",
        "# resulting data splits after processing the data\n",
        "training_data = process_data(train_data)\n",
        "testing_data = process_data(test_data)\n",
        "\n",
        "from transformers import AutoTokenizer, get_scheduler, BertConfig, AdamW, BertModel\n",
        "\n",
        "configuration = BertConfig()\n",
        "setattr(configuration, 'l1', 512)\n",
        "setattr(configuration, 'num_classes', 3)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def tokenize_func(ex):\n",
        "    return tokenizer(ex['text'], padding='max_length', truncation=True)\n",
        "\n",
        "# Removes unnecessary columns and renames columns to match the model specification\n",
        "train1 = training_data.map(tokenize_func)\n",
        "test1 = testing_data.map(tokenize_func)\n",
        "train2 = train1.remove_columns(\"text\")\n",
        "test2 = test1.remove_columns(\"text\")\n",
        "training_dataset = train2.rename_column(\"label\", \"labels\")\n",
        "testing_dataset = test2.rename_column(\"label\", \"labels\")\n",
        "\n",
        "training_dataset.set_format(\"torch\")\n",
        "testing_dataset.set_format(\"torch\")\n",
        "\n",
        "# Define the number of active learning rounds to conduct, the budget, and the number of classes in twitter dataset\n",
        "nclasses = 3\n",
        "n_rounds = 3\n",
        "budget = 300\n",
        "\n",
        "train_size_pc     = 200\n",
        "test_size_pc      = 1000\n",
        "unlabeled_size_pc = 5000\n",
        "\n",
        "# Ensure reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate random index lists for subset creation\n",
        "full_train_subset = []\n",
        "full_unlabeled_subset = []\n",
        "full_test_subset = []\n",
        "\n",
        "for class_num in range(nclasses):\n",
        "    train_idx_pool = []\n",
        "    for train_idx in range(len(training_dataset)):\n",
        "        if training_dataset[train_idx][\"labels\"].item() == class_num:\n",
        "            train_idx_pool.append(train_idx)\n",
        "    test_idx_pool = []\n",
        "    for test_idx in range(len(testing_dataset)):\n",
        "        if testing_dataset[test_idx][\"labels\"].item() == class_num:\n",
        "            test_idx_pool.append(test_idx)\n",
        "\n",
        "    # Get a random subset from each pool\n",
        "    train_idx_subset = np.random.choice(train_idx_pool, size=train_size_pc, replace = False).tolist()\n",
        "    remaining_idx = list(set(train_idx_pool) - set(train_idx_subset))\n",
        "    unlabeled_idx_subset = np.random.choice(remaining_idx, size=unlabeled_size_pc, replace = False).tolist()\n",
        "    test_idx_subset = np.random.choice(test_idx_pool, size=test_size_pc, replace = False).tolist()\n",
        "    full_train_subset.extend(train_idx_subset)\n",
        "    full_unlabeled_subset.extend(unlabeled_idx_subset)\n",
        "    full_test_subset.extend(test_idx_subset)\n",
        "\n",
        "tweet_train = Subset(training_dataset, full_train_subset)\n",
        "tweet_unlabeled = Subset(training_dataset, full_unlabeled_subset)\n",
        "testing_dataset = Subset(testing_dataset, full_test_subset)\n",
        "\n",
        "model = CustomBERTModel(configuration)\n",
        "state_dict = torch.load(model_directory)\n",
        "model.load_state_dict(state_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZOlyDtw6rrI"
      },
      "source": [
        "### BADGE\n",
        "This method is based on the paper [Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds](https://arxiv.org/abs/1906.03671). The strategy is meant to select points that are both diverse (e.g., their embeddings span multiple directions) and uncertain (e.g., their contribution to the loss is large). The following steps are taken:\n",
        "\n",
        "* Calculate the pseudo-label for each point in the unlabeled set. The pseudo-label is the class with the highest probability.\n",
        "* Compute the cross-entropy loss for each point in the unlabeled set using this pseudo-label.\n",
        "* Obtain the resulting loss gradients on the last linear layer of the model for each point. (These are referred to as the hypothesized loss gradients.)\n",
        "* Using these gradients as a form of embedding for each unlabeled point, run k-means++ initialization on this embedding set, retrieving $k$ centers. Each center is a point from the unlabeled set, and $k$ represents the active learning budget.\n",
        "* Request labels for the $k$ points whose embeddings were selected.\n",
        "\n",
        "Here we create a instance of distil.active_learning_strategies.badge.BADGE with same parameters passed to distil.active_learning_strategies.random_sampling.RandomSampling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHbsLEaN6utZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee2c1087-45f3-4a56-bb15-20fe56be4801"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Testing accuracy: 57.83\n",
            "-------------------------------------------------\n",
            "Round 1\n",
            "-------------------------------------------------\n",
            "Number of training points - 900\n",
            "Training..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 training accuracy: 0.4722222222222222\n",
            "10 training accuracy: 0.5011111111111111\n",
            "15 training accuracy: 0.5333333333333333\n",
            "20 training accuracy: 0.5611111111111111\n",
            "25 training accuracy: 0.5555555555555556\n",
            "30 training accuracy: 0.5855555555555556\n",
            "35 training accuracy: 0.5822222222222222\n",
            "40 training accuracy: 0.5633333333333334\n",
            "45 training accuracy: 0.5922222222222222\n",
            "50 training accuracy: 0.5944444444444444\n",
            "55 training accuracy: 0.5844444444444444\n",
            "60 training accuracy: 0.5966666666666667\n",
            "65 training accuracy: 0.5855555555555556\n",
            "70 training accuracy: 0.6077777777777778\n",
            "75 training accuracy: 0.6022222222222222\n",
            "80 training accuracy: 0.5977777777777777\n",
            "85 training accuracy: 0.6088888888888889\n",
            "90 training accuracy: 0.6222222222222222\n",
            "95 training accuracy: 0.6155555555555555\n",
            "100 training accuracy: 0.6344444444444445\n",
            "105 training accuracy: 0.5855555555555556\n",
            "110 training accuracy: 0.6122222222222222\n",
            "115 training accuracy: 0.6322222222222222\n",
            "120 training accuracy: 0.6322222222222222\n",
            "125 training accuracy: 0.6433333333333333\n",
            "130 training accuracy: 0.6144444444444445\n",
            "135 training accuracy: 0.6144444444444445\n",
            "140 training accuracy: 0.63\n",
            "145 training accuracy: 0.6277777777777778\n",
            "150 training accuracy: 0.6366666666666667\n",
            "155 training accuracy: 0.6255555555555555\n",
            "160 training accuracy: 0.6366666666666667\n",
            "165 training accuracy: 0.6488888888888888\n",
            "170 training accuracy: 0.6133333333333333\n",
            "175 training accuracy: 0.6044444444444445\n",
            "180 training accuracy: 0.6388888888888888\n",
            "185 training accuracy: 0.6288888888888889\n",
            "190 training accuracy: 0.6411111111111111\n",
            "195 training accuracy: 0.6188888888888889\n",
            "200 training accuracy: 0.6188888888888889\n",
            "Epoch: 200 Training accuracy: 0.619\n",
            "Testing accuracy: 59.43\n",
            "-------------------------------------------------\n",
            "Round 2\n",
            "-------------------------------------------------\n",
            "Number of training points - 1200\n",
            "Training..\n",
            "5 training accuracy: 0.455\n",
            "10 training accuracy: 0.5183333333333333\n",
            "15 training accuracy: 0.5375\n",
            "20 training accuracy: 0.5391666666666667\n",
            "25 training accuracy: 0.5575\n",
            "30 training accuracy: 0.5775\n",
            "35 training accuracy: 0.5808333333333333\n",
            "40 training accuracy: 0.57\n",
            "45 training accuracy: 0.5916666666666667\n",
            "50 training accuracy: 0.5783333333333334\n",
            "55 training accuracy: 0.5908333333333333\n",
            "60 training accuracy: 0.5858333333333333\n",
            "65 training accuracy: 0.5875\n",
            "70 training accuracy: 0.5808333333333333\n",
            "75 training accuracy: 0.6066666666666667\n",
            "80 training accuracy: 0.5941666666666666\n",
            "85 training accuracy: 0.6008333333333333\n",
            "90 training accuracy: 0.5975\n",
            "95 training accuracy: 0.5816666666666667\n",
            "100 training accuracy: 0.615\n",
            "105 training accuracy: 0.595\n",
            "110 training accuracy: 0.61\n",
            "115 training accuracy: 0.6141666666666666\n",
            "120 training accuracy: 0.6258333333333334\n",
            "125 training accuracy: 0.6025\n",
            "130 training accuracy: 0.5941666666666666\n",
            "135 training accuracy: 0.5983333333333334\n",
            "140 training accuracy: 0.6233333333333333\n",
            "145 training accuracy: 0.5883333333333334\n",
            "150 training accuracy: 0.5975\n",
            "155 training accuracy: 0.6141666666666666\n",
            "160 training accuracy: 0.6091666666666666\n",
            "165 training accuracy: 0.595\n",
            "170 training accuracy: 0.5908333333333333\n",
            "175 training accuracy: 0.6125\n",
            "180 training accuracy: 0.6091666666666666\n",
            "185 training accuracy: 0.6083333333333333\n",
            "190 training accuracy: 0.5983333333333334\n",
            "195 training accuracy: 0.5983333333333334\n",
            "200 training accuracy: 0.6183333333333333\n",
            "Epoch: 200 Training accuracy: 0.618\n",
            "Testing accuracy: 60.27\n",
            "Training Completed\n"
          ]
        }
      ],
      "source": [
        "strategy_args = {'batch_size' : 5}\n",
        "strategy = BADGE(tweet_train, tweet_unlabeled, model, nclasses, strategy_args)\n",
        "\n",
        "# Use the same training parameters as before\n",
        "args = {'lr':1e-4, 'batch_size':5, 'n_epoch': 200, 'optimizer':'adam'} \n",
        "dt = data_train(tweet_train, model, args)\n",
        "\n",
        "# Update the model used in the AL strategy with the loaded initial model\n",
        "strategy.update_model(model)\n",
        "\n",
        "# Get the test accuracy of the initial model\n",
        "acc = np.zeros(n_rounds)\n",
        "acc[0] = dt.get_acc_on_set(testing_dataset)\n",
        "print('Initial Testing accuracy:', round(acc[0]*100, 2), flush=True)\n",
        "\n",
        "# User Controlled Loop\n",
        "for rd in range(1, n_rounds):\n",
        "    print('-------------------------------------------------')\n",
        "    print('Round', rd) \n",
        "    print('-------------------------------------------------')\n",
        "\n",
        "    # Use select() to obtain the indices in the unlabeled set that should be labeled\n",
        "    idx = strategy.select(budget)\n",
        "\n",
        "    # Add the selected points to the train set. The unlabeled set shown in the next couple lines \n",
        "    # already has the associated labels, so no human labeling is needed. Again, this is because \n",
        "    # we already have the labels a priori. In real scenarios, a human oracle would need to provide \n",
        "    # then before proceeding.\n",
        "    tweet_train = ConcatDataset([tweet_train, Subset(tweet_unlabeled, idx)])\n",
        "    remaining_unlabeled_idx = list(set(range(len(tweet_unlabeled))) - set(idx))\n",
        "    tweet_unlabeled = Subset(tweet_unlabeled, remaining_unlabeled_idx)\n",
        "\n",
        "    print('Number of training points -', len(tweet_train))\n",
        "\n",
        "    # Update the data used in the AL strategy and the training class\n",
        "    strategy.update_data(tweet_train, tweet_unlabeled)\n",
        "    dt.update_data(tweet_train)\n",
        "\n",
        "    # Retrain the model and update the strategy with the result\n",
        "    clf = dt.train()\n",
        "    strategy.update_model(clf)\n",
        "\n",
        "    # Get new test accuracy\n",
        "    acc[rd] = dt.get_acc_on_set(testing_dataset)\n",
        "    print('Testing accuracy:', round(acc[rd]*100, 2), flush=True)\n",
        "\n",
        "print('Training Completed')\n",
        "\n",
        "#Saving accuracies for further analysis\n",
        "with open(os.path.join(base_dir,'badge.txt'), 'w') as f:\n",
        "    for item in acc:\n",
        "        f.write(\"%s\\n\" % item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edfDUHCa6zN0"
      },
      "source": [
        "### Reload Data\n",
        "\n",
        "Here we reload, reprocess, and retokenize the data and prepare for a different active learning strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8z_IdTN362bp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b56487fd-5b98-4279-ae03-6da220370e02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset tweet_eval (/root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n",
            "WARNING:datasets.builder:Found cached dataset tweet_eval (/root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-9642c90231075bed.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-aad948fd6d1a9b85.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-8d66e21073d341c9.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-074821155257838e.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-efcff37138bd934c.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-2276388c7af259c7.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-de36b5493c1aff7d.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-0bacefc92dafa711.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-5e6d87e2ee031e39.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-791a24d1773be097.arrow\n",
            "Some weights of the model checkpoint at prajjwal1/bert-small were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Converts all text to lowercase\n",
        "def to_lower(ex):\n",
        "    ex['text'] = ex['text'].lower()\n",
        "    return ex\n",
        "\n",
        "# Removes punctuation from the text\n",
        "def remove_punc(ex):\n",
        "    ex['text'] = ''.join(x for x in ex['text'] if x not in string.punctuation)\n",
        "    return ex\n",
        "\n",
        "# Converts text to ASCII (also converts accented text to normal)\n",
        "def to_ascii(ex):\n",
        "    ex['text'] = ''.join(x for x in unicodedata.normalize('NFD',ex['text']) if unicodedata.category(x) != 'Mn')\n",
        "    return ex\n",
        "\n",
        "# List of stopwords (commonly used words in English)\n",
        "stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \n",
        "             \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\",\n",
        "             \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \n",
        "             \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\",\n",
        "             \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\",\n",
        "             \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \n",
        "             \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\",\n",
        "             \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\",\n",
        "             \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\",\n",
        "             \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\",\n",
        "             \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
        "\n",
        "# Removes stopwords\n",
        "def remove_stopwords(ex):\n",
        "    ex['text'] = ' '.join(x for x in ex['text'].split() if x not in stopwords)\n",
        "    return ex\n",
        "\n",
        "# Process the data by making all text lowercase, removing puncuation, converting to ascii, and removing stopwords\n",
        "def process_data(data):\n",
        "    data1 = data.map(remove_punc)\n",
        "    data2 = data1.map(to_ascii)\n",
        "    data3 = data2.map(to_lower)\n",
        "    data4 = data3.map(remove_stopwords)\n",
        "    return data1\n",
        "\n",
        "# 0 = negative, 1 = positive, 2 = no impact, 3 = mixed (both positive and negative)\n",
        "train_data = load_dataset('tweet_eval', 'sentiment', split='train')\n",
        "test_data = load_dataset('tweet_eval', 'sentiment', split='test')\n",
        "\n",
        "# resulting data splits after processing the data\n",
        "training_data = process_data(train_data)\n",
        "testing_data = process_data(test_data)\n",
        "\n",
        "from transformers import AutoTokenizer, get_scheduler, BertConfig, AdamW, BertModel\n",
        "\n",
        "configuration = BertConfig()\n",
        "setattr(configuration, 'l1', 512)\n",
        "setattr(configuration, 'num_classes', 3)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def tokenize_func(ex):\n",
        "    return tokenizer(ex['text'], padding='max_length', truncation=True)\n",
        "\n",
        "# Removes unnecessary columns and renames columns to match the model specification\n",
        "train1 = training_data.map(tokenize_func)\n",
        "test1 = testing_data.map(tokenize_func)\n",
        "train2 = train1.remove_columns(\"text\")\n",
        "test2 = test1.remove_columns(\"text\")\n",
        "training_dataset = train2.rename_column(\"label\", \"labels\")\n",
        "testing_dataset = test2.rename_column(\"label\", \"labels\")\n",
        "\n",
        "training_dataset.set_format(\"torch\")\n",
        "testing_dataset.set_format(\"torch\")\n",
        "\n",
        "# Define the number of active learning rounds to conduct, the budget, and the number of classes in twitter dataset\n",
        "nclasses = 3\n",
        "n_rounds = 3\n",
        "budget = 300\n",
        "\n",
        "train_size_pc     = 200\n",
        "test_size_pc      = 1000\n",
        "unlabeled_size_pc = 5000\n",
        "\n",
        "# Ensure reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate random index lists for subset creation\n",
        "full_train_subset = []\n",
        "full_unlabeled_subset = []\n",
        "full_test_subset = []\n",
        "\n",
        "for class_num in range(nclasses):\n",
        "    train_idx_pool = []\n",
        "    for train_idx in range(len(training_dataset)):\n",
        "        if training_dataset[train_idx][\"labels\"].item() == class_num:\n",
        "            train_idx_pool.append(train_idx)\n",
        "    test_idx_pool = []\n",
        "    for test_idx in range(len(testing_dataset)):\n",
        "        if testing_dataset[test_idx][\"labels\"].item() == class_num:\n",
        "            test_idx_pool.append(test_idx)\n",
        "\n",
        "    # Get a random subset from each pool\n",
        "    train_idx_subset = np.random.choice(train_idx_pool, size=train_size_pc, replace = False).tolist()\n",
        "    remaining_idx = list(set(train_idx_pool) - set(train_idx_subset))\n",
        "    unlabeled_idx_subset = np.random.choice(remaining_idx, size=unlabeled_size_pc, replace = False).tolist()\n",
        "    test_idx_subset = np.random.choice(test_idx_pool, size=test_size_pc, replace = False).tolist()\n",
        "    full_train_subset.extend(train_idx_subset)\n",
        "    full_unlabeled_subset.extend(unlabeled_idx_subset)\n",
        "    full_test_subset.extend(test_idx_subset)\n",
        "\n",
        "tweet_train = Subset(training_dataset, full_train_subset)\n",
        "tweet_unlabeled = Subset(training_dataset, full_unlabeled_subset)\n",
        "testing_dataset = Subset(testing_dataset, full_test_subset)\n",
        "\n",
        "model = CustomBERTModel(configuration)\n",
        "state_dict = torch.load(model_directory)\n",
        "model.load_state_dict(state_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JGHcJDy64Z4"
      },
      "source": [
        "### Least Confidence Sampling\n",
        "\n",
        "Implements the Least Confidence Sampling Strategy a active learning strategy where the algorithm selects the data points for which the model has the lowest confidence while predicting its label.\n",
        "    \n",
        "Suppose the model has $nclasses$ output nodes denoted by $\\overrightarrow z$ and each output node is denoted by $z_j$. Thus, $j \\in [1, nclasses]$. Then for a output node $z_i$ from the model, the corresponding softmax would be $$\\sigma(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}$$\n",
        "        \n",
        "Then the softmax can be used pick $budget$ no. of elements for which the model has the lowest confidence as follows, $$\\mbox{argmin}_{{S \\subseteq {\\mathcal U}, |S| \\leq k}}{\\sum_S(\\mbox{argmax}_j{(\\sigma(\\overrightarrow{\\boldsymbol{z}}))})}$$ where $\\mathcal{U}$ denotes the Data without lables i.e. $unlabeled_x$ and $k$ is the $budget$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYZFs9_w7BQD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc547ee9-94a7-4194-bee7-ec9a1e83a787"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Testing accuracy: 57.83\n",
            "-------------------------------------------------\n",
            "Round 1\n",
            "-------------------------------------------------\n",
            "Number of training points - 900\n",
            "Training..\n",
            "5 training accuracy: 0.4211111111111111\n",
            "10 training accuracy: 0.46\n",
            "15 training accuracy: 0.5155555555555555\n",
            "20 training accuracy: 0.4988888888888889\n",
            "25 training accuracy: 0.52\n",
            "30 training accuracy: 0.5166666666666667\n",
            "35 training accuracy: 0.5511111111111111\n",
            "40 training accuracy: 0.52\n",
            "45 training accuracy: 0.5644444444444444\n",
            "50 training accuracy: 0.55\n",
            "55 training accuracy: 0.5533333333333333\n",
            "60 training accuracy: 0.5755555555555556\n",
            "65 training accuracy: 0.5811111111111111\n",
            "70 training accuracy: 0.5644444444444444\n",
            "75 training accuracy: 0.5822222222222222\n",
            "80 training accuracy: 0.5755555555555556\n",
            "85 training accuracy: 0.5755555555555556\n",
            "90 training accuracy: 0.5833333333333334\n",
            "95 training accuracy: 0.5633333333333334\n",
            "100 training accuracy: 0.5877777777777777\n",
            "105 training accuracy: 0.5988888888888889\n",
            "110 training accuracy: 0.5477777777777778\n",
            "115 training accuracy: 0.5811111111111111\n",
            "120 training accuracy: 0.5877777777777777\n",
            "125 training accuracy: 0.5733333333333334\n",
            "130 training accuracy: 0.6022222222222222\n",
            "135 training accuracy: 0.5922222222222222\n",
            "140 training accuracy: 0.5866666666666667\n",
            "145 training accuracy: 0.6022222222222222\n",
            "150 training accuracy: 0.5866666666666667\n",
            "155 training accuracy: 0.6122222222222222\n",
            "160 training accuracy: 0.5911111111111111\n",
            "165 training accuracy: 0.6\n",
            "170 training accuracy: 0.6033333333333334\n",
            "175 training accuracy: 0.5933333333333334\n",
            "180 training accuracy: 0.6055555555555555\n",
            "185 training accuracy: 0.5844444444444444\n",
            "190 training accuracy: 0.5866666666666667\n",
            "195 training accuracy: 0.5844444444444444\n",
            "200 training accuracy: 0.61\n",
            "Epoch: 200 Training accuracy: 0.61\n",
            "Testing accuracy: 59.87\n",
            "-------------------------------------------------\n",
            "Round 2\n",
            "-------------------------------------------------\n",
            "Number of training points - 1200\n",
            "Training..\n",
            "5 training accuracy: 0.38\n",
            "10 training accuracy: 0.44666666666666666\n",
            "15 training accuracy: 0.49583333333333335\n",
            "20 training accuracy: 0.49583333333333335\n",
            "25 training accuracy: 0.48833333333333334\n",
            "30 training accuracy: 0.49833333333333335\n",
            "35 training accuracy: 0.5225\n",
            "40 training accuracy: 0.5208333333333334\n",
            "45 training accuracy: 0.5108333333333334\n",
            "50 training accuracy: 0.5433333333333333\n",
            "55 training accuracy: 0.5391666666666667\n",
            "60 training accuracy: 0.5358333333333334\n",
            "65 training accuracy: 0.5433333333333333\n",
            "70 training accuracy: 0.5183333333333333\n",
            "75 training accuracy: 0.5466666666666666\n",
            "80 training accuracy: 0.5316666666666666\n",
            "85 training accuracy: 0.5291666666666667\n",
            "90 training accuracy: 0.5558333333333333\n",
            "95 training accuracy: 0.555\n",
            "100 training accuracy: 0.55\n",
            "105 training accuracy: 0.5458333333333333\n",
            "110 training accuracy: 0.5566666666666666\n",
            "115 training accuracy: 0.5391666666666667\n",
            "120 training accuracy: 0.555\n",
            "125 training accuracy: 0.5341666666666667\n",
            "130 training accuracy: 0.57\n",
            "135 training accuracy: 0.5491666666666667\n",
            "140 training accuracy: 0.5416666666666666\n",
            "145 training accuracy: 0.5441666666666667\n",
            "150 training accuracy: 0.5691666666666667\n",
            "155 training accuracy: 0.5533333333333333\n",
            "160 training accuracy: 0.5291666666666667\n",
            "165 training accuracy: 0.5591666666666667\n",
            "170 training accuracy: 0.5491666666666667\n",
            "175 training accuracy: 0.5608333333333333\n",
            "180 training accuracy: 0.5575\n",
            "185 training accuracy: 0.575\n",
            "190 training accuracy: 0.5491666666666667\n",
            "195 training accuracy: 0.5508333333333333\n",
            "200 training accuracy: 0.5558333333333333\n",
            "Epoch: 200 Training accuracy: 0.556\n",
            "Testing accuracy: 61.03\n",
            "Training Completed\n"
          ]
        }
      ],
      "source": [
        "strategy_args = {'batch_size' : 5}\n",
        "strategy = LeastConfidenceSampling(tweet_train, tweet_unlabeled, model, nclasses, strategy_args)\n",
        "\n",
        "# Use the same training parameters as before\n",
        "args = {'lr':1e-4, 'batch_size':5, 'n_epoch': 200, 'optimizer':'adam'} \n",
        "dt = data_train(tweet_train, model, args)\n",
        "\n",
        "# Update the model used in the AL strategy with the loaded initial model\n",
        "strategy.update_model(model)\n",
        "\n",
        "# Get the test accuracy of the initial model\n",
        "acc = np.zeros(n_rounds)\n",
        "acc[0] = dt.get_acc_on_set(testing_dataset)\n",
        "print('Initial Testing accuracy:', round(acc[0]*100, 2), flush=True)\n",
        "\n",
        "# User Controlled Loop\n",
        "for rd in range(1, n_rounds):\n",
        "    print('-------------------------------------------------')\n",
        "    print('Round', rd) \n",
        "    print('-------------------------------------------------')\n",
        "\n",
        "    # Use select() to obtain the indices in the unlabeled set that should be labeled\n",
        "    idx = strategy.select(budget)\n",
        "\n",
        "    # Add the selected points to the train set. The unlabeled set shown in the next couple lines \n",
        "    # already has the associated labels, so no human labeling is needed. Again, this is because \n",
        "    # we already have the labels a priori. In real scenarios, a human oracle would need to provide \n",
        "    # then before proceeding.\n",
        "    tweet_train = ConcatDataset([tweet_train, Subset(tweet_unlabeled, idx)])\n",
        "    remaining_unlabeled_idx = list(set(range(len(tweet_unlabeled))) - set(idx))\n",
        "    tweet_unlabeled = Subset(tweet_unlabeled, remaining_unlabeled_idx)\n",
        "\n",
        "    print('Number of training points -', len(tweet_train))\n",
        "\n",
        "    # Update the data used in the AL strategy and the training class\n",
        "    strategy.update_data(tweet_train, tweet_unlabeled)\n",
        "    dt.update_data(tweet_train)\n",
        "\n",
        "    # Retrain the model and update the strategy with the result\n",
        "    clf = dt.train()\n",
        "    strategy.update_model(clf)\n",
        "\n",
        "    # Get new test accuracy\n",
        "    acc[rd] = dt.get_acc_on_set(testing_dataset)\n",
        "    print('Testing accuracy:', round(acc[rd]*100, 2), flush=True)\n",
        "\n",
        "print('Training Completed')\n",
        "\n",
        "#Saving accuracies for further analysis\n",
        "with open(os.path.join(base_dir,'lcf.txt'), 'w') as f:\n",
        "    for item in acc:\n",
        "        f.write(\"%s\\n\" % item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CL7j6p8P7HSk"
      },
      "source": [
        "### Reload Data\n",
        "\n",
        "Here we reload, reprocess, and retokenize the data and prepare for a different active learning strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gajIxCTo7KCa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a66c54de-542c-45e9-94e4-fe720c00465c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset tweet_eval (/root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n",
            "WARNING:datasets.builder:Found cached dataset tweet_eval (/root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343)\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-9642c90231075bed.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-aad948fd6d1a9b85.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-8d66e21073d341c9.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-074821155257838e.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-efcff37138bd934c.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-2276388c7af259c7.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-de36b5493c1aff7d.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-0bacefc92dafa711.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-5e6d87e2ee031e39.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/tweet_eval/sentiment/1.1.0/12aee5282b8784f3e95459466db4cdf45c6bf49719c25cdb0743d71ed0410343/cache-791a24d1773be097.arrow\n",
            "Some weights of the model checkpoint at prajjwal1/bert-small were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Converts all text to lowercase\n",
        "def to_lower(ex):\n",
        "    ex['text'] = ex['text'].lower()\n",
        "    return ex\n",
        "\n",
        "# Removes punctuation from the text\n",
        "def remove_punc(ex):\n",
        "    ex['text'] = ''.join(x for x in ex['text'] if x not in string.punctuation)\n",
        "    return ex\n",
        "\n",
        "# Converts text to ASCII (also converts accented text to normal)\n",
        "def to_ascii(ex):\n",
        "    ex['text'] = ''.join(x for x in unicodedata.normalize('NFD',ex['text']) if unicodedata.category(x) != 'Mn')\n",
        "    return ex\n",
        "\n",
        "# List of stopwords (commonly used words in English)\n",
        "stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \n",
        "             \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\",\n",
        "             \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \n",
        "             \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\",\n",
        "             \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\",\n",
        "             \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \n",
        "             \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\",\n",
        "             \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\",\n",
        "             \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\",\n",
        "             \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\",\n",
        "             \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
        "\n",
        "# Removes stopwords\n",
        "def remove_stopwords(ex):\n",
        "    ex['text'] = ' '.join(x for x in ex['text'].split() if x not in stopwords)\n",
        "    return ex\n",
        "\n",
        "# Process the data by making all text lowercase, removing puncuation, converting to ascii, and removing stopwords\n",
        "def process_data(data):\n",
        "    data1 = data.map(remove_punc)\n",
        "    data2 = data1.map(to_ascii)\n",
        "    data3 = data2.map(to_lower)\n",
        "    data4 = data3.map(remove_stopwords)\n",
        "    return data1\n",
        "\n",
        "# 0 = negative, 1 = positive, 2 = no impact, 3 = mixed (both positive and negative)\n",
        "train_data = load_dataset('tweet_eval', 'sentiment', split='train')\n",
        "test_data = load_dataset('tweet_eval', 'sentiment', split='test')\n",
        "\n",
        "# resulting data splits after processing the data\n",
        "training_data = process_data(train_data)\n",
        "testing_data = process_data(test_data)\n",
        "\n",
        "from transformers import AutoTokenizer, get_scheduler, BertConfig, AdamW, BertModel\n",
        "\n",
        "configuration = BertConfig()\n",
        "setattr(configuration, 'l1', 512)\n",
        "setattr(configuration, 'num_classes', 3)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def tokenize_func(ex):\n",
        "    return tokenizer(ex['text'], padding='max_length', truncation=True)\n",
        "\n",
        "# Removes unnecessary columns and renames columns to match the model specification\n",
        "train1 = training_data.map(tokenize_func)\n",
        "test1 = testing_data.map(tokenize_func)\n",
        "train2 = train1.remove_columns(\"text\")\n",
        "test2 = test1.remove_columns(\"text\")\n",
        "training_dataset = train2.rename_column(\"label\", \"labels\")\n",
        "testing_dataset = test2.rename_column(\"label\", \"labels\")\n",
        "\n",
        "training_dataset.set_format(\"torch\")\n",
        "testing_dataset.set_format(\"torch\")\n",
        "\n",
        "# Define the number of active learning rounds to conduct, the budget, and the number of classes in twitter dataset\n",
        "nclasses = 3\n",
        "n_rounds = 3\n",
        "budget = 300\n",
        "\n",
        "train_size_pc     = 200\n",
        "test_size_pc      = 1000\n",
        "unlabeled_size_pc = 5000\n",
        "\n",
        "# Ensure reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate random index lists for subset creation\n",
        "full_train_subset = []\n",
        "full_unlabeled_subset = []\n",
        "full_test_subset = []\n",
        "\n",
        "for class_num in range(nclasses):\n",
        "    train_idx_pool = []\n",
        "    for train_idx in range(len(training_dataset)):\n",
        "        if training_dataset[train_idx][\"labels\"].item() == class_num:\n",
        "            train_idx_pool.append(train_idx)\n",
        "    test_idx_pool = []\n",
        "    for test_idx in range(len(testing_dataset)):\n",
        "        if testing_dataset[test_idx][\"labels\"].item() == class_num:\n",
        "            test_idx_pool.append(test_idx)\n",
        "\n",
        "    # Get a random subset from each pool\n",
        "    train_idx_subset = np.random.choice(train_idx_pool, size=train_size_pc, replace = False).tolist()\n",
        "    remaining_idx = list(set(train_idx_pool) - set(train_idx_subset))\n",
        "    unlabeled_idx_subset = np.random.choice(remaining_idx, size=unlabeled_size_pc, replace = False).tolist()\n",
        "    test_idx_subset = np.random.choice(test_idx_pool, size=test_size_pc, replace = False).tolist()\n",
        "    full_train_subset.extend(train_idx_subset)\n",
        "    full_unlabeled_subset.extend(unlabeled_idx_subset)\n",
        "    full_test_subset.extend(test_idx_subset)\n",
        "\n",
        "tweet_train = Subset(training_dataset, full_train_subset)\n",
        "tweet_unlabeled = Subset(training_dataset, full_unlabeled_subset)\n",
        "testing_dataset = Subset(testing_dataset, full_test_subset)\n",
        "\n",
        "model = CustomBERTModel(configuration)\n",
        "state_dict = torch.load(model_directory)\n",
        "model.load_state_dict(state_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nsmb3cdb7MeS"
      },
      "source": [
        "### Margin Sampling\n",
        "\n",
        "Implements the Margin Sampling Strategy a active learning strategy similar to Least Confidence Sampling Strategy. While least confidence only takes into consideration the maximum probability, margin sampling considers the difference between the confidence of first and the second most probable labels.  \n",
        "    \n",
        "Suppose the model has $nclasses$ output nodes denoted by $\\overrightarrow{\\boldsymbol{z}}$ and each output node is denoted by $z_j$. Thus, $j \\in [1, nclasses]$. Then for a output node $z_i$ from the model, the corresponding softmax would be $$\\sigma(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}} \n",
        "        \n",
        "Let, $$m = \\mbox{argmax}_j{(\\sigma(\\overrightarrow{\\boldsymbol{z}}))}$$\n",
        "Then using softmax, Margin Sampling Strategy would pick $budget$ no. of elements as follows, $$\\mbox{argmin}_{{S \\subseteq {\\mathcal U}, |S| \\leq k}}{\\sum_S(\\mbox{argmax}_j {(\\sigma(\\overrightarrow{\\boldsymbol{z}}))}) - (\\mbox{argmax}_{j \\ne m} {(\\sigma(\\overrightarrow{\\boldsymbol{z}}))})} $$ where $\\mathcal{U}$ denotes the Data without lables i.e. $unlabeled_x$ and $k$ is the $budget$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3E0lcVz7PPo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71a187aa-0b69-416d-aa0a-ac99b88f3a23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Testing accuracy: 57.83\n",
            "-------------------------------------------------\n",
            "Round 1\n",
            "-------------------------------------------------\n",
            "Number of training points - 900\n",
            "Training..\n",
            "5 training accuracy: 0.43555555555555553\n",
            "10 training accuracy: 0.4955555555555556\n",
            "15 training accuracy: 0.5266666666666666\n",
            "20 training accuracy: 0.5522222222222222\n",
            "25 training accuracy: 0.5533333333333333\n",
            "30 training accuracy: 0.5533333333333333\n",
            "35 training accuracy: 0.5655555555555556\n",
            "40 training accuracy: 0.5688888888888889\n",
            "45 training accuracy: 0.5444444444444444\n",
            "50 training accuracy: 0.5555555555555556\n",
            "55 training accuracy: 0.57\n",
            "60 training accuracy: 0.5855555555555556\n",
            "65 training accuracy: 0.6\n",
            "70 training accuracy: 0.58\n",
            "75 training accuracy: 0.5988888888888889\n",
            "80 training accuracy: 0.5888888888888889\n",
            "85 training accuracy: 0.5866666666666667\n",
            "90 training accuracy: 0.5888888888888889\n",
            "95 training accuracy: 0.61\n",
            "100 training accuracy: 0.5933333333333334\n",
            "105 training accuracy: 0.5933333333333334\n",
            "110 training accuracy: 0.5955555555555555\n",
            "115 training accuracy: 0.5911111111111111\n",
            "120 training accuracy: 0.5855555555555556\n",
            "125 training accuracy: 0.6188888888888889\n",
            "130 training accuracy: 0.6055555555555555\n",
            "135 training accuracy: 0.59\n",
            "140 training accuracy: 0.62\n",
            "145 training accuracy: 0.6166666666666667\n",
            "150 training accuracy: 0.5866666666666667\n",
            "155 training accuracy: 0.5988888888888889\n",
            "160 training accuracy: 0.5977777777777777\n",
            "165 training accuracy: 0.5988888888888889\n",
            "170 training accuracy: 0.6122222222222222\n",
            "175 training accuracy: 0.6144444444444445\n",
            "180 training accuracy: 0.6044444444444445\n",
            "185 training accuracy: 0.6077777777777778\n",
            "190 training accuracy: 0.5944444444444444\n",
            "195 training accuracy: 0.5955555555555555\n",
            "200 training accuracy: 0.6044444444444445\n",
            "Epoch: 200 Training accuracy: 0.604\n",
            "Testing accuracy: 58.77\n",
            "-------------------------------------------------\n",
            "Round 2\n",
            "-------------------------------------------------\n",
            "Number of training points - 1200\n",
            "Training..\n",
            "5 training accuracy: 0.43833333333333335\n",
            "10 training accuracy: 0.4575\n",
            "15 training accuracy: 0.49333333333333335\n",
            "20 training accuracy: 0.5341666666666667\n",
            "25 training accuracy: 0.56\n",
            "30 training accuracy: 0.5375\n",
            "35 training accuracy: 0.5516666666666666\n",
            "40 training accuracy: 0.5383333333333333\n",
            "45 training accuracy: 0.5616666666666666\n",
            "50 training accuracy: 0.56\n",
            "55 training accuracy: 0.5375\n",
            "60 training accuracy: 0.5791666666666667\n",
            "65 training accuracy: 0.5625\n",
            "70 training accuracy: 0.5791666666666667\n",
            "75 training accuracy: 0.5508333333333333\n",
            "80 training accuracy: 0.58\n",
            "85 training accuracy: 0.5725\n",
            "90 training accuracy: 0.6008333333333333\n",
            "95 training accuracy: 0.5791666666666667\n",
            "100 training accuracy: 0.5775\n",
            "105 training accuracy: 0.57\n",
            "110 training accuracy: 0.5775\n",
            "115 training accuracy: 0.5858333333333333\n",
            "120 training accuracy: 0.5883333333333334\n",
            "125 training accuracy: 0.5716666666666667\n",
            "130 training accuracy: 0.5658333333333333\n",
            "135 training accuracy: 0.5866666666666667\n",
            "140 training accuracy: 0.5725\n",
            "145 training accuracy: 0.5808333333333333\n",
            "150 training accuracy: 0.5891666666666666\n",
            "155 training accuracy: 0.5891666666666666\n",
            "160 training accuracy: 0.585\n",
            "165 training accuracy: 0.585\n",
            "170 training accuracy: 0.5558333333333333\n",
            "175 training accuracy: 0.5758333333333333\n",
            "180 training accuracy: 0.5775\n",
            "185 training accuracy: 0.58\n",
            "190 training accuracy: 0.6016666666666667\n",
            "195 training accuracy: 0.5833333333333334\n",
            "200 training accuracy: 0.5925\n",
            "Epoch: 200 Training accuracy: 0.593\n",
            "Testing accuracy: 61.2\n",
            "Training Completed\n"
          ]
        }
      ],
      "source": [
        "strategy_args = {'batch_size' : 5}\n",
        "strategy = MarginSampling(tweet_train, tweet_unlabeled, model, nclasses, strategy_args)\n",
        "\n",
        "# Use the same training parameters as before\n",
        "args = {'lr':1e-4, 'batch_size':5, 'n_epoch': 200, 'optimizer':'adam'} \n",
        "dt = data_train(tweet_train, model, args)\n",
        "\n",
        "# Update the model used in the AL strategy with the loaded initial model\n",
        "strategy.update_model(model)\n",
        "\n",
        "# Get the test accuracy of the initial model\n",
        "acc = np.zeros(n_rounds)\n",
        "acc[0] = dt.get_acc_on_set(testing_dataset)\n",
        "print('Initial Testing accuracy:', round(acc[0]*100, 2), flush=True)\n",
        "\n",
        "# User Controlled Loop\n",
        "for rd in range(1, n_rounds):\n",
        "    print('-------------------------------------------------')\n",
        "    print('Round', rd) \n",
        "    print('-------------------------------------------------')\n",
        "\n",
        "    # Use select() to obtain the indices in the unlabeled set that should be labeled\n",
        "    idx = strategy.select(budget)\n",
        "\n",
        "    # Add the selected points to the train set. The unlabeled set shown in the next couple lines \n",
        "    # already has the associated labels, so no human labeling is needed. Again, this is because \n",
        "    # we already have the labels a priori. In real scenarios, a human oracle would need to provide \n",
        "    # then before proceeding.\n",
        "    tweet_train = ConcatDataset([tweet_train, Subset(tweet_unlabeled, idx)])\n",
        "    remaining_unlabeled_idx = list(set(range(len(tweet_unlabeled))) - set(idx))\n",
        "    tweet_unlabeled = Subset(tweet_unlabeled, remaining_unlabeled_idx)\n",
        "\n",
        "    print('Number of training points -', len(tweet_train))\n",
        "\n",
        "    # Update the data used in the AL strategy and the training class\n",
        "    strategy.update_data(tweet_train, tweet_unlabeled)\n",
        "    dt.update_data(tweet_train)\n",
        "\n",
        "    # Retrain the model and update the strategy with the result\n",
        "    clf = dt.train()\n",
        "    strategy.update_model(clf)\n",
        "\n",
        "    # Get new test accuracy\n",
        "    acc[rd] = dt.get_acc_on_set(testing_dataset)\n",
        "    print('Testing accuracy:', round(acc[rd]*100, 2), flush=True)\n",
        "\n",
        "print('Training Completed')\n",
        "\n",
        "#Saving accuracies for further analysis\n",
        "with open(os.path.join(base_dir,'margin.txt'), 'w') as f:\n",
        "    for item in acc:\n",
        "        f.write(\"%s\\n\" % item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3YXvHSxM4Cp"
      },
      "source": [
        "## Visualizing the Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nMpiERpM4Y7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "01b6e9ae-c0d7-4d15-8069-f4d948ae36a9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'DISTIL_Twitter')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOydd3hURduH70kI6dkklAAJKXQIJUEQlC4qSlE/wYqCgGJBEQV9BSxYwFeRomIX0VcjKCA2EJSmqKB0pCSACZAECCGk183ufH/MZlMhCdlNnfu69gpn9pQ5BJ4z5zfP/B4hpUSj0Wg0DQeHmu6ARqPRaKoXHfg1Go2mgaEDv0aj0TQwdODXaDSaBoYO/BqNRtPA0IFfo9FoGhg68Gs0Gk0DQwd+jaaGEUIcEkIMrul+aBoOOvBrai1CiBNCiGwhRLoQIkUI8acQ4iEhhIPl+0+FEK8U2X+SECLSsn+CEGKdEMJTCPGTECLD8jEKIfKKbL8vhBgshIgrcp6tQoj7K9jHAUXOlSmEkEW2M4QQgeWdQ0oZKqXcajnfHCHEFyWuUeH+aDQVoVFNd0CjKYdRUsqNQggDMAh4E+gDTCi6kxBiEDAPuEFKuVcI4QuMApBS3lhkv0+BOCnls0XaBl9u56SU2wAPy3mCgRjAW0qZf7nntDVCCEcppamm+6GpPegRv6ZOIKVMlVJ+D9wBjBdCdC2xS29gu5Ryr2X/C1LKz6SU6dXdVwAhxBAhxD9Ftn8RQuwssr1NCHGL5c8nhBDXCiFuAGYBd1jeFvYLIeYCA4AllrYllmM6Wc55QQgRJYS4vci5PxVCvGd548kEhlTTbWvqCHrEr6lTSCn/tsgyA0p89RfwshDiReBnYJeUMrfaO1jIDqC9EKIpkAp0B/KFEJ5APtAL2Fb0ACnleiHEPKCdlPKegnYhRD/gCynlx5Ztd+AX4HngRqAb8IsQ4qCU8rDlsLuB4cBIoLH9blNTF9Ejfk1d5DTgW7TBIrncCvQE1gJJQoiFQgjHGugfUspsYCcwELgC2A/8AfQD+gLHpJRJl3n6kcAJKeUyKWW+5S1nNXBbkX2+k1L+IaU0SylzLvtGNPUSPeLX1EX8gQslG6WUPwE/WSZ/hwArgSjgg+rtnpVfgcFAnOXPyah5ilzL9uUSBPQRQqQUaWsEfF5kO7YK59fUc/SIX1OnEEL0RgX+3y+2j2WUuwnYDJScC6hOCgL/QMuff0UF/kFcPPCX5ZNesi0W+FVK6V3k4yGlfLic82g0gA78mjqCEMJLCDESWIHSu/8p8f3NQog7hRA+QnElKsDuuMxLNhJCuBT5OF3GOf4EOgJXAn9LKQ9hGa0Dv13kmAQguCBltUhbmyLbPwIdhBD3CiGcLJ/eQojOl9FHTQNEB35NbecHIUQ6apQ7G1hIiVROC8nAA8AxIA34ApgvpYy4zOu+B2QX+Syr7AmklJnAHuCQlDLP0rwdOCmlPHeRw1ZafiYJIfZY/vwmMEYIkSyEeMuSqXQ9cCdqvuMs8BrgXNk+ahomQlfg0mg0moaFHvFrNBpNA0MHfo2mAgghxpawYij4HKrpvmk0lUVLPRqNRtPAqBN5/E2bNpXBwcE13Q2NRqOpU+zevfu8lLJZyfY6EfiDg4PZtWtXTXdDo9Fo6hRCiJNltWuNX6PRaBoYOvBrNBpNA0MHfo1Go2lg1AmNvyyMRiNxcXHk5GjjwZrCxcWFgIAAnJwux81Ao9HUFHU28MfFxeHp6UlwcDBCiJruToNDSklSUhJxcXGEhITUdHc0Gk0lqLNST05ODk2aNNFBv4YQQtCkSRP9xqXR2ImEhAi2bw9m61YHtm8PJiHhcm2nSlNnR/yADvo1jP7712jsQ0JCBFFRkzGbswDIzT1JVNRkAPz8xlb5/HV2xK/RaDT1lejo2dagX4DZnEV09GybnF8H/irg6OhIWFgYXbt2ZdSoUaSkpBT7PiwsjDvvvLNY23333Ye/vz+5uaoc7Pnz5ylYlXzixAlcXV0JDw+nc+fOXHnllXz66afFjv/222/p3r07nTt3plu3bnz77bfFzu3m5kZ6emF98WnTpiGE4Pz58za8c41GY09yc09Vqr2yNJjAHxEBwcHg4KB+RthALnN1dWXfvn0cPHgQX19f3nnnHet3R44cwWQysW3bNjIzM4sd5+joyCeffFLmOdu2bcvevXs5cuQIK1asYPHixSxbpqzg9+/fz4wZM/juu+84cuQI33//PTNmzODAgQPW49u1a8d3330HgNlsZvPmzfj7+1f9ZjUajd3Jz08nKupBLlZAzdk50CbXaRCBPyICJk+GkydBSvVz8mTbBP8CrrrqKuLj463by5cv59577+X666+3BuICpk2bxqJFi8jPz7/kOdu0acPChQt56623AHjjjTeYNWuWNYsmJCSEmTNnMn/+fOsxd955J1999RUAW7dupV+/fjRqVKencjSaBkFKym/s2tWDM2c+wtd3JA4ObsW+d3Bwo02buTa5Vr2ICNOmwb59F/9+xw6wKCtWsrJg0iT46KOyjwkLg8WLK3Z9k8nEpk2bmDRpkrXtq6++4pdffiEyMpK3336bu+++2/pdYGAg/fv35/PPP2fUqFGXPHfPnj2JjIwE4NChQ8yYMaPY97169Sr2ptGhQwe+//57kpOTWb58Offccw8//fRTxW5Eo9FUOyZTNjExs4mLW4yLSxvCw7dhMPQjISGC6OjZ5Oaewtk5kDZt5tpkYhfqSeAvj5JBv7z2ipKdnU1YWBjx8fF07tyZ6667DoBdu3bRtGlTAgMD8ff3Z+LEiVy4cAFfX1/rsTNnzuTmm29mxIgRl7zG5dhm33rrraxYsYK//vqLDz74oNLHazSa6iEtbSeRkePIyoqkVatHaNv2dRwd3QGVvWOrQF+SehH4yxuZBwcreackQUGwdevlX7dA48/KymLYsGG88847TJ06leXLlxMZGWmdtE1LS2P16tU88MAD1mPbt29PWFgYX3/99SWvsXfvXjp3VjW0u3Tpwu7du+nRo4f1+927dxMaGlrsmDvuuIMrrriC8ePH4+DQINQ8jaZOYTbncfLkK5w8OQ9n55Z07/4zvr7XFd8pIgJmz4ZTpyAwEObOhbE2ehBIKe32AbyBVUAkcAS4CrgNOASYgV4VOc8VV1whS3L48OFSbRfjiy+kdHOTUin86uPmptqrgru7u/XPe/bskYGBgTI3N1cGBATI+Ph463ebN2+WQ4YMkVJKOX78eLly5UoppZQHDx6UQUFBMigoSEopZUxMjAwNDbUeFxMTI8PDw+Unn3wipZRy7969sl27djImJsb6fdu2beXevXtLnfv999+Xx48fl1JKGRQUJBMTE6t2sxehMr8HjUYjZXr6P3LnznC5ZQvy8OHxMi8vufRONgpawC5ZRky194j/TWC9lHKMEKIx4AakALcC1aZBFDwk7fXwBAgPD6d79+68+uqr+Pv706pVK+t3AwcO5PDhw5w5c6bYMaGhofTs2ZM9e/ZY2/7991/Cw8PJycnB09OTqVOnct999wEqPfS1115j1KhRGI1GnJyceP311wkLCyvVnwcffNB2N6fRaKqMlCZiYxcQE/McjRp5Exq6hmbNbim+U04O/PwzPPigmogsSlaWCmI2CFx2K70ohDAA+4A2soyLCCG2AjOklOVWWOnVq5csWYjlyJEjVglEU3Po34NGUz5ZWceJjBxPWtqfNG06mg4d3qNxY0thrMxM+OknWLUK1q6FjIyLn0gIMJsrfF0hxG4pZa+S7fYUgEOARGCZEGKvEOJjIYR7RQ8WQkwWQuwSQuxKTEy0Xy81Go3GTkhpJj7+HXbt6kFW1mE6d44gNHQljbMbKw3/1luhWTO47TbYtAnuugvWr1eyRFlcrL2S2DPwNwJ6Au9JKcOBTOCZih4spfxQStlLStmrWbNSJSM1Go2mVpOTc4oDB4Zx7NijeHsPpHf7bfity0WMGgXNm8M996hc84kTYfNmOHMGPvwQhg2DefPArXgeP25uSqO2AfbU+OOAOCnlX5btVVQi8Gs0Gk1dRErJ2bOfcfz440hzPh3OjqXlvATElnDIz1ej9ilTYPRouOoqZSdQEjtPTNot8EspzwohYoUQHaWUUcBQ4LC9rqfRaDQ1TW7uWY4eGE9S5s8YThjo9Gw2rvER0K4dTJ+ugn2vXkqrL4+xY22bgVIEe2f1PAZEWDJ6ooEJQoj/A94GmgFrhRD7pJTD7NwPjUajsR8nTnBu0/McbbEck1M+bT+CgCOtEBOnqmDfvXvFgn01YdfAL6XcB5ScUV5j+Wg0Gk3d5ehRWL0a47qvODZoP+euBc9TrnRKfBj3lx6GWpztppd1VoECW+aCz3//+18ABg8eTK9ehc+7Xbt2MXjwYDZs2GDd18PDg44dOxIWFsa4cePYunUrBoOBsLAwOnXqVMqTpzw75pCQEMLCwujZsyfbt29n7NixvPfee9Z9/vrrL7p3747RaLTz34pGU0+REg4dghdfVCP4jh1J+nYWO586TOI1DgR7TSP8njTcp79Vq4M+YN+Vu7b6VHXlrpRSfnHgCxm0KEiKOUIGLQqSXxyo4rJdWXzlblEGDRokW7duLdetWyellHLnzp1y0KBBpfbZuXOndXvLli1yxIgRUkops7KyZMeOHeXvv/8upZRy3759sm3btjI6OlpKKWV0dLRs27at3L9/v5Sy+IrdDRs2yG7dusmzZ8/KkJAQee7cOWkymWSvXr3ktm3bqnzPJdErdzX1GrNZyt27pZw1S8qOHdUKWiGk8Zq+MnJlX7llC/Lvv7vKtLQ9Nd3TMqGGVu7WCiL+iWDyD5PJMqqVcCdTTzL5B1XGbGw3+0yePPXUU8ydO5cbb7yx0se6urpazd/g0nbMn3/+ebFjBw4cyPHjx/Hz82PGjBk8/fTT9O7dm+7du9O/f/+q35hGU98xm+Hvv2H1avWJiVGZN4MHw9SppAxrQWTidHJyTtG69X8ICXkRBwfnmu51pagXgX/a+mnsO3txX+YdcTvINRW34swyZjHpu0l8tLtsX+awFmEsvuHS7m8F7pwFzJw5kzvuuANQ/vxr1qxhy5YteHp6VvRWAEhOTubYsWMMHDgQqJgdcwE//PAD3bp1A+Chhx7is88+Y+vWrZRc+azRaIpgMsEff6hA/803EBcHTk4wdKhKqbz5Zkw+7so+OfZRXF3bWuyTr67pnl8W9SLwl0fJoF9ee0UpcOe8GM8++yyvvPIKr732WoXOt23bNnr06MGxY8eYNm0aLVq0qHBfnnrqKV555RWaNWvG0qVLAXBwcODBBx9k165dNGnSpMLn0mgaBPn5yp539WpYswYSEsDZWS2gmjsXRo0CHx/AYp+8u8A+eQpt275mtU+ui9SLwF/eyDx4cTAnU0v7MgcZgth631Y79QquueYann32WXbs2FGh/QcMGMCPP/5ITEwMffv25fbbbycsLKxCdszz589nzJgxpc7p4OCgrZk1mgJyc5U1wurV8O23cOGCWhE7fDiMGaN+FnlDV/bJL3Py5Ks4O7eie/df8PW9tgZvwDY0iIgwd+hc3JyKL392c3Jj7lDbLH++FM8++yyvv/56pY4JCQnhmWeesb4pzJgxg1dffZUTJ04Aqij7vHnzmD59uq27q9HUP7KzVZC/915llTBiBKxcCTfcoGSdxES1fccdxYJ+RsY/7NnTh5MnX8HP7x569/6nXgR9qCcj/vIomMCdvWk2p1JPEWgIZO7QuVWe2C2p8d9www3WlM4Chg8fzuV4DT300EO88cYbnDhxolJ2zBqNBuVwuW6dGtmvXascMH18lCnamDFw7bVK1ikDZZ/8BjExz9OokTddu35L06Y3V/MN2Be72TLbEm3LXHvRvwdNrSE1FX74Qdkbb9igvO2bN4f/+z+1enbwYDVhewmyso5Z7JO306zZGNq3f4/GjZtWT//twMVsmRvEiF+j0dRTkpLgu+9UsN+4EYxGaNUKHnhABfv+/cHRsdzTKPvkd4mOfhoHB2c6d46gefO7ELXIZsGW6MCv0WjqFmfPKs1+1SqVlWMyqQLaUy2+OH36lO14eRFyck4RGTmRlJRN+PreQMeOS3F2blX+gXUYHfg1Gk3tJy5OTcSuWgW//67sE9q3h6efVsG+Z89Km6DJIvbJYKZDhw9p2fL+ejvKL4oO/BqNpnYSE6MmZ1etgr8sZT1CQ+H551Ww79r1sh0vc3PPcvTogyQlfY/BMJBOnT7F1TXEhp2v3ejAr9Foag9RUSrQr14Ne/eqtp491YKq0aOhY8cqX+LcuZUcPfowJlMGbdsuJCDgcYRoEJntVnTg12g0NYeUcPBg4cj+0CHV3rcvzJ+v0i/btLHJpYzGCxw79ijnzi3H07M3nTp9hrt7w8xIa1iPORtTYMvco0cPevbsyZ9//lns+8WLF+Pi4kJqaqq1rcB+OTw8nI4dOzJw4EB+/PHHYsd98cUXdO/endDQUHr06MH9999PSkoKoCyfC+ycw8LCylytq9HUaqSE3bth5kw1gu/eHV56CZo0gTffhNhY2L4dZsywWdBPSlrHzp1dSUxcSXDwy4SH/9lggz7QgGyZz56VQX/+KcWWLTLozz/lF2fPVur4sihqy7x+/Xo5cODAYt9feeWVsn///vKTTz6xthW1X5ZSyr1798qgoCC5ceNGKaWUP/30k+zZs6eMi4uTUkqZn58vly5dKiMjI6WUpe2caxpty6ypECaTlH/+KeWTT0oZHKzsjR0dpbz2Winfe0/KM2fsclmjMU1GRt5f6+2T7QUN2pY5IYHJUVFkmc0AnMzNZXJUFABj/fxsco20tDR8LIZOAP/++y8ZGRm8++67zJ07lwkTJpR5XFhYGM8//zxLlixh6NChzJ07lzfeeAN/f39AvVVMnDjRJn3UaKoVk0ll4KxapTJyTp9WC6iuuw6eew5uvlmN8u1EcvJWoqIm1Gn7ZHtRLwL/tGPH2JeRcdHvd6SlkVtihXKW2cykyEg+On26zGPCPDxY3L79Ja9bYNmQk5PDmTNn2Lx5s/W7FStWcOeddzJgwACioqJISEjA7yIPmZ49ezJ//nxAWTD37NnzktcdO3Ysrq6uAFx33XXWYzWaGsdoVLn1q1apXPtz58DFRfnijB4NI0eCt7ddu2AyZRMTM4u4uMW4urar0/bJ9qJeBP7yKBn0y2uvKEVtmbdv3864ceM4ePAgQgiWL1/OmjVrcHBwYPTo0axcuZJHH320zPPIi/Tjn3/+4d577yU9PZ158+ZZvf4jIiKKlXbUaGqU3Fy1anbVKrWKNjkZ3N2VGdro0crx0sOjWrqSlvY3R46MIzs7Cn//R2nT5r912j7ZXtSLwF/eyDx4+3ZO5pb23g9ydmZreLhN+nDVVVdx/vx5EhMTSUhI4NixY1x33XUA5OXlERISctHAv3fvXqvfTWhoKHv27GHIkCF069aNffv28eijj5KdnW2Tfmo0NiErC9avV9k4P/4IaWlgMCgP+9Gjlae95a20Oqiv9sn2okFk9cxt0wa3Eku43RwcmGujjAGAyMhITCYTTZo0Yfny5cyZM4cTJ05w4sQJTp8+zenTpzl5snRNgAMHDvDyyy8zZcoUQFXxmjFjBnFxcdZ9dNDX1ArS02HFCrjtNmjWTAX4DRuU2+W6dUrW+fxzuOWWag36Re2TW7S4t17ZJ9uLejHiL4+CCdzZ0dGcys0l0NmZuW3aVHlit6gts5SSzz77DEdHR1asWMG6deuK7ft///d/rFixgj59+rBt2zbCw8PJysqiefPmvPXWWwwdOhRQNs6JiYnceOONmEwmvL296dq1K8OGDSu8nyIaf9OmTdm4cWOV7kOjuSgpKfD992pkv2GDknX8/GDcOBXwBw2CRjUTRqQ0cerUfE6ceJ5GjXzo2vU7mja9qUb6Yg8iEhJsHrMK0LbMmiqhfw/1kPPn1cTs6tWqWpXRCAEBhV72V19dIcdLe6Lsk8eRlrajXtgnl6RkJiIoleLDjh0rFfy1LbNGo7k4Z86ourOrV8Ovv6pUzJAQmDZNSTq9e1fK8dJeFLdPdqFz5y9p3vzOemWsFpeTw7Rjx4oFfVCZiLOjo20y6teBX6NpqJw6pfLrV6+GP/5QK2o7doRnnlHBPizssk3Q7IGyT55ASspmfH1vpGPHj+u8fbJZSo5kZfF7airbUlL4PTW1zESUAk5d4rvKoAO/RtOQ+PdfFehXr4a//1Zt3brBnDkq2HfpUquCPRTYJ3/K8ePTUPbJH9Gy5aQ6OcrPM5vZnZ7OttRUfk9N5Y/UVC7k5wPg5+TEAG9vnjAY+O+pU5zNyyt1fOBFykVWFh34NZr6zpEjhcHesu6EXr3g1VdVsC8nHbomUfbJk0lK+gGDYRCdOi2rU/bJqfn5bE9NtQb6v9PTybFIOB1cXfm/pk3pbzDQ32Cgraur9WHW1MmpTI3fVpmIOvBrNPUNKeHAgcJgf/iwar/qKliwQE3SBgfXaBcrQoF9stmcSdu2iwgImFrr7ZPjc3P53RLkt6WkcCAzEwk4Aj09PXm4VSsGGAz0Mxho3rjxRc9jr0zEAnTg12jqA1LCrl2Fwf74cTUZO3AgPPywKjhu8X+q7Sj75CmcO7fCYp/8P9zdO9V0t0ohpSQyK8s6mv89NZWYnBxAjc6v8vLiheBg+hsM9PH0xKOSaa9j/fxsFuhLogN/FfDw8CCjhEfQnDlz+Oijj2jWrJm1bevWrXhb/EmmTZvGypUriY2NxcGSJZGQkMCkSZOIjY3FaDQSHBzMa6+9xr333gvAqVOnMBgMGAyGMvP2z549y7Rp09i5cyfe3t74+fmxePFiOnTowKFDh3jssceIj4/HbDYzbtw4nn32WYQQfPrpp0ycOJF9+/bRvXt3ALp27cqPP/5IcHAwwcHBeHp64mhJ3Xv33Xe5+mrteVJrMJuVfXFBsD91SuXUX3MNPPWUWkjVvHlN97JSJCWtIyrqfozGRIKDXyYw8BkcHGpHmMozm9mTnq5G8xZ9Psmizzd3cqK/wcBUf3/6Gwz08PDAqRZkQV2Usiw7a9vHFrbM8osvpAwKklII9fOLLyp3fBkUtWUu4IUXXpDz588vc3+TySQDAwNlnz595ObNm63tkydPlosXL7Zu79+/v9hx48ePlytXrizznGazWfbt21e+99571rZ9+/bJ3377TWZlZck2bdrIDRs2SCmlzMzMlDfccINcsmSJlFLKZcuWydatW8vbb7/demxoaKiMiYmRUkoZFBQkExMTL/VXoG2ZqxujUcrNm6WcMkXKli2VvXHjxlKOHCnlsmVSJiXVdA8vC6MxtYR98t6a7pJMNRrl+qQk+Wx0tBy8d690/fVXyZYtki1bZPsdO+SEI0fk0tOn5dHMTGk2m2u6u2VCQ7ZlJiICJk9W/iIAJ0+qbYCxY6utG1u3biU0NJQ77riD5cuXM2TIEADOnDnD9ddfb92vYPRdEbZs2YKTkxMPPfSQta1Hjx4ALF26lH79+lnP7ebmxpIlSxg8eLDVImLkyJH89ttvREVF0dEGZe00dsBohM2b1ah+zRq1wMrVFW68sdDx0surpnt52SQnbyUy8j5yc2MJDHyG4OA5NWKffMaizxdIN/szMjCjfG3CPTx4sFUr+hsM9PPyooWNsmtqCrsGfiGEN/Ax0BWQwEQgCvgKCAZOALdLKZOrdKFp0wqzFcpixw611LwoWVkwaRJ89FHZx4SFweLFl9WdRYsW8cUXXwDg4+PDli1bAFi+fDl33XUXN998M7NmzcJoNOLk5MSUKVO44447WLJkCddeey0TJkygVauK5ScfPHiQK664oszvDh06VOq7tm3bkpGRQVpaGgAODg48/fTTzJs3j88++6zUOYYMGYKjoyPOzs78VVDwWmN/cnLgl19UsP/uO2Wd4OGhgvzo0Srou9dt10mTKZvo6JnEx79Z7fbJUkqiLPnzBcE+uog+39fLi2eDghhgMNDHywvPGrKlsBf2vps3gfVSyjFCiMaAGzAL2CSl/K8Q4hngGeA/du3FxRY92GgxREmeeOIJZsyYUawtLy+PdevWsXDhQjw9PenTpw8bNmxg5MiRDBs2jOjoaNavX89PP/1EeHg4Bw8eLDZPYE/uvvtu5s6dS0xMTKnvtmzZQtOm9WcpfK0mM7O442V6uvKuv+kmFeyvv15529cD0tL+4siR8dVmn2w0m9mbkVFsIva80QhAM4s+P8Wiz4fXdn3eBtgt8AshDMBA4D4AKWUekCeEuBkYbNntM2ArVQ385Y3Mg4OVvFOSoCBVNKIa2LBhAykpKXTr1g2ArKwsXF1dGTlyJAC+vr7cfffd3H333Vb5ZfTo0eWeNzQ0lFWrVpX5XZcuXfjtt9+KtUVHR+Ph4YFXEWmgUaNGTJ8+nddee+1yb09zuaSlwdq1ysv+p58gOxuaNoU77lDB/ppr4BJpf3UNszmPEyde4tSpV3F29qdHj434+Ay1+XXS8/PZkZZmHc3vSEsj25IT39bFhRG+vgzw9qa/wUCHIvnzDQV7jvhDgERgmRCiB7AbeBzwk1KesexzFigzX0kIMRmYDBAYGFi1nsydW1zjB3BzU+3VxPLly/n444+56667AMjMzCQkJISsrCx27NhB3759cXNzIz09nX///bfC93zNNdcwa9YsPvzwQyZb5i0OHDhAamoqY8eOZd68eWzcuJFrr72W7Oxspk6dytNPP13qPPfddx+vv/466enptrtpTdkkJyvHy1Wr4OefIS8PWrSACRNUsB84sMYcL+1JRsYBjhwZR2bmflq0mEC7doto1Mhgk3OfLZI//3tqKvsyMjCh9PkwDw8eaNnSulCqZR3X522BPf91NQJ6Ao9JKf8SQryJknWsSCmlEKJMe1Ap5YfAh6DcOavUk4IJ3NmzVcpbYKAK+lWc2M3KyiIgIMC6/eSTTwLFNX6AL7/8kvXr1/P+++9b29zd3enfvz8//PADp06d4tFHH6VRo0aYzWbuv/9+evfuXaE+CCFYs2YN06ZN47XXXsPFxYXg4GAWL16Mq6sr3333HY899hhTpkzBZDJx7733llkQpnHjxkydOpXHH3/8cv86NJciMUTLSY8AACAASURBVFE5Xq5apSZq8/OhdWt45BHleHnVVbXCBM0emM35xMa+YbFP9q2yfbKUkmPZ2cUmYo9bala4OjjQx8uLWUFB9DcY6OvlhVc9fIhWFbvZMgshWgA7pJTBlu0BqMDfDhgspTwjhGgJbJVSXjKdRNsy11707+ESnD6tsnBWrYLfflN5923bqlF9geNlPZcYsrKOEhk5vkr2yfkWfb7oiP6cRZ9v0qiRdSQ/wNubcA8PGtfTB+jlUO22zFLKs0KIWCFERyllFDAUOGz5jAf+a/n5nb36oNHYjYiIst8gT55UjperVqnFVVJC584wa5Ya2XfvXu+DPRTYJ79DdPR/Km2fnJGfz1/p6Va3yh1paWRa9PkQFxdu8PW1BvtObm4NTp+3BfZ+B3oMiLBk9EQDE1Cy29dCiEnASeB2O/dBo7EtZa0LmTABnnsOCjKjevSAF18sdLxsQOTknCQycmKF7ZMT8vL4o0ha5d70dEyAAHp4eDChZUurv42/1udtgl0Dv5RyH1DqNQM1+tdo6iazZxdPFAC1yCo+Hv77XxXs27Wrmb7VINJqn/w4IMu0T5ZSctyizxcE+mMWfd7FwYE+np48ExhIf4OBqwwGDFqftwv6b1WjqSynTpXdbjTCf+y7JKW2ouyTHyAp6cdi9sn5ZjP7Lfp8gXSTYNHnfRs1op/BYM246enpibPW56sFHfg1mooSFwczZyrdviyqmnZcRzl37muLfXIWrdss4KTHeOYnpPN76n62p6WRYTIBEOziwnUWfX6ARZ930Pp8jaADv0ZTHllZMH8+vPaaysy56SZlp2CRKIBqXxdSGzAak/gn6hHSzn9NslN3lro8z4aYpuTLfxBAN3d3xvv5WSdiA+rJquP6gH6vqgJCCO655x7rdn5+Ps2aNbOuxi3glltuoW/fvsXa5syZg7+/P2FhYXTp0oXly5cX+37hwoV06tSJbt260aNHD5588kmMllfk4OBgzp8/b+3D9OnTrce98cYbzJkzx5a32XCREr78UtWhnTNH+eRERirvnI8+Uiu/hVA/P/ywWg3/agIpJf9mZ/PZ2bO8sO991v3RkQvnV/MxkxhrXEiaUxueat2add26caFfP/b37s2SDh24089PB/1aRoMZ8SckRBAdPZvc3FM4OwfSps1c/Pyq9h/V3d2dgwcPkp2djaurK7/88gv+JYpdpKSksHv3bjw8PIiOjqZNkdJpBZ4+x44d44orrmDMmDE4OTnx/vvv8/PPP7Njxw68vb3Jy8tj4cKFZGdn4+TkVOz8zs7OfPPNN8ycOVN76tiSv/9W5n/bt0N4uMrkGTiw8PuxY+t9oDdJyYES/japeck8wruMYB2Jju051eJzHm5+NUu1Pl+naBCBPyEhgqioyZjNKhMjN/ckUVHK3qCqwX/48OGsXbuWMWPGWN03t23bZv3+m2++YdSoUfj5+bFixQpmzZpV6hzt27fHzc2N5ORkmjdvzty5c/ntt9+sxVsaN27MM888U+o4UD47kydPZtGiRcxtYFKDXYiPVzr+558rG4VPPoFx48BSjKY+k2Uy8XdamjXQb09LI92izwc6O3O3WxTDzM/jlH+a1q3/w8CQF2vEPllTdepF4D92bBoZGRe3ZU5L24GUxZ04zeYsIiMncfp02bbMHh5htG9fvi3znXfeyUsvvcTIkSM5cOAAEydOLBb4ly9fzvPPP4+fnx+jR48uM/Dv2bOH9u3b07x5c9LS0sjIyCAkpOIFpadMmUL37t3L9ODRVJDsbHjjDZWOaTKp4D9zJnh61nTP7Mb5vDz+KDAyS0lhd0YG+VIigK7u7tzj58cAg4GrPBtjjH/RYp/cnk7dfsdguKqmu6+pAvUi8JdHyaBfXntl6N69OydOnGD58uUMHz682HcJCQkcO3aM/v37I4TAycmJgwcP0rVrV0B5+ixbtoyjR4/yww8/lHn+DRs28J///IeUlBS+/PLLMksfenl5MW7cON566y1cXV2rfE8NCinhq6/g6achNlbl4M+fD5V48NYFpJScyMkpJtscsaxFaCwEvT09mdG6Nf0NBq728sLHIimmpf3FkX+qzz5ZUz3Ui8Bf3sh8+/ZgcnNL2zI7OwcRHr61yte/6aabmDFjBlu3biUpKcna/vXXX5OcnGwdvaelpbF8+XKrJFOg8X///fdMmjSJf//9Fy8vLzw8PIiJiSEkJIRhw4YxbNgwRo4cSV5e3kX7MG3aNHr27MmECROqfD8Nhp07lY7/55+q8M7nn8OgQTXdK5tgkpJ/CvLnLYH+tOXfj8HRkX4GA+MsGTe9PD1xKSFlKfvkFzl16r92tU/W1AzlBn4hhKOU0lQdnbEXbdrMLabxAzg4uNGmjW008YkTJ+Lt7U23bt3YWsTff/ny5axfv56rrlKvxTExMVx77bWltPibbrqJpUuX8tlnn/Hggw8yc+ZMHn74YVasWIG3tzdSSnIs1YEuhq+vL7fffjtLly5l4sSJNrmvesvp00rG+d//VDHyjz+G++6r0zp+tsnE35ZC4L+npvJnaippFn0+wNmZQRbv+QEGA6Hu7pfMn1f2yfeSmXnA5vbJmtpBRUb8x4QQq4FlUsrD9u6QPSiYwLV1Vk8BAQEBTJ06tVjbiRMnOHnyZLE0zpCQEAwGQ5klDJ9//nnuvvtuHnjgAR5++GEyMzPp06cPzs7OeHh40K9fP8LDwy/Zj+nTp7NkyRKb3FO9JDsbFi6EV18tXGU7a1adrFebZDTyZ5HR/K70dIyWhWWhbm7cXSR/PqiCqZTKPnk+J068YBP7ZE3tpVxbZiGEJ3AnhQZrnwArpJRp9u+eQtsy117qxO9BSli5Uun4J0/CrbfC668ri+Q6gJSSkzk5xfxtDlv0eSeLPl8wmr/aYMC3RMpvRShun3wb7du/W2n7ZE3t47JtmaWU6cBHwEdCiEHAl8AiIcQq4GUp5XGb91ajsRW7dysd//fflWPmsmUwZEhN9+qSmKTkUGZmsYnYOEt9aC+LPj/WMqLv7emJaxUkKmWfvITo6Gcs9snL8fO701a3oqmlVEjjB0agRvzBwAIgAhgArAM62LF/Gs3lceaMknE++0zVsP3wQ5g4sVbq+DkmEzst+vw2iz6fatHnWzVuzABLkZH+BgNd3d1xtJG/jbJPnkBKyhZ8fYfTseNHl7RP1tQfKqTxA1uA+VLKP4u0rxJCDLzIMdWClFIXYahB7FW9rUrk5Cgdf948Vct2xgxlo2yoPZOTyUaj1X/+99RUdqank2f5u+zi5sYdzZtbpZsgFxeb/xtX9snLOH58GhezT9bUbyoS+LtLKTPK+kJKObWs9urAxcWFpKQkmjRpov/B1gBSSpKSknCpLR4sUqqqV089pXT8W25R+fi1wBf/lEWfL5BuDmZmAkqfv8LTk8cDAqz5800bN7ZrX3Jzz3D06GSSkn7E23swHTsuw9U12K7X1NQ+KhL43xFCPC6lTAEQQvgAC6SUNZozGBAQQFxcHImJiTXZjQaNi4tLsWLzNcaePUrH37ZNlTbctAmuucaul4xISGB2dDSncnMJdHZmbps2jPXzw2zR54tOxMZa9HlPR0eu9vLijmbNGODtTW9PT9yqUXoqap/crt1i/P0fQwjtr9MQqeiIP6VgQ0qZLIS4dF5hNeDk5FQpWwNNPeTsWSXjLFumdPwPPoBJk+yu40ckJDA5KoosSx3Yk7m5TIiM5I1TpziRm0tKfj4ALS36/FOWtMruHh420+crg9GYxNGjU0hM/ApPzyvp1Okz3N07VXs/NLWHigR+ByGEj5QyGUAI4VvB4zQa+5CTA4sWKR0/NxemT4dnn602HX92dLQ16BdglJKDWVnc16IFAyyBPsQO+nxlSUpaS1TU/RiNSYSEvELr1v/BwUH/923oVORfwAJguxBiJar+8RhA20Bqqh8p4ZtvlI4fEwM336x0/Pbtq+nykvUXLnAyt2yPJ5OUfNSxY7X0pTzy89M4fvwJzp79BHf3bnTr9hOenmE13S1NLaEiefz/E0LsBgqSn2+tqyt4NXWYvXvhiSfg11+ha1fYuBGGVo93TI7JRMS5cyyMjeVwVhaOQFkeJoHOtcOiODl5C5GRE8jNjSUwcCbBwS9o+2RNMSr0zielPCSESARcAIQQgVLKi1Sc1mhsSEKCknGWLoUmTeC99+D++6GR/eWK83l5vHf6NEvi4zlnNNLD3Z3/deqEWUoeOXasmNzj5uDA3CJFdmoCkymL6OiZxMe/hatre8LDtX2ypmwqsoDrJpTc0wo4BwQBR4BQ+3ZN06DJzYXFi1Ud2+xsNdp/7jmwFKexJ1FZWSyKjeWzhARyzGaG+/oyvXVrhnh7WzX7Rg4OZWb11BSpqTuIjBxPdvZR/P0fs9gnu9VYfzS1m4oMm14G+gIbpZThQoghwD3lHKPRXB5Swpo1SsePjoZRo1SBlA72XSAupeS31FQWxMbyQ1ISzkJwb4sWPBEQQBf30v7zY/38ajTQF1DcPjmAHj024eNj31RWTd2nIoHfKKVMEkI4CCEcpJRbhBDll6bSaCrLvn1qZL91K4SGws8/w3XX2fWSRrOZVYmJLIiNZXdGBk2dnHg+KIhH/P3xs/NiqqqSkbGfI0fGaftkTaWpSOBPEUJ4AL8BEUKIc0CmfbulaVAkJCgZ5+OPwdcX3n0XHnjArjp+an4+H585w5txccTm5tLB1ZX3O3RgnJ9flUzPqgNln/w6J07Msdgnf0/TpqNquluaOkRF/mfdDGQDTwBjAQPwkj07pWkg5ObCW2/Byy8rHX/aNPUA8PGx2yVP5uTwZlwcH585Q7rJxCCDgXfat2dEkyaXLE5SW8jKiuLIkfGkp/9Fs2a306HDuzg5NanpbmnqGJcM/BZnzh+llEMAM/BZtfRKU7+REr77Thmo/fsvjBgBCxaAHXPgd6alsSA2llUWi487mjfnydatuaKOFFMvbp/sSpcuK2je/I6a7pamjnLJwC+lNAkhzEIIg5Qytbo6panHHDigRvZbtkCXLrB+PQwbZpdLmaXkh6QkFsTGsi01FS9HR55o3Zqp/v60ri3mchWgtH3yxzg7t6zpbmnqMBWRejKAf4QQv1BE269JZ05NHeTcuUId39sbliyBBx+0i46fZTLx2dmzLIqL41h2NoHOzixs25ZJLVviVQ35/7ZC2Sd/wvHjTwCSjh0/pkWLiTVuA6Gp+1Tkf8E3lo9GU3ny8uDtt+GllyArCx57DJ5/Xk3i2pizubm8c/o078XHk5SfT29PT1Z06cLopk1p5FC3XChzc88QFfUAFy6s1fbJGptTEcsGretrKo+U8MMPykDt+HEYPlzp+J1s7wp5MCODRXFxfJGQgFFKbmrShOmtW9PfYKiTo+Nz577i6NFHtH2yxm5UZOVuDFCq1JKUsmbXp2tqL//8o/LxN22Czp3hp5/ghhtsegkpJRuTk1kQG8uG5GRcHRyY1LIlTwQE0N6tbq5YVfbJj5CY+DWenlfSufP/cHOrHaZvmvpFRaSeohXaXYDbANu/p2vqPomJSsb58ENlkfzWW/DQQ+DkZLNL5JnNLLcYph3IzMTPyYlXQkJ4qFUrmtjwOtXN+fM/cvToAxb75Lm0bv20tk/W2I2KSD1JJZoWW9w6ny/vWCHECSAdZWaYL6XsJYToAbwPeAAngLFSyrRK9ltTm8jLU5O1L70EGRnw6KPwwgs21fEvGI18cPo0b8fHcyYvj67u7nzSsSN3+/nhXMf0+6KUtE/u3n09Hh49arpbmnpORaSenkU2HVBvAJUZigyRUp4vsv0xMENK+asQYiLwFPBcJc6nqS1ICT/+qHT8Y8eUnLNwoZJ3bMS/2dksjovjkzNnyDKbuc7Hh2WdOnG9j0+d1O+Lkpy82WKfHKftkzXVSkULsRSQD8QAt1fhmh1Q9g8AvwAb0IG/7nHwIDz5JPzyi5qwXbcObrzRZqf/02KYtub8eRoJwVg/P54ICKC7h4fNrlFTKPvkZ4iPf9tin/wHBkPfmu6WpgFREalnSHn7XOpw4GchhAQ+kFJ+CBxC2UB8i5ovaF3WgUKIycBkgMDAwCp0QWNTzp9XMs7774OXF7z5Jjz8sE10/HyzmTXnz7MwLo4daWn4NGrEzMBAHvX3p2UtKXJSVYrbJ0+lTZtXtX2yptqpiNQzD3i9oOC6EMIHmC6lfLYC5+8vpYwXQjQHfhFCRAITgbeEEM8B3wN5ZR1oeUh8CNCrV69SWUWaaiYvT5mnvfgipKfDI4/AnDmqOEoVSc/P55OzZ3kzLo6YnBzauriwpH177mvRAvdabphWUczmXIt98mvaPllT41RE6rlRSjmrYENKmSyEGA6UG/illPGWn+eEEGuAK6WUbwDXAwghOgAjLqvnmupBSiXjPPkkHD0K11+vdPzQqtfhicvJ4e34eD44fZpUk4l+Xl4saNuWm5o2xbGO6/dFKW6fPNFin+xV093SNGAqkg7hKISwvmcLIVyBct+7hRDuQgjPgj+jgv1By+gfoVakPIvK8NHURg4fVhO2I0eq7R9/VN46VQz6+9LTuffIEUL++os3YmO53teXHT178nvPnvxfs2Z1IuhHREBwMDg4qJ8REaX3MZvzOXlyHrt398ZoPEfXrj/QqdNSHfQ1NU5FRvwRwCYhxDLL9gQq5tLpB6yxZF40Ar6UUq4XQjwuhJhi2ecbYNnFTqCpIZKSlIzz3nvg6QmLFilppwqFScxSsv7CBRbExrI5JQUPR0emtGrF4wEBhLi62q7v1UBEBEyerBwoAE6eVNsAY8eqn9o+WVObEVKWL58LIW4ArrVs/iKl3GDXXpWgV69ecteuXdV5yYaJ0ah0/DlzIC1NLb568UVo2vSyT5ljMvF5QgKL4uI4kpWFf+PGTA0IYHLLlnjXwQVX+fkQEKBqx5TEYID33zfTps3bZGc/g6OjGx06vKvtkzU1hhBit5SyV8n2ikzuhgBbpZTrLduuQohgKeUJ23dTU2MU6PhRUarc4cKF0LXrZZ8uMS+Pd0+f5p34eBKNRsI9PPiic2dua9aMxnVswdX580rhWrdO/UxOLns/F5cTnDkzgRYttrJjxwjWrPmI1q1bEhqK9dOhg00XMms0l0VFpJ6VwNVFtk2Wtt526ZGmejl8WC3AWr8e2rdXxmojRsBl6uyRmZksiovjfwkJ5JjNjPD1ZXrr1gz29q4zC66kVOV/165VwX7HDtXWvDncdJNq79Ejgvvvn03z5qc4d641O3dez7XXfoWrK6SkLCUjYwKtWgkOHFC1481mde5GjVTw79qVYg+Edu3sWmlSoylGuVKPEGKflDKsRNt+KWW1rSvXUo8duHBBSTrvvgseHio3f8qUy9LxpZT8mpLCgrg4fkxKwlkIxrVowRMBAXR2d7d93+1Aejps3FgY7M+cUe29eytj0REj4Ior1GTuN99E4OY2GReXrGLnMBo707//ulL2ydnZ6kXq4EE4dKjwExOjHiig/to7diz+MOjaFdq0gXqS0aqpAS5b6gEShRA3SSm/t5zoZuB8OcdoaitGo1p89cILkJqqiqG8+CI0a1b5U5nNrExMZEFsLHsyMmjq5MQLQUE84u9P8ypMBFcXR48WBvpff1V/NV5eqiDY8OFqIbKfX+njWracSW5uVql2D4/MMj3zXV0hLEx9ipKVBUeOFH8Y7NgBK1YU7uPiohZGF30ghIZCSIh6CGk0l0NFRvxtUZk9rQABxAL3Sin/tX/3FHrEbyPWr1c6/pEjMHSoytbp1q3Sp0nNz+fD06d5Kz6euNxcOrm58WRAAPf4+eFai4enubkqwBcE++PHVXuXLoWj+n79SmvwZnMuaWk7SE7eSHLyJtLStl/kCoLBg81V7mdGhvoVlXxDiI0t3MfVVVkilXxDCAzUDwRNIZc94rcE+L5CCA/LdoYQojdQbYFfU0UiI1XA/+knJSZ/9x2MGlVpHf9EdjZvxsfz8ZkzZJhMDPH25v0OHbjR1xeHWqrfx8WpIL92rSoPkJmpRtHXXKNK/w4frkbPRZHSTEbGfmugT039DbM5G3DA07M3jo5emEylDWWdnW1jLeLhoSSm3iVm0dLS1JRM0YfB5s3w+eeF+7i7qwdZyTeE1q0ve9pGUw+pzHRSIHCXEOJOIJXiPv2a2siFC0rGefddcHODN95QpQ8rKcP8nZbGgthYViUm4iAEdzRrxpOtW9PT09NOHb98TCYllxSM6vfvV+1BQTB+vAr0Q4aov44CpJTk5ERbA31y8mby85UbuZtbZ1q2vB8fn6EYDINwcvImISGCqKjJmM2Fco+Dgxtt2sy16715eUHfvupTlJSU4g+DQ4fUy92nnxbu4+lZ+EAoOrHcqpV+IDRELin1CCGCgbssHyMQBPSq7lROLfVUkvx8+OADVRQlJQUeeEB55TdvXuFTmKTkh/PnWRAXx++pqRgcHZncqhVT/f0JcHGxY+crT1KSCnRr18KGDep55+gI/fsXSjhduhQPcHl5CSQnb7YE+o3k5p4EoHFjf3x8rsXHZyg+PkNxdm5V5jUTEiKIjp5Nbu4pnJ0DadNmLn5+Y6vjdivMhQulHwgHD6p6OQUYDKXfDkJDoUUL/UCoD1xM6rlo4BdCbAe8gBXACinlMSFEjJQypMwD7IgO/JXg559V2cPDh9XQdvFi6N69wodnmkx8evYsi+PiOJ6dTZCzM9MCApjUsiWetSTfUEo1kl+7Vn3++kulSzZvriZkhw9XlkLe3oXH5Oenk5r6mzXQZ2b+A4CjowEfn2ssgf5aXF071Jm008slMbH0A+HQIfUALcDHp/TbQWhopcYOmlrA5Wj8CYA/ynqhGXCMMmrvamoJUVEqH3/tWmjbFr79ViWdVzCIncnNZUl8PO+fPs2F/Hyu9PTkqy5duLVpUxrVgtnCjIzi6ZanT6v2Xr3guedUsO/Vq3Bi02w2kpLyF8nJG0lJ2URa2g6kzEcIZwyG/oSEzMPH51o8PXsiRO2dkLYHzZrB4MHqU4CUcO5c6beDFSvUS2MBTZuW/YZQhcXdmhqgPKnHANyKknraA97AMCnl39XTPYUe8V+C5GQl4yxZolI9nnsOpk6FCvrX/5ORwcK4OL5MSMAoJbc0bcr01q252surxke+x44Vjup/+005Q3t5qdH8iBHKP65FC7WvlGYyMw9adfqUlF8xmzMBgadnL+uI3svrahwd65Y3UE0ipVrTUNYbQlqR+e3mzUu/HYSGqjcHTc1RaamnjBM0R1XeugsIlFKWWUDFHujAXwb5+aqo+fPPKzG3QMcvK/G8BFJKfklOZkFsLD8nJ+Pm4MCEFi2YFhBAO7eaKwqSm6sCfEGwL0i37NxZBfqS6ZbZ2SesI/rk5E0YjUq8dnXtaA303t6DcXLS0cfWSAnx8aXfEA4fVm9nBbRsWfYbgsFQc31vSFQ58Jc4WZCU8qRNelYBdOAvwS+/KB3/0CH1vr54MfQofyF1rtnM8oQEFsbF8U9mJi0aN+Yxf38eatUK3xoykImPL0y33LixMN1yyBAV6IumW+blnSclpXBCNicnGoDGjVsWCfRDcXEJqJF70agHwqlTpd8ODh8udDMFZXRX8mHQpYvKPtLYDpsG/upGB34LR4/CjBnKT6dNG5Weecst5er4SUYjH5w+zdvx8ZzNy6ObuztPBgRwl58fztWs35tMajK2YFRfkG4ZGFg4qi9ItzSZMklJ2WYZ0W8kI2MfAI6OXnh7D7Zm37i5da5xWUpzacxmZV9d9O3g0CG1UC0np3C/wMDSi9I6d1brEzSVRwf+ukxKCrz8Mrz9thoOP/ssPP54uTr+8awsFsXF8enZs2SZzVzv48P01q25zsenWgNlUpJKs1y7VqVdFqRb9utXGOy7dAEpjaSn77SO6NPStiOlESEaYzBcbR3Re3r2wsGhdmQYaaqGyaQ8i0q+IURGKumvgJCQ0m8InTuraS3NxbnswC+E6Cel/KO8NnvSYAN/fj58/LGasE1KgkmT4JVXLqnjSyn5IzWVBXFxfHf+PE5CMNbPjycDAujq4VEt3S5ItyyQcHbsUCO+Zs1UuuWIEWqC1mCQZGYeso7oU1J+xWRKBwQeHuHWEb3B0F8XJG9g5OdDdHTxt4NDh1TymtGo9hFCvfiWTDvt2FGNjzRVC/x7pJQ9y2uzJw0y8G/apDwFDh6EgQOVjh8eftHd881mvjl/ngWxsfydno5vo0Y83KoVj/r706KCGT5VISNDdbkg3TI+XrVfcUXhqL5XL8jLO2Ud0aekbCYv7ywArq7trCN6H58hulqVpkyMRjXpX/IN4ehR9bAAldLbrl3pN4SOHatURK5OUuk8fiHEVSgf/mZCiCeLfOUFNKzE5+rk2DGl43//vXq/XbUKbr31ojp+en4+S8+cYXFcHCdzc2nn6so77dszvkUL3O1smHbsWOGo/tdfVbqlp2dhuuWNN0KTJhdISdlCcvJGdu7cRHb2MQCcnJpbJ2R9fIbi4hJk175q6gdOTkri6dwZxowpbM/LU/8eS74hfP+9kpNAyYvt25dOO23fvuEVx7mUUNoY8LDsU3SuPQ0YU+YRmssnNVXp+G+9pbT7V19VI/6LvLPG5eTwVnw8H54+TarJxACDgcXt2jGqaVO7FSsvSLcsCPbHVAync2dlATRiBPTtm0129u8kJ28kLm4TkZF7AImjowcGwyBatXoEH5+huLt31ROyGpvRuHFhIL/99sL23FwlDxV9O9i/H775prA4jpOTKo5T8g2hPhfHqYjUY03dFEI4AB5SytLWhHakXks9JlOhjn/+PEyYAHPnFq5MKsGe9HQWxMbydWIiUkrGWAzTrvTyskv3CtIt161T6ZYZGeq5VJBueeON+TRrttsq36Sm/oGUeQjhhJdXX+uI3tPzShwcGtiwSlNrqWhxnLJqIdSl4jhV0fi/BB5ClVzciZJ63pRSzrdHR8uiPg5tvgAAIABJREFU3gb+zZtVPv6BAzBggNLxe5aeOjFLybqkJBbExbE1JQUPR0ceaNmSqf7+BNs4raFouuW6daoEIShbX6XVS666KpLs7AKdfismUyoA7u49ikzIDqBRo+qZTNZobEVZxXEOHYITJwr3Kas4TteuEBxc+2ohVCXw75NShgkhxgI9gWeA3VLKijt/VZF6F/iPH4ennlJ+OsHBMH8+jB5dSsfPNpn4PCGBRXFxRGZlEeDszOP+/jzQqhUGG76DXrhQvJh4UlJhuuXw4XDDDfE0b76JlBRlh5CXp4xyXFxCrIHe2/saGjeufBUvjaYuUJHiOG5upYvjhIbWbHGcqpRedBJCOAG3AEuklEYhRO1P/q+NpKYqGWfxYvUeOW+eGvGX0PHP5eXxbnw8754+TaLRSE8PDyI6d+a2Zs1wssG/ICnVS0bBqH779sJ0SzWqT+HKK7eSl1dghxBJcjI4OTW1ZN2oj6trmyr3RaOpC1SmOM6mTfC//xXuU7I4TsHkckDApddeRkTA7NlqJXRgoAodY23k/F2REf9U4D/AfmAEqiDLF1LKAbbpQvnU+RG/yQSffKIWXiUmwn33qd9iy5bFdjuSmcnCuDg+P3uWXCkZ2aQJ0wMCGOTtXeWJ0EulW44alcN11/2Jn58K9OnpuwAzDg5ueHsPsmbfuLt3Q03zaDSaS1FWcZxDh+Ds2cJ9vLzKrpbWqhV8+SVMnlzc5sLNTdlzVSb429qrp5GUMr/SB14mdTrwb92qsnP271eVQRYvVtHWgpSSLSkpLIiNZd2FC7g4ODDOz48nAgLoVMV16sePFwb6rVsL0y2HDTNxyy176dFjIybTJlJTf8dszgEcLROyBU6WfXBwaGCJzxqNHSmrOM6hQ8oSuwBvbzX5XHTlcgFBQcXnG8qjKhq/HzAPaCWlvFEI0QW4Skq5tOKXrxp1MvBHR6t8/DVr1G/r9dfhttus73ZGs5mvzp1jYVwcezMyaObkxKP+/jzcqhXNLnOVSV5eobvlunVqUQtAp06S228/xqBBG2nadBNpaVvIz08GwN29mzXQGwwDadRIu2RpNNVNyeI4771X9n5CFKahVoSqBP6fgGXAbCllDyFEI2CvlLJbxS9fNepU4E9LK9TxnZxg5kxV6NySfZNiNPLhmTO8FRdHfF4end3ceDIggHv8/HC5jByx06eLu1sWpFuOHHmGm2/eRKdOm8jP30hubhygCoKrCdlr8fG5hsaNy7dx1mg01UtwsDK1K4mtRvyXWrlbIOc0lVJ+LYSYCSClzBdCmCp+6QaCyaSqW8+eDQkJqrL3vHlKsANisrN5My6OpWfPkmEycY23Nx927MgNvr44VEK/N5ng778LR/V796r29u3TePrpX+nTZyMGwyaysw8BkJvrayktqOwQXF3b6oVTNiDinwhmb5rNqdRTBBoCmTt0LmO71a6au5q6y9y5ZWv8c+fa5vyXyur5G5W+mSmEaIKl7KIQoi+QapvL1xN+/VXp+Pv2qRzIH39UxjTAX2lpLIiNZXViIg5CcGfz5jwZEEB4JYzHL1wo7m6ZlAQuLrncdtt2pk/fRFDQJvLz/wZMODi44uIygJYtx+PjMxQPjzA9IWtjIv6JYPIPk8kyqv+VJ1NPMvmHyQA6+GtsQsEEbrVn9Qgh9kopw4UQPYG3ga7AQVT93TFSygO26UL51FqpJyZG5eOvXq1+M6+/Drffjgn47vx5FsbG8kdaGgZHRx5s1YrH/P0JqIBtoJTwzz+FnvXbt6vSgr167WPMmE306LHx/9u77/CoqvSB4993UklCQkIkAZLQpSiCEJAIKypYEBUbLsUKiFioUX/q6uq6a8E1EMECLLI2iooN14KEIii9CUiRFkKA0AkJJfX8/rg3MKTRksxM8n6eZ565uffOnfdkknfOPffcc/D3X4AxJwAHwcHt7W6WXQkJicPhKP9B2Sqr7Lxs0jLTTj32ZOyxnjNPP6/YvYI8U/SkNyYkhh3DKmx+IqXO6kL68TsPzvY18AMgQBbQFaiwxO92MjKsZpxRo6zBPP75T4iP55ivL//dtYvE1FS2njxJA39/3m7cmH6RkQSd5YarY8fO7G6ZmmqoU2cbd9yRxODBs4mImAMcBCAgoAWhoQPs5pvOeHvrPHalMcZw5OSRMxN4oYResHzoxKFij3FJwCVEBkUSGRRZbNIHSElPYfKaydx72b34eOnwFMp9lVbj3wO8j5XsizDG/KMc4zqD29T48/Otdvznn7fa8R94AF57jT3h4YzdtYtxu3dzODeXDsHBxEdFcUd4ON6l3HC1devpWv28eRAYuJerr55Dt26zufTSJHx8rNqjn1/UqRp9aOj1+PnVqZjyurnsvGz2Zu4tNqGnHXNazkwjK69o3zg/Lz9qV69N7aDaRAZFnnqODIqkdvXT62oF1jojkddPrM+O9KI1e2+HN7n5uUQFRzGk/RAeafsINfxrlOvvQKnSnHevnooec780bpH4FyywZr1atQri4iAxkTUtWjBq506m7NtHrjHcGR5OfHQ0V5cwk3R2tnWYgmS/c2cGrVrNp0uX2bRvn0Rw8FoAvL1rUKPG9U53yF5aZS7IGmNIz0ovsUbunNwPnjhY7DHCA8KLJvKC5O6U0IP9gi/o91q4jR8gwCeA8beOJ9Q/lIRFCcxNnkuQbxADrhzA0A5DqV+j/oX+SpS6YBeS+FcZY0qe+aMCuTTxJyfDM8/AF19AVBRm5Eh+vukmElJTmXX4MAEOB/1q12ZYVBSNihkwbfdu+PFHK9HPnZtNdPQS2refTefOSURFLUEkFxE/QkI6OY1k2QYRDxn+7xzl5OWw99jeEtvNndvVT+aeLPL6gtp5kUReqJZeK7AWvl7lf9PZ2Xr1rNqzilGLRzFt3TTyTT73tLiHER1GcFXUVeUem1IFLiTxhxljim/wPPc3TQYysEb2zDXGxIpIa2Ac4A/kAo8bY5aWdhyXJP6MDHjjDUhIAC8vcp95hsn33ce/Dx7kj+PHqe3ry5C6dRlYpw5hTrM45OXBsmUFbfX5pKevpU2b2XTsmESLFvPx8TkGOKheve2pRB8cfDVeXp43eagxhqNZR8/abp6WmcaB4weKPUbNajWL1MSLq6WH+IV45FlP6tFUxi4Zy/gV40nPSqdjdEfi4+K5venteDkq15e7cj8umWzdTvyxxpgDTut+BkYbY34UkVuAZ4wx15Z2nApN/Pn51ghLzz0HaWlk9enDuCef5PX8fPbm5HBFYCDx0dH0qlULX7v9/tAh+PlnK9mvXLmd+vVnExubRLt2cwgK2g9AtWpNnUayvBYfn9CKKc8FyMnLYd+xfUXbzYuppRdXO/f18j2zaSWwaLt5ZFAkEUERFVI7dwcZWRlMWjWJxCWJJB9JpnFYY4ZdNYyHWj9EoO/FDc2hVEncKfHPBCYZYz4Tkd7AbcaYPqUdp8IS/6+/Wv3xV6zgRLt2jH3qKV6OjOREfj43h4URHxVFl9BQQFi71up9M3fuAXJy5tCmTRKxsbOJjNwGgLd3bWrWLEj0XfD3jyr/+EtRUDsvUiPP2FPkQuiB4wcwFP27CKsWVmwzS+EmmBr+Fz+oXGWVm5/L1xu+JmFRAkt2LSGsWhiD2g7iyfZPUrt67bMfQKnz4KrEvx04jHXz13hjzAQRaQ7MxOot5ACuLpjhq9BrBwIDAWJiYtruKO7+5bKyY4fVjv/552TVqcO7gwfzdPv2eHt5cV9EBCOio6lPIHPmwE8/HSM5eQExMUm0aTObJk3smUoIpmbN606NexMQ0KxCkl9ufu6pni1FLoAeO7Mv+oncE0Ve7+vlW6RppbikHhEYgZ+33h9QVowxLNy5kFGLR/H1hq/xdnjTp2Uf4uPiaRlRYaOhqErOVYm/rjFml4jUAmYBg7Hm6/3FGPOliNwLDDTGdC3tOOVW48/MhDfewLz1FnkOB/+97z6G3Xkn1apX57G6demeU4dlMx2sXr0MY6wbp1q0WISPTw7G+BIQ0JHISCvRBwW1xeEom8lRjDFkZGectd18T8aeEmvnof6h53QxNNQ/VGvnLrb10FYSFycyafUkjucc54aGNxAfF8+NjW7Uz0ZdFJck/kIBvAxkAi8CNYwxRqy/6nRjTKkTxpZ54s/Ph08+If+553Ds2cM3N97I4P798Y+pxy3H61L914McSptHVFQSrVr9QmBgBsYI0Ia6dbsQHt6VkJCOeHkFnNfb5ubnsu/YvrO2m6dlpp3RVbCAj8PnzH7mgUWbWQqWtXbueQ6dOMT45eMZu3QsezL3cHmtyxnRYQR9WvbRz1NdkApP/CISCDiMMRn28izgFWA08JgxZp6IdAHeNMa0Le1YZZr4f/uN7CFD8F25kuXNmzP4iScwjRoQu3EL4elLuKLlbMLC9gKQldWEsLAu1K/f1b4gW7PI4YwxZGZnFm03Lyah7z+2v9jaeQ3/Gud0MTS0WigOHXen0svKzWLaumkkLEpg7b61RAZF8mS7JxkUO4iaAUX/BpUqiSsSf0OsoR7AGhpiijHmVRHpBLxtrzuJ1Z1zRWnHupDEP+H1m2neeBZ5NfPxOuggeV0nuv9xCWFffsmO+mFMHHotuXW9aZG/guiIrQCcOBGBt3cXmjTpSnitzmTkVTun2/yLq517O7xLbGZxrqVHBkXi73328XtU1WOMIWlbEgmLEpi5dSbVvKvxcOuHGR43nMZhjV0dnvIALm/quRjnm/gnvH4zza6cSb5TPpVsCFkOO5vUJDD8EA4xnMz2Z1dmIw55RbJbgtiYcZI9GVZC3398P/mm6IwHIX4hZ+1zHhkUSVi1MK2dqzKzbt86Ri0axeS1k8nJy6FHsx6M6DCCTjGd9DqAKlGVSvwLvvAi75Jipqkx8PtRBysP57PiMGzMgDxj1c4jAiNO18QDizaz1K5em4jACKr5eN6NVqrySMtM492l7/Le8vc4dOIQ7eq0Iz4unrtb3I13GXUuUJVHlUr88+aI1VG0sHxY6jeySC29ZkBNrZ0rj3I85zgfrf6I0YtHs/nQZuqF1GPoVUMZ0GYA1f10+kxlqVKJv6Qav9d+B3/pqZOHqcoj3+Tz3abvSFiUwIKUBQT7BTOwzUCGXDWE6JBoV4enXKykxF8pq7kbttyAo9BIAo6T1nqlKhOHOOjRrAfzH57P0gFL6da4G6MXj6bhmIb0/aovK/esdHWIyg1VysQ/8Lmf2LjqJrz2OyDfqulvXHUTA5/7ydWhKVVu2tVtx7R7prF1yFaGtB/Cd5u+o+2Etlz30XX878//FdtZQVVNlbKpRykF6SfTmbhyIm8veZudR3fStGZThncYzgOtHtBOClVElWrqUUpBiH8I8VfHs3XIVqbcNYUg3yAGfT+ImMQYXpr7EvuO7XN1iMpFNPErVcn5ePnQu2Vvlj2yjHkPziMuKo5X5r9CzOgYHpnxCBv2b3B1iKqCaeJXqooQETrX78yM3jPY+MRGHmr9EJ+u/ZQW77Wg+5TuzNk+B09o+lUXTxO/UlVQ0/CmjLt1HCnDUnjl2ldYvns5XT7uQpsJbfh0zadk52W7OkRVjjTxK1WFXRJ4CS92fpEdw3Yw8baJZOdlc//X99Pw7YaM/HUkR04ecXWIqhxo4ldK4e/tT/82/Vn72Fp+6PMDzcKb8ezsZ4kaFcXQH4ey/fB2V4eoypAmfqXUKQ5x0K1JN5IeSGLVo6u4q/ldvLf8PRqPbUzPL3qyOHWxq0NUZUATv1KqWK0jW/PxnR+TPDSZp69+mqRtScR9EEfHSR35asNX5OXr8CeeShO/UqpUdYPr8kbXN9g5fCdv3/w2ezL2cPfnd9P0naa8s/QdjmUfc3WI6jxp4ldKnZMg3yCGXDWEzYM3M73ndGoF1mLwj4OJHh3N87OfZ3fGbleHqM6RJn6l1Hnxcnhxd4u7Wdh/IQv7LeT6Btcz8reR1E+sz4PfPMiavWtcHaI6C038SqkLFhcdx/R7p7N58GYGxQ7iy/Vf0mpcK2745AZ+2vKT3hDmpjTxK6UuWsPQhozpNoadw3fyepfXWb9/Pd0md6Pl+y2ZtGoSWblZrg5ROdHEr5QqM6HVQnm207NsH7qdj+/4GG+HN/1n9KdeYj3+Nf9fHDh+wNUhKjTxK6XKga+XL/e3up9Vj64i6f4k2tRuw4tzXyRmdAyP/e8x/jz4p6tDrNI08Sulyo2I0KVhF37o+wPrHltHn5Z9mLR6Es3eaUaPaT2Yv2O+XgdwAU38SqkKcVmty5h4+0RShqXwwjUv8FvKb3T+sDPtJ7Zn2rpp5ObnujrEKkMTv1KqQkUERfDKda+QMjyF97u/z9Gso/T+sjeNxjRi1KJRHM066uoQKz1N/EoplwjwCWBQ7CA2PLGBGb1m0KBGA+J/jid6dDRP/fwUKekprg6x0tLEr5RyKYc4uK3pbcx7aB7LHllG9ybdSVycSMO3G9L7y94s363zbZc1TfxKKbcRWyeWKXdPYdvQbQzrMIzv//yedv9pR+cPOzNj0wzyTb6rQ6wUNPErpdxOTEgMb934FqkjUkm4MYHkI8n0mNaD5u82Z9zycRzPOe7qED2aJn6llNsK9gtmRNwItg7ZytS7pxLsF8xj3z9GzOgY/j737+zN3OvqED2SJn6llNvzdnjT6/JeLB2wlF8e+oVOMZ341/x/US+xHgNmDGD9/vWuDtGjaOJXSnkMEeGaetfwTa9v2PjkRvpd2Y8pa6dw2XuXccvkW0jalqQ3hJ0DTfxKKY90ac1Lea/7e6QMT+Gf1/2TlXtWcsMnN3Dl+Cv5+PePyc7LdnWIbksTv1LKo4UHhPPCNS+QPCyZD27/gNz8XB785kEavN2AN359g8MnDrs6RLcj5XlaJCLJQAaQB+QaY2JF5DOgqb1LDeCIMaZ1aceJjY01y5drX16l1NkZY5i5dSajFo1i1rZZBPoE0u/KfgzrMIyGoQ1dHV6FEpEVxpjYIusrIPHHGmOKHYtVRBKAdGPMK6UdRxO/UupCrNm7hlGLRjFl7RTyTB53NLuD+Lh4ro6+2tWhVYiSEr/LmnpERIB7gamuikEpVbldEXEFH97xIcnDkvm/jv/H3O1z6TipI3EfxDF9/XTy8vNcHaJLlHfiN8DPIrJCRAYW2vYXYK8xZnM5x6CUquLqVK/Da11eI2V4CmO7jWXfsX30/KInTcY2YcySMWRmZ7o6xApV3k09dY0xu0SkFjALGGyMmW9vex/YYoxJKOG1A4GBADExMW137NhRbnEqpaqWvPw8vt30LQmLEli4cyE1/GvwaNtHGdx+MHWD67o6vDLjkjb+QgG8DGQaY94SEW9gF9DWGJN6ttdqG79SqrwsTl1MwqIEvtrwFQ5x0Pvy3sTHxdMqspWrQ7toFd7GLyKBIlK9YBm4EVhnb+4KbDyXpK+UUuWpQ1QHvuj5BVsGb+GJdk/w1YavaD2+NV0/7soPm3+olAPDlWcbfwTwq4j8DiwFvjfG/GRv64Ve1FVKuZEGoQ1IvDmR1BGpjOw6ko0HNtJ9Sncuf+9yJq6cyMnck64OscxUWFPPxdCmHqVURcvOy+bzPz4nYVECq9NWUyuwFk+0e4LH2z1OeEC4q8M7J27XnVMppdyZr5cv911xHysHrmT2A7OJrRPLS/NeInp0NIP+N4hNBza5OsQLpolfKaVKISJc3+B6vu/zPesfX8/9V9zPh6s/pNm7zbh96u38kvyLxw0Mp4lfKaXOUfNLmjPhtgmkDE/hpc4vsSh1Edd+dC3t/tOOqWunkpOX4+oQz4kmfqWUOk+1Amvx8rUvkzIshfG3jiczO5M+X/Wh0ZhGvLXwLdJPprs6xFJp4ldKqQtUzacaA9sOZP0T6/mu93c0CmvE07OeJnp0NCNmjmDHEfe88VQTv1JKXSSHOLj10luZ++BcVgxcwW1Nb2PMkjE0GtOIXtN7sWzXMleHeAZN/EopVYba1G7D5Lsms33odkbEjeDHLT/SfmJ7rvnvNXy78Vu3uCFME79SSpWD6JBo3rzhTVKHpzL6ptGkpKdwx2d30OydZry/7H2O5xx3WWya+JVSqhxV96vOsA7D2DJkC5/d8xmh1UJ5/IfHiR4dzQtzXiAtM63CY9LEr5RSFcDb4c29l93L4v6LWfDwAq6pdw2vLXiNeon16PdtP9btW3f2g5QRTfxKKVWBRIROMZ34+q9fs+nJTQy4cgDT1k2j5fstufnTm5m1dRbGGCavnUz9xPo4/uGgfmJ9Jq+dXHYxeMIdZzpWj1KqMjt4/CDjlo/jnWXvkJaZRnRwNHuP7SU7L/vUPgE+AUy4bQJ9W/Y95+O6fDz+i6GJXylVFWTlZjF13VQGfjeQnPyidwHXC6lH8rDkcz6eDtKmlFJuzs/bj4daP0Rufm6x21PSU8rkfTTxK6WUm4kJiTmv9edLE79SSrmZV7u8SoBPwBnrAnwCeLXLq2VyfE38SinlZvq27MuE2yZQL6QeglAvpN55X9gtjV7cVUqpSkov7iqllAI08SulVJWjiV8ppaoYTfxKKVXFaOJXSqkqxiN69YjIfuBC5zALBw6UYTiupGVxP5WlHKBlcVcXU5Z6xphLCq/0iMR/MURkeXHdmTyRlsX9VJZygJbFXZVHWbSpRymlqhhN/EopVcVUhcQ/wdUBlCEti/upLOUALYu7KvOyVPo2fqWUUmeqCjV+pZRSTjTxK6VUFePxiV9EaojIdBHZKCIbRCRORMJEZJaIbLafQ+19RUTGiMgWEVkjIm1cHX8BEWkqIqudHkdFZJgnlgVARIaLyB8isk5EpoqIv4g0EJEldsyfiYivva+f/fMWe3t910Z/JhEZapfjDxEZZq/ziM9FRCaJyD4RWee07rxjF5EH7f03i8iDblKOnvZnki8isYX2f84uxyYRuclp/c32ui0i8mxFlsEphuLK8m87h60Rka9FpIbTtrIvizHGox/AR8AAe9kXqAG8CTxrr3sWGGkv3wL8CAjQAVji6vhLKJMXkAbU88SyAHWB7UA1++fPgYfs5172unHAY/by48A4e7kX8Jmry+BUlsuBdUAA4A0kAY095XMBrgHaAOuc1p1X7EAYsM1+DrWXQ92gHM2BpsA8INZpfQvgd8APaABstf+nvOzlhnau+B1o4SafyY2At7080ukzKZeyeHSNX0RCsH6JHwAYY7KNMUeAHlhfCNjPd9jLPYCPjWUxUENEaldw2OeiC7DVGLMDzy2LN1BNRLyxkuYe4Hpgur29cFkKyjgd6CIiUoGxlqY5VgI8bozJBX4B7sJDPhdjzHzgUKHV5xv7TcAsY8whY8xhYBZwc/lHf1px5TDGbDDGbCpm9x7ANGNMljFmO7AFaG8/thhjthljsoFp9r4VqoSy/Gz/fQEsBqLs5XIpi0cnfqxvwP3Af0VklYhMFJFAIMIYs8feJw2IsJfrAjudXp9qr3M3vYCp9rLHlcUYswt4C0jBSvjpwArgiNMft3O8p8pib08HalZkzKVYB/xFRGqKSABWrTgaD/xcnJxv7J5QJmeeXo5+WGdeUE5l8fTE7411yvS+MeZK4BjWqespxjpf8pg+q3a79+3AF4W3eUpZ7DbjHlhfzHWAQCq4hlhWjDEbsE69fwZ+AlYDeYX28YjPpTieHHtlJCJ/A3KByeX5Pp6e+FOBVGPMEvvn6VhfBHsLTq/t53329l1YtbUCUfY6d9INWGmM2Wv/7Ill6QpsN8bsN8bkAF8BHbGaDrztfZzjPVUWe3sIcLBiQy6ZMeYDY0xbY8w1wGHgTzzzcylwvrF7QpmceWQ5ROQh4Fagr/2FDOVUFo9O/MaYNGCniDS1V3UB1gMzgIKeBw8C39rLM4AH7N4LHYB0p1Ned9Gb08084JllSQE6iEiA3VZf8LnMBe6x9ylcloIy3gPMcfrDdzkRqWU/x2C170/BMz+XAucb+0zgRhEJtc/mbrTXuasZQC+7t1gDoAmwFFgGNBGrd5kvVpPqDBfGeYqI3Aw8A9xujDnutKl8ylLRV7TL+gG0BpYDa4BvsHod1ARmA5uxemGE2fsK8C7W1fC1OPUEcIcHVpPIQSDEaZ2nluUfwEasNvJPsHolNLT/aLdgNWX52fv62z9vsbc3dHX8hcqyAOuL63egiyd9LliViD1ADtYZcv8LiR2r3XmL/XjYTcpxp72cBewFZjrt/ze7HJuAbk7rb8E6Y9sK/M2NPpMtWG32q+3HuPIsiw7ZoJRSVYxHN/UopZQ6f5r4lVKqitHEr5RSVYwmfqWUqmI08SulVBWjiV95BBExIpLg9PNTIvJyGRzXT0SSxBoR9a+Ftn0oItvl9IipCy/2/S4wxpdF5ClXvLeqnLzPvotSbiELuEtEXjfGHCjD414JYIxpXcL2p40x00vYppRH0hq/8hS5WHOPDi+8QUTqi8gceyzz2fYdtoX3CRORb+x9FovIFfYduZ8C7ewafaNzCURE3haRv9vLN4nIfBFxiMhtYs0nsMo+i4iw93lZRD4SkQUiskNE7hKRN0VkrYj8JCI+9n7JTuuXikjjYt67kf2aFfbxmtnre4o1Z8DvIjL/3H+tqirSxK88ybtAX7GG43Y2FvjIGHMF1uBWY4p57T+AVfY+z2MNP7wPGAAsMMa0NsZsLeZ1/3Zq6ikYOOs54K8icp39Xg8bY/KBX4EOxhowcBrWLfgFGmENS3071pfNXGNMS+AE0N1pv3R7/TtAYjHxTAAGG2PaAk8B79nr/w7cZIxpZb+HUiXSph7lMYwxR0XkY2AIVsIsEIc1hg5Yw0O8WczLOwF328eZYw+zHHwOb1ukqccYc1xEHgHmA8OdvjCigM/sgc98sSajKfCjMSZHRNZiTaLxk71+LVDfab+pTs+jnd9XRIKAq4Ev5PR0BX7282/AhyLyOdageEqVSGv8ytMkYo1tEujiOFpijatUx2ndWOAdu8b+KNYYRAWyAOwzgxxzeqyUfM4lG59wAAABJ0lEQVSsgJkSlsH6fz1in50UPJrbxx0EvIA1YuMKEXGX+QyUG9LErzyKMeYQ1hSO/Z1WL8QanRCgL9agaoUtsLchItcCB4wxRy8kBhGpB8RjXRjuJiJX2ZtCOD007oXOS/tXp+dFzhvseLeLSE87DhGRVvZyI2PMEmPM37EmJ3IeslepM2jiV54oAQh3+nkw8LCIrAHuB4YW85qXgbb2Pm9w7onZuY1/tYj4YU31+ZQxZjfWF9BEEfG33+MLEVkBXGjPo1A7xqEUcyEb68urv4j8DvzB6en2/m1fFF6H9UX4+wW+v6oCdHROpdyEiCRjDYVclt1VlSpCa/xKKVXFaI1fKaWqGK3xK6VUFaOJXymlqhhN/EopVcVo4ldKqSpGE79SSlUx/w9qPPWd71FsjQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Load the accuracies previously obtained\n",
        "with open(os.path.join(base_dir,'random.txt'), 'r') as f:\n",
        "  acc_ = f.readlines()\n",
        "acc_rd = [round(float(x)*100, 2) for x in acc_]\n",
        "with open(os.path.join(base_dir,'entropy.txt'), 'r') as f:\n",
        "  acc_ = f.readlines()\n",
        "acc_en = [round(float(x)*100, 2) for x in acc_]\n",
        "with open(os.path.join(base_dir,'badge.txt'), 'r') as f:\n",
        "  acc_ = f.readlines()\n",
        "acc_bd = [round(float(x)*100, 2) for x in acc_]\n",
        "with open(os.path.join(base_dir,'lcf.txt'), 'r') as f:\n",
        "  acc_ = f.readlines()\n",
        "acc_lcf = [round(float(x)*100, 2) for x in acc_]\n",
        "with open(os.path.join(base_dir,'margin.txt'), 'r') as f:\n",
        "  acc_ = f.readlines()\n",
        "acc_mg = [round(float(x)*100, 2) for x in acc_]\n",
        "\n",
        "# Plot them using matplotlib\n",
        "x_axis = np.array([train_size_pc*nclasses+budget*i for i in range(n_rounds)])\n",
        "plt.figure()\n",
        "plt.plot(x_axis, acc_rd, 'b-', label='RANDOM',marker='o')\n",
        "plt.plot(x_axis, acc_en, 'g-', label='ENTROPY',marker='o')\n",
        "plt.plot(x_axis, acc_bd, 'c', label='BADGE',marker='o')\n",
        "plt.plot(x_axis, acc_lcf, 'r', label='LEAST CONF',marker='o')\n",
        "plt.plot(x_axis, acc_mg, 'y', label='MARGIN',marker='o')\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel('No of Examples')\n",
        "plt.ylabel('Test Accuracy')\n",
        "plt.title('DISTIL_Twitter')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3b49280313144a40ac6a12f9abb5988e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_de852474beab40b58cbdd77e7dd7171c",
              "IPY_MODEL_a059a3cf23af423da59429220d4b4d46",
              "IPY_MODEL_a47c6bb568284c5e8b6fa462cb8bbfd3"
            ],
            "layout": "IPY_MODEL_8ec7969ba8974b0abe79f858049283a4"
          }
        },
        "de852474beab40b58cbdd77e7dd7171c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94425dc1c3c84da1a8864eb715d5c472",
            "placeholder": "​",
            "style": "IPY_MODEL_c380161aabd2477ca16503c3bab06cb5",
            "value": "100%"
          }
        },
        "a059a3cf23af423da59429220d4b4d46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6f70b78a62d4f46ac73f90d00783e86",
            "max": 12284,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_36280274ea9846d290081bf93f439d91",
            "value": 12284
          }
        },
        "a47c6bb568284c5e8b6fa462cb8bbfd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6b184399862473b9d140229614ce2c0",
            "placeholder": "​",
            "style": "IPY_MODEL_698f98eab782407ebbc5fb0871de6d62",
            "value": " 12284/12284 [00:04&lt;00:00, 2172.77ex/s]"
          }
        },
        "8ec7969ba8974b0abe79f858049283a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94425dc1c3c84da1a8864eb715d5c472": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c380161aabd2477ca16503c3bab06cb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e6f70b78a62d4f46ac73f90d00783e86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36280274ea9846d290081bf93f439d91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a6b184399862473b9d140229614ce2c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "698f98eab782407ebbc5fb0871de6d62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}